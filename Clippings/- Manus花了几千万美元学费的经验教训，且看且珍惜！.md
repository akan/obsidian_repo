---
title: "Manus花了几千万美元学费的经验教训，且看且珍惜！"
source: "https://mp.weixin.qq.com/s/rrLxEMt9MhqbOuSJ8b3u2g"
author:
  - "[[404]]"
published:
created: 2025-08-28
description: "希望这篇文章能给构建Agent的朋友一些灵感。"
tags:
  - "KV缓存"
  - "上下文工程"
  - "智能体设计"
abstract: "本文分享了Manus项目在构建AI智能体过程中通过几千万美元实践总结出的上下文工程核心原则与优化经验。"
---
404 *2025年07月19日 21:27*

这是一篇Manus内部花了几千万美元的实践分享，考虑到很多用户现在无法访问manus所以帮忙将原文贴在这里，仅供学习参考。

![Image](https://mmbiz.qpic.cn/mmbiz_jpg/FTOSnyk3X7O3wqR4DzpusWvV15nRQzeg10fyEuqDlpZtjqDSCQ3ZJMOtT2yicIF4dDWpK15Vg8nxeeiaVumWPAXw/640?wx_fmt=jpeg&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1)

在 Manus 项目的最开始，我和我的团队面临了一个关键决策：我们应该使用开源基础来训练一个端到端的代理模型，还是基于前沿模型的上下文学习能力构建一个代理？

在我 NLP 领域的第一个十年里，我们没有那种选择的奢侈。在遥远的 BERT（是的，已经过去了七年）的那些日子里，模型在转移到新任务之前必须进行微调——并进行评估。这个过程每个迭代都要花几周时间，尽管与今天的 LLMs 相比，这些模型非常小。对于快速发展的应用，尤其是 PMF 之前的应用，这种缓慢的反馈循环是致命的。这是我上一家创业公司的一个苦涩教训，在那里我为开放信息提取和语义搜索从头训练模型。 接着出现了 GPT-3 和 Flan-T5，我的内部模型在一夜之间变得无关紧要。讽刺的是，正是这些相同的模型标志着情境学习的开始——以及一条全新的发展道路。

这个来之不易的教训让选择变得清晰： Manus 将押注于情境工程 。这使我们能够在几小时内而不是几周内交付改进，并使我们的产品与底层模型保持正交： 如果模型进步是涨潮，我们希望 Manus 成为船 ，而不是固定在海底的柱子。

然而，上下文工程却远非易事。它是一门实验科学——我们重建了四次智能体框架，每次都是在发现更优的上下文构建方法后。我们亲昵地将这种架构搜索、提示词调试和经验性猜测的过程称为" 随机 梯度下降法 ". 它并不优雅，但有效。

本文分享了我们在自己的"SGD"过程中达到的局部最优解。如果你正在构建自己的 AI 智能体，希望这些原则能助你更快收敛。

### 围绕 KV 缓存进行设计

如果必须选择一个指标，我会认为 KV 缓存命中率是生产阶段 AI 代理最重要的指标。它直接影响延迟和成本。要理解原因，让我们看看典型代理是如何运行的：

在接收到用户输入后，智能体通过一系列工具使用来完成任务。在每次迭代中，模型根据当前上下文从预定义的动作空间中选择一个动作。然后在该动作在环境中执行（例如 Manus 的虚拟机沙盒），以产生一个观察结果。该动作和观察结果被追加到上下文中，形成下一次迭代的输入。这个循环会一直持续到任务完成。

你可以想象，随着每一步的进行，上下文会不断增长，而输出——通常是一个结构化的函数调用——则相对较短。这使得在智能体与聊天机器人之间，预填充与解码之间的比例严重失衡。例如，在 Manus 中，平均输入与输出的 token 比例约为 100:1。

幸运的是，具有相同前缀的上下文可以利用 KV 缓存，这能大幅降低首次获取 token 的时间（TTFT）和推理成本——无论你是使用自托管模型还是调用推理 API。而且我们谈论的不是微小的节省：以 Claude Sonnet 为例，缓存的输入 token 成本为 0.30 美元/百万 token（MTok），而未缓存的成本为 3 美元/MTok——相差 10 倍。

![Image](https://mp.weixin.qq.com/s/www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg%20stroke='none'%20stroke-width='1'%20fill='none'%20fill-rule='evenodd'%20fill-opacity='0'%3E%3Cg%20transform='translate(-249.000000,%20-126.000000)'%20fill='%23FFFFFF'%3E%3Crect%20x='249'%20y='126'%20width='1'%20height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

从上下文工程的角度来看，提高 KV 缓存命中率涉及几个关键实践：

1.保持你的提示前缀稳定。 由于 LLMs 的自回归特性，即使是一个 token 的差异也可能使从该 token 开始的所有缓存失效。一个常见的错误是在系统提示的开头包含时间戳——尤其是精确到秒的时间戳。当然，它允许模型告诉你当前时间，但它也会使你的缓存命中率降低。

2.让你的上下文只追加不修改。 避免修改先前的动作或观察结果。确保你的序列化是确定性的。许多编程语言和库在序列化 JSON 对象时不能保证键的稳定顺序，这可能会无声地破坏缓存。

3.需要时明确设置缓存断点。 某些模型提供者或推理框架不支持自动增量前缀缓存，而是需要在上下文中手动插入缓存断点。设置这些断点时，要考虑潜在的缓存过期问题，至少要确保断点包含系统提示的结尾。

此外，如果你使用 vLLM 等框架自托管模型，请确保 前缀/提示缓存已启用，并使用会话 ID 等技术来在分布式工作节点间一致地路由请求。

### 遮蔽，而非移除

随着你的智能体具备更多功能，其行为空间自然会变得更加复杂——用直白的话来说，就是工具的数量呈爆炸式增长。近期 MCP 的流行更是火上浇油。如果你允许用户可配置的工具，我向你保证：总有人会不可避免地将数百个神秘工具接入你精心构建的行为空间。结果，模型更有可能选择错误的行为或采取低效的路径。简而言之，你武装到牙齿的智能体反而变得更笨了。

自然的反应是设计一个动态动作空间——也许通过某种 RAG 类似的技术按需加载工具。我们在 Manus 中也尝试过这种方法。但我们的实验表明有一个明确的规则：除非绝对必要， 避免在迭代过程中动态添加或删除工具 。这主要有两个原因：

1.在大多数 LLMs 中，工具定义在序列化后位于上下文的前部，通常位于系统提示之前或之后。因此，任何更改都会使后续所有操作和观察的 KV 缓存失效。

2.当先前的行为和观察仍然指向当前上下文中不再定义的工具时，模型会感到困惑。如果没有约束解码 ，这种情况通常会导致模式违规或幻觉行为 。

为了解决这个问题同时还能改进动作选择，Manus 使用了一种上下文感知的状态机来管理工具的可用性。它不是移除工具，而是在解码过程中掩盖 token logits，以防止（或强制）根据当前上下文选择某些动作。

![Image](https://mp.weixin.qq.com/s/www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg%20stroke='none'%20stroke-width='1'%20fill='none'%20fill-rule='evenodd'%20fill-opacity='0'%3E%3Cg%20transform='translate(-249.000000,%20-126.000000)'%20fill='%23FFFFFF'%3E%3Crect%20x='249'%20y='126'%20width='1'%20height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

实际上，大多数模型提供程序和推理框架都支持某种形式的响应预填充 ，这允许您在不修改工具定义的情况下限制操作空间 。函数调用通常有三种模式（我们将使用 NousResearch 的 Hermes 格式作为示例）：

•自动 – 模型可以选择调用函数或不调用。通过仅预填充回复前缀实现：<|im\_start|>assistant

•必须 – 模型必须调用函数，但选择不受约束。通过预填充到工具调用标记实现：<|im\_start|>assistant<tool\_call>

•指定 – 模型必须调用一个特定子集中的函数 ：通过在函数名开头预填充实现： <|im\_start|>assistant<tool\_call>{"name": “browser\_

利用这一点，我们通过直接遮蔽 token logits 来约束动作选择。例如，当用户提供新的输入时，Manus 必须立即回复而不是采取行动。我们还特意设计了具有一致前缀的动作名称——例如，所有与浏览器相关的工具都以 browser\_开头，而命令行工具则以 shell\_开头。这使我们能够轻松地强制代理在特定状态下仅从一组特定的工具中选择，而无需使用状态 logits 处理器。

这些设计有助于确保 Manus 代理循环保持稳定——即使在模型驱动架构下也是如此。

### 使用文件系统作为上下文

现代前沿 LLMs 现在提供 128K 个 token 或更多的上下文窗口。但在实际代理场景中，这通常不够，有时甚至是一个缺点。有三个常见的痛点：

1.观察结果可能很大 ，尤其是在代理与网页或 PDF 等非结构化数据交互时。很容易超出上下文限制。

2.模型性能往往会在某个特定的上下文长度之外下降 ，即使窗口技术上是支持的。

3.长输入仍然很昂贵 ，即使有前缀缓存。你仍然需要支付传输和预填充每个标记的成本。

为了解决这个问题，许多代理系统实现了上下文截断或压缩策略。但过于激进的压缩不可避免地会导致信息丢失。问题是根本性的：代理本质上必须根据所有先前的状态来预测下一步行动——而你无法可靠地预测哪个观察结果可能在十步之后变得关键。从逻辑上讲，任何不可逆的压缩都存在风险。

这就是为什么我们将文件系统视为 Manus 中的终极上下文：容量无限、天生持久，并且可以直接由代理本身操作。模型学会按需写入和读取文件——使用文件系统不仅作为存储，更作为结构化、外化的内存。

![Image](https://mp.weixin.qq.com/s/www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg%20stroke='none'%20stroke-width='1'%20fill='none'%20fill-rule='evenodd'%20fill-opacity='0'%3E%3Cg%20transform='translate(-249.000000,%20-126.000000)'%20fill='%23FFFFFF'%3E%3Crect%20x='249'%20y='126'%20width='1'%20height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

我们的压缩策略始终设计为可恢复 。例如，只要保留 URL，网页内容可以从上下文中移除，如果文档路径在沙盒中仍然可用，文档内容也可以被省略。这允许 Manus 在不永久丢失信息的情况下缩短上下文长度。

在开发这个功能时，我发现自己在想象一个状态空间模型（SSM）要在代理环境中有效工作需要什么。与 Transformer 不同，SSM 缺乏完整的注意力机制，并且难以处理长距离的逆向依赖。但如果它们能够掌握基于文件的记忆——将长期状态外部化而不是在上下文中持有——那么它们的速度和效率可能会解锁一类新的代理。代理式 SSM 可能是真正的《神经图灵机》的继承者。Neural Turing Machines.

### 通过吟诵来操控注意力

如果你使用过 Manus，可能注意到一个有趣的现象：在处理复杂任务时，它倾向于创建一个 todo.md 文件——并且随着任务的进展逐步更新它，勾选已完成的项。

这不仅仅是可爱行为——它是一种刻意机制来操纵注意力.

![Image](https://mp.weixin.qq.com/s/www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg%20stroke='none'%20stroke-width='1'%20fill='none'%20fill-rule='evenodd'%20fill-opacity='0'%3E%3Cg%20transform='translate(-249.000000,%20-126.000000)'%20fill='%23FFFFFF'%3E%3Crect%20x='249'%20y='126'%20width='1'%20height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

在 Manus 中，一个典型的任务需要大约 50 次工具调用 。这是一个漫长的循环——由于 Manus 依赖 LLMs 进行决策，它容易偏离主题或忘记早期目标，特别是在长上下文或复杂任务中。

通过不断重写待办事项列表，Manus 将其目标重述到上下文的末尾 。这使全局计划进入模型的近期注意力范围，避免了" 中间丢失 "问题，并减少了目标错位。实际上，它正在使用自然语言来使其自身注意力偏向任务目标——而无需进行特殊的架构变更。

### 保留错误信息

智能体可能会犯错。这不是一个 bug——这是现实。语言模型会胡言乱语，环境会返回错误，外部工具会表现异常，而意想不到的边缘情况总是出现。在多步骤任务中，失败不是例外；它是循环的一部分。

然而，一个常见的冲动是隐藏这些错误：清理痕迹，重试操作，或重置模型状态，然后将其交给神奇的“ 温度 ”。这感觉更安全，更可控。但它是有代价的： 抹去失败会消除证据 。而没有证据，模型就无法适应。

![Image](https://mp.weixin.qq.com/s/www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg%20stroke='none'%20stroke-width='1'%20fill='none'%20fill-rule='evenodd'%20fill-opacity='0'%3E%3Cg%20transform='translate(-249.000000,%20-126.000000)'%20fill='%23FFFFFF'%3E%3Crect%20x='249'%20y='126'%20width='1'%20height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

根据我们的经验，改进智能体行为最有效的方法之一出奇地简单： 在上下文中保留错误的操作 。当模型看到一个失败的行动——以及由此产生的观察结果或堆栈跟踪——它会隐式地更新其内部信念。这使其先验概率偏离相似行动，从而降低重复相同错误的可能性。 事实上，我们相信错误恢复是真正智能体行为的明显指标。然而，它仍然在大多数学术研究和公开基准中代表性不足，这些基准往往专注于理想条件下的任务成功。

### 不要被少数样本击倒

少样本提示是一种常见的改进 LLM 输出的技术。但在智能体系统中，它可能以微妙的方式适得其反。

语言模型是出色的模仿者；它们模仿上下文中的行为模式 。如果你的上下文中充满了相似的过去行动-观察对，模型会倾向于遵循该模式，即使它不再是最优的。

这在涉及重复决策或行动的任务中可能很危险。例如，当使用 Manus 来帮助审阅一批 20 份简历时，智能体常常陷入一种节奏——仅仅因为上下文中看到的行为而重复相似的行动。这会导致漂移、过度泛化，有时甚至产生幻觉。

![Image](https://mp.weixin.qq.com/s/www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg%20stroke='none'%20stroke-width='1'%20fill='none'%20fill-rule='evenodd'%20fill-opacity='0'%3E%3Cg%20transform='translate(-249.000000,%20-126.000000)'%20fill='%23FFFFFF'%3E%3Crect%20x='249'%20y='126'%20width='1'%20height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

解决方法是增加多样性 。Manus 在行为和观察中引入了少量的结构化变化——不同的序列化模板、替代性措辞、顺序或格式中的轻微噪声。这种可控的随机性有助于打破模式并调整模型的注意力。 换句话说， 不要让自己陷入少数样本的困境 。你的上下文越统一，你的智能体就越脆弱。

### 结论

上下文工程仍然是一门新兴的科学——但对于智能体系统来说，它已经至关重要。模型可能越来越强大、快速且便宜，但无论原始能力有多大，都无法替代记忆、环境和反馈的需求。你如何塑造上下文最终决定了你的智能体的行为：它运行的速度、恢复的能力以及扩展的范围。

在 Manus，我们通过反复重写、走死胡同以及数百万用户的实际测试中学到了这些教训 。我们在这里分享的任何内容都不是普遍真理——但这些是我们行之有效的模式。如果它们能帮助您避免一次痛苦的迭代，那么这篇帖子就完成了它的任务。

智能体的未来将是一次次通过上下文构建起来的。把上下文工程好。

### \-END-

参考文献：Yichao 'Peak' Ji《AI 代理的上下文工程：从构建 Manus 项目中的经验教训》（2025-07-18）

原文链接和源文件我都打包到公众号里了 有需要的小伙伴回复 `Manus` 自取即可，有收获的话可以分享给身边有需要的人。

个人观点，仅供参考

继续滑动看下一个

AI飞升录

向上滑动看下一个