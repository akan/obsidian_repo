---
title: "张小珺对话OpenAI姚顺雨：生成新世界的系统"
source: "https://mp.weixin.qq.com/s?__biz=Mzg5NTc0MjgwMw==&chksm=c1b2a3f48c9de5d48ddff3525addfbf960253ef842eb7f02917246f0ae65c625d4b50f819ae5&idx=1&mid=2247519472&sn=900eec42f7afa37a6d2f874d23bbee4e#rd"
author:
  - "[[张小珺]]"
published:
created: 2025-09-15
description:
tags:
  - "AI智能体"
  - "语言模型"
  - "交互方式"
  - "创业机会"
  - "OpenAI研究"
abstract: "OpenAI研究员姚顺雨分享了对AI智能体发展的见解，认为创业公司的最大机会在于设计新的交互方式，并探讨了语言模型如何通过推理实现泛化以及未来Agent生态的多元可能性。"
---
*2025年09月15日 13:57*

The following article is from 语言即世界language is world Author 张小珺

[

**语言即世界language is world**.

这是原「张小珺」公众号，是我新发起的内容工作室。和我们一起，从这里，探索新世界。

](https://mp.weixin.qq.com/?__biz=Mzg5NTc0MjgwMw==&chksm=c1b2a3f48c9de5d48ddff3525addfbf960253ef842eb7f02917246f0ae65c625d4b50f819ae5&idx=1&mid=2247519472&sn=900eec42f7afa37a6d2f874d23bbee4e#)

![Image](https://mmbiz.qpic.cn/sz_mmbiz_gif/qpAK9iaV2O3sAVsSPfCN9UX44XiaoicbUJIrOGuaujdMNY6iaQewDZEX1GY3tcVk3QGeKJyUMMHBSMALvO8B7DZwsA/640?wx_fmt=gif&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1#imgIndex=0)

2025 年 4 月， OpenAI 研究员姚顺雨发布了一篇有名的博文《 The Second Half 》，宣告 AI 主线程的游戏已进入下半场。这之后，我们与他进行了一场播客对谈。

姚顺雨毕业于清华和普林斯顿大学，博士期间意识到语言是人类发明的最重要的工具，也是最有可能构建通用系统的，于是转向 Language Agent 研究，至今已 6 年。

这场对谈有两位主持人，分别是我和李广密。姚顺雨表达了许多此前从未分享过的观点。比如：

- 创业公司最大的机会是：能够设计不同的 **interface** **（交互方式）。**
- **最终，借助模型的能力或许能产生** **beyond ChatGPT** **（超越** **ChatGPT** **）的交互方式，变成** **Super App** **（超级应用）。**
- **我们的想象力仍被以往的交互方式所限制，还有许多尚未诞生的交互方式。这些新的交互方式，会改变我们的世界。**
- **OpenAI** **可能会成为新世界里非常重要的一环，但这并不代表，这个世界会被这样一个单极系统垄断。如果这样，世界就太灰暗了。**
- **最终智能的边界，可能不是由一家机构定义，而是由不同** **Super App** **共同定义的。**
- **也许，这个世界在变得越来越单极的同时，也在变得越来越多元。**

**我们的谈话从个体出发，共同探索由人、组织、AI** 、人与机器的交互，所抵达的这个世界智能的边界以及人类与机器的全景。

此前， 我们关于 Manus 肖宏、 Youware 明超平、 Lovart 陈冕的访谈，记录了华人 Agent 创业者在应用上的探索。而姚顺雨的访谈，描绘的则是另一面：他在硅谷最前沿的 AI 实验室做 Agent 研究，他如何看待这波浪潮、模型与应用的边界，以及那些 Agent 逐浪者呢？

这是 **「语言即世界工作室」（** **Language is World Studio** **）** 成立后发布的第三篇长文访谈，姚顺雨很意外地从另一个角度帮我回答了，我们工作室创立的初心。

**为什么我们相信语言是这个世界的本质奥秘？**

他的表达是： **“** **语言是人为了实现泛化而发明出来的工具，这一点比其他东西更本质。** **”**

不得不说，姚顺雨的通篇表达有一种技术之美感，我个人十分喜欢。

**以下是对姚顺雨的访谈节选（作者进行了语言优化。）**

![Image](https://mmbiz.qpic.cn/sz_mmbiz_png/FNn39iboZZiapQKGy1NYC1P7fjLIXA3kOz80ibpIP3cg9UxBNFcrE3e23ib82TyAkkm5ck6GKrpib3ItWiaXm3pIibETg/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1#imgIndex=1)
- **本次访谈发生在2025年5月，访谈为个人观点，与所供职公司无关。**
- **本集的播客已在小宇宙、苹果** **Podcast** **、** **Spotify** **上线，相应视频在Bilibili上线。请搜索账号：张小珺商业访谈录。**

---

超 13000 人的「AI 产品市集」社群！不错过每一款有价值的 AI 应用。

邀请从业者、开发人员和创业者，飞书扫码加群：

![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/qpAK9iaV2O3vjfgc8R5F1VLq8aaBdbicwZFjaqzADFlxgtbXCBgZDs3cbiaX2G0Uc2icQBbAjAUKhibk9icLcg6F9u8g/640?wx_fmt=png&from=appmsg&wxfrom=5&wx_lazy=1&tp=webp#imgIndex=1)

进群后，你有机会得到：  

- 最新、最值得关注的 AI 新品资讯；
- 不定期赠送热门新品的邀请码、会员码；
- 最精准的AI产品曝光渠道

---

  

## 01

## 序

## 1.1 人：“我一直有这个非共识，我想要去做Agent”

**张小珺：我们今天的嘉宾是** **OpenAI** **姚顺雨，他的研究方向是** **Agent** **。前段时间顺雨写了一篇有名的博文《** **The Second Half** **》，告诉大家** **AI** **游戏已进入下半场。**

**这次节目我们第一次尝试有两位主持人，除了我还有大家熟悉的广密。**

**顺雨，我看了你的资料和你写的文字，从你的语言里读到一种反叛精神，我对你这个人很感兴趣。你能不能先给大家做一个自我介绍，聊聊你的经历？**

**姚顺雨：** 你说反叛精神？这很有意思。

我感觉我是个非常乖的学生。从小到大就是按部就班的学习。

本科从合肥考到清华，读姚班。在姚班大家会告诉你去美国读 PhD ，我就去美国读 PhD ，我在普林斯顿读 PhD 。读 PhD 之后很自然， OpenAI 是做 research （研究）最好的地方，就加入 OpenAI—— 感觉我前 28 年的人生，非常的乖。

**张小珺：你是** **15-19** **年在清华姚班，** **19-24** **年在** **Princeton** **，** **24** **年毕业进** **OpenAI** **。你在本科学的不是** **AI** **，是怎么进入** **AI** **领域，继而又进入** **Agent** **领域？**

**姚顺雨：** 姚班的传统偏理论计算机科学，但我可能有一点反叛精神吧。

当时，我觉得很多重要理论问题已经解决得差不多，比如将某个图算法的复杂度从 n 的 2.83 次方优化到 n 的 2.82 次方，这种改进在现实中意义不大。

我在 2016 年上李建老师的一门课，看到一个 multi-modal embedding （多模态嵌入）的 demo ，展示了 embedding （向量表示 / 嵌入）一个非常神奇的例子：比如用 “king” 的 embedding 减去 “man” ，再加上 “queen” ，结果接近 “woman” 的 embedding—— 这让我第一次意识到，深度学习在语义表示上居然能做到这么惊艳的 计算。

- **“king** **（国王）** **”** **的向量** **− “man** **（男人）** **”** **的向量** **\+ “queen** **（王后）** **”** **的向量** **≈ “woman** **（女人）** **”** **的向量。**

当时清华，尤其姚班，在 Deep Learning （深度学习）的老师和资源还比较有限。 2018 年，我按照姚班传统去海外交流，去了 MIT ，师从吴佳俊学长，我才真正系统性开始做 Deep Learning 。

最初我做的是 Computer Vision （计算机视觉），但渐渐意识到 Vision 很难实现通用人工智能。 **我的直觉告诉我：** **Language** **是一个更核心、更有潜力的方向，于是读博后转向语言模型研究。**

**张小珺：你是怎么进入** **Agent** **方向的？**

**姚顺雨：** 也算是某种机缘巧合吧。我的导师之前做过一些研究，探讨怎么在一个简单的语言游戏环境中训练智能体（ Agent ）。大概 2016 或 2017 年的工作。

那个项目是用一个基础 RNN 模型，在一个很小规模的文字游戏里，训练模型进行一些简单动态交互。比如，模型可以学会， “ 过桥之后就可以到河对岸 ”—— 这样简单的常识或逻辑推理。

我读博，本来是被计算机视觉（ Computer Vision ）录取，但我已经不太想做视觉了，主动去找语言（ NLP ）老师聊。

我遇到现在的导师 Karthik Narasimhan （普林斯顿计算机科学副教授），开始一起头脑风暴项目点子。我当时说：现在的语言模型，比如 GPT-2 ，已经比你们当年用的模型强太多，它们玩游戏是不是表现也会更好？

他说， maybe that' s a good idea 。我们就开始做了。

从那以后，我就一直做智能体相关工作，到现在 6 年了。

**张小珺：** **Agent** **或** **Language** **最吸引你的是什么？**

**姚顺雨：** 是它的可泛化性（ generalizable ）。绝大多数事，你都可以用语言表达。

我当时隐隐约约有个直觉：你如果真想去实现 AGI （通用人工智能） —— 那时还没人提 “AGI” 这个词 —— **但如果你真的想做一个非常通用的系统（** **general system** **），你就得去构建一个智能体。**

回头看 AI 历史，很久很久以前，从 Herbert Simon （赫伯特 · 西蒙）在 1960 年代开始，大家最早的想法就是要做一个 Agent 。当时大家的野心很大 —— 想用一个夏天搞定视觉，再用另一个夏天搞定语言，拼在一起，去做一个 Agent ，他就应该比人还聪明。

但这事太难了。 慢慢地， AI 变得非常碎片化。 大家研究的问题越来越小。比如，有的人研究视觉一小部分问题，有的人研究语言某个子任务，越来越细分，越来越垂直。

但到 2015 年之后，开始出现 Scaling Law （扩展规律），包括很多研究突破，历史上一些关键时刻也在提示我们： 也许我们应该从这种 “ 垂直式思维（ vertical thinking ） ” 重新回到更 “ 通用式思维（ general thinking ） ” ，再去尝试构建真正通用的系统。

**张小珺：当你进入** **Agent** **系统做研究，要让语言模型真正行动起来，你意识到最重要的几件事是什么？**

**姚顺雨：** **第一年最大收获是：要用** **GPT** **，不要用** **BERT** **。**

- **BERT：“来自Transformer的双向编码表示”，由Google AI在2018年发布的一种NLP预训练模型。**

可能现在很多人不知道 BERT ，当时语言领域最火的模型叫 BERT 。想法是：我有一句话，通过某种方式学到这句话的一个表示，通过这个表示做很多下游任务，比如做一些单选题，或者基于选择的任务。

当时 95% 的人做 BERT ，只有 5% 的人做 GPT 。这也是因为当时 NLP 的主要任务都是一些：我有一句话，这句话是积极的还是不积极的；我很讨厌这个电影，这是一个负面的句子。都是非常简单的事。这种事 BERT 确实效果更好。

但你会发现，如果你要做一个 language Agent ，你需要的不只是选择能力，而是去自由产生新动作的能力。

当然如果你在玩围棋，或者视频游戏，选择有限。如果你玩马里奥兄弟，他就是上、下、左、右。但如果你玩基于语言的游戏，动作是自由的。比如我在这个游戏可以用剑杀怪兽，或者我可以去第三个房间，或者我可以用金色钥匙打开第一个房间的门。 BERT 永远做不到。

世界的本质就是，你的行为空间是 open-ended （开放）的，这种在开放空间决策的能力 BERT 永远做不到。我发现这个之后，就再也没用过 BERT 。

**第二个** **learning** **是：任务或环境非常重要。**

当你有一个非常差的任务，你永远不可能学到非常好的东西。当时有很多人在做：这个句子是正面的还是负面的？ a 这句话能不能导致 b 这句话成立？当时这些任务看上去很难，现在看非常简单。

首先你要找一个足够有挑战的任务，这个任务能做出本质的新方法。 当时你想去做 Agent 或语言 Agent ，实际上没什么选择，只能去做文字游戏。

Zork 是个非常经典的文字游戏。你在一个基于文字的世界里，有点像一个互动脚本，可以往下走、往上走，可以去各个房间，可以做各种各样的事。

但你会发现，这个环境还是有很多缺陷，能学到的局限在这个环境，这个环境还是不够大。而且你如果用 RL 学这个环境，就会像用 RL 学传统的视频游戏，可以把这个游戏打通关，但对于其他任务没有迁移作用。你可以把围棋下得特别好，但对世界上其他事情没有价值。

我们需要一个更好的环境。

**张小珺：你博士期间的研究工作：语言智能体（** **Language Agent** **）、** **ReAct** **（浏览维基百科进行推理）、** **Reflextion** **（反思）、** **Tree of Thoughts** **（思维树）、** **digital automation** **（数字自动化）、** **WebShop** **（网上购物）** **——** **这些研究跨度很大，它们的共性问题是什么？你是怎么按着兴趣一步一步延伸的？**

**姚顺雨：** 从我的角度，是非常自然的过程。当我意识到环境有问题， 我第一个比较重要的工作是 WebShop ， **首先要解决环境问题。** 如果没有一个好的任务或环境，把这个游戏刷得再高，没有意义。

2015 年有一个非常好的工作叫 World of Bits （比特世界）。当时想法是，我们应该把电脑或互联网作为一个环境，这个环境比游戏更 exciting （令人兴奋）。但因为各种技术限制，没有做得特别好。到 2021 年，我和导师讨论，觉得这时可能是一个自然的时间点重新去做。

我当时也觉得，技术还没完全成熟，大多数人还在研究一些比较标准的任务： a 能不能导致 b ，或者翻译，或者从一篇文章回答问题。那个阶段想做互联网上的 Agent ，技术还没 ready （准备好）。但也正因为技术没成熟，反而是一个好的时间点开始做。到 2022 年，我们就做了 WebShop 这个环境。

2022 年， GPT-3.5 发布，还有后来 Chain of Thought （思维链）出现，带来新的方法层面上的机会。我们就做了 ReAct 这个工作。我现在还是觉得，我自己最喜欢的工作是 ReAct 。

- **ReAct: Synergizing Reasoning and Acting in Language Models，在语言模型中协同推理与行动，是一种让大语言模型在与外部环境交互时，同时进行“推理”和“行动”的方法框架。**

之后，基于这两个方向：一方面做更多方法（ method ），一方面做更多任务（ task ）。

但总体来说，我的研究有两个核心：

- 一是怎么去做一些有价值、和现实世界更相关的任务和环境；
- 二是怎么去做一些简单、但又通用的方法。

**张小珺：** **ReAct** **的提出标志了范式的变化吗？**

**姚顺雨：** 这需要 5 年或 10 年以后再去看。

当时学术界还不太能接受，我去做一个 prompting （提示工程），把它作为 research （研究）。传统意义上，你需要提出一些 fancy （花哨）的东西 —— 需要提出一些数学公式，训练一个模型，证明很多理论，或者做很多工程上的事。但如果你只是去用一个模型，感觉太软了。

不过， 当时最有价值的，就是去研究怎么使用模型。 如果你想训练模型，会落后 OpenAI 或这些公司好几年。你做的很有可能几年前别人已经发现了。如果你想做不一样的，可能怎么去使用模型更有价值。

**张小珺：为什么你做这件事情比大部分人都早？**

**姚顺雨：** 有幸运的部分，我 PhD 做的第一个事就是基于语言模型做 Agent 。当时做的人很少，因为它太难了，或者不是一个共识类的事情。当时共识类任务是做问答，做翻译，或者做一些已经被社区接受的任务。

**我一直有这个非共识：我想要去做** **Agent** **。**

**另一点是，我一直想做简单且通用的东西。** 我不想做一个很复杂、但只能在一个领域奏效的东西。 这个方向在传统意义上很难被接受，大家习惯了做 AI 的方式：把问题不停细分，做很多细分方法。

可能并没有多少人想做一个简单且通用的系统，或者认为这是可能的 —— 尤其 20 年之内。

  

## 02

## 系统

## 2.1 机器的手：“人最重要的affordance是手，AI呢？”

张小珺：今天我们的话题是 **Agent** **和强化学习，我们很好奇你会怎么定义** **Agent** **？**

**姚顺雨：** 这是一个很好的问题。要结合讨论背景看。

从自然语言处理的角度， Agent 是相对于一个只会生成文章或对话的系统而言。它能和外界交互，比如使用计算器、互联网，或调用各种工具。也就是说，不仅能生成内容，还能操作和互动。

但从更广义的 AI 背景看， Agent 是一个非常古老的概念。

**任何能进行自我决策、与环境交互，并试图** **optimize reward** **（优化奖励）的系统，都可以被称为** **Agent** **。**

从这个角度出发，今天我们讲的 Agent 更多是指：怎么基于语言模型这样的 foundation model （基础模型）去做具备自我决策能力的系统，而不是传统意义上基于规则或仅在某个领域用强化学习（ RL ， Reinforcement Learning ）训练出来的 Agent 。

因为 “Agent” 这个词在不同时代有不同定义 —— 你可以说 AlphaGo 是 Agent ，也可以说 Waymo 是 Agent ，甚至可以说机器人是 Agent 。这个词的意义很依赖具体情境。

**张小珺：你研究的** **“Language Agent”** **（语言智能体）和传统** **Agent** **，存在本质区别吗？**

**姚顺雨：** **本质区别是可以推理，因为推理才可以泛化。**

举个简单的例子，我做 ReAct 一个很强的动机是：我做完 colm ，我的第一个工作之后，在思考一个问题 —— 为什么我可以一下子去玩一个新的游戏，但现在这些系统或 AI 需要几十万步甚至几百万步训练，才能完成类似的事？

我发现，是因为我可以思考。我看到一个全新的环境，会想：这个灯是黑的，那可能有危险，基于常识可能有怪兽；我现在最重要的是点亮灯。基于之前的上下文（ Context ），灯在我后面，那我应该先向后走。

如果没有这样的思考能力，而是直接从复杂语言去预测 “ 我要往后走 ” ，就很难 —— 没有推理做不到。

**最大区别在于，语言模型提供了一个足够强的先验（** **prior** **），这个先验让你可以推理，而推理又可以在不同的环境间泛化。**

所以核心是推理能力，推理才能带来泛化。

**张小珺：从你的视角看，** **Agent** **是一个怎样的演变历程？它是怎么一步步发展到今天的？**

**姚顺雨：** 我可以说一下自己的理解，可能不完整，或者有一些错误。

最早的 AI ，我们称为 Good Old-Fashioned AI （符号主义 AI ），想法很简单：我注重的是推理，我怎么想，就把这些规则写出来，让 AI 也这么做。比如，如果温度高于 30 度，空调就应该降温。 这种基于规则的 AI ，可以造出很多早期智能体，比如最早的机器人、最早证明数学定理的系统，很多是这么做出来的。

但很快， 1980 年代，大家发现这个东西有瓶颈 —— 你不管写多少规则，还是很难覆盖这个世界上所有可能发生的情况。

那时符号主义走向极致，大家开始做专家系统：找很多专家，把这世界上所有可能的规则都写下来，是不是就能得到 AGI ？或者一个通用的、有用的系统？

但后来发现，无论你写多少规则，还是有很多特殊情况无法处理。这些规则只能用于一个任务。比如你写了一个诊断心脏病的系统，写了很多规则，但人千变万化，你没办法处理所有情况，这个系统也没法处理肺病。导致了第一次 AI 寒冬。

后来我们有了新的神经网络（ Neural Network ），也就是第二波 Agent 兴起，标志是 Deep Reinforcement Learning （深度强化学习）。 典型事件是 DeepMind 玩视频游戏、做 AlphaGo ， OpenAI 玩机器手、打 Dota 。

这一波核心是：我有一个虚拟环境，可以无限次尝试，有奖励机制，还有通用网络架构，我就像黑盒一样去学怎么 maximize reward （最大化奖励），它就变强了。

这个方向取得了很多成功，最有名的是 AlphaGo 。但还是有老问题：每做一个新环境，都要做很多 Environment-Specific （环境特定）工程。比如做 Dota ，要调很多参数（ parameter tuning ），做很多基于这个环境的工程。最大问题是：这些方法没法泛化。

你学了一个围棋 Agent ，没办法玩别的游戏。你在一个环境里学到的东西，没办法迁移到另一个环境。这肯定是不理想的。而且，如果你所有能解决的问题都在虚拟环境里，或者是像游戏那样可以无限次玩的环境，你就没法找到很好的真实世界应用。

第三波 Agent 是从大语言模型开始的。我们发现它可以做推理，而基于推理，就能进入一些新的环境，比如编程、互联网、各种数字环境。这些数字环境有一个共性：大多数都是基于语言的，需要推理。

这一次 Agent 的核心区别有两点：一方面是方法上，我们使用语言模型，用推理去构建能处理各种问题的 Agent ；另一方面是环境本身也发生了进化，从早期符号主义环境（比如数学定理），到下围棋、打游戏，再到今天互联网、编程、电脑操作这些更接近真实世界的数字环境。

**所以这是两条线：一条是方法线，一条是任务线。**

**大家可能更多注意到方法线，容易忽视任务线。但这两条线是相辅相成的。**

**张小珺：我一直有一个基础疑问。** **OpenAI** **提出的大模型能力分级从** **Level 1** **到** **Level 5** **，很多人都很熟悉了：**

- **Level 1** **是聊天机器人（** **Chatbot** **）**
- **Level 2** **是推理者（** **Reasoner** **）**
- **Level 3** **是智能体（** **Agent** **）**
- **Level 4** **是创新者（** **Innovator** **）**
- **Level 5** **是组织者（** **Organizer** **）**

**但这个五级划分的内在逻辑是什么？为什么是先有聊天机器人、推理者，然后才是** **Agent** **？** **Level 4** **和** **Level 5** **又是怎么来的？它们之间是递进关系吗，还是各自独立发展？**

**姚顺雨：** 逻辑是，首先你要有语言的先验知识。基于语言的先验知识，最早能做出来的应用是 Chatbot （ L1 ）。接下来，基于语言先验，你需要具备推理能力，这是 Reasoner （ L2 ）。

当你既有语言知识，又具备推理能力，才可能进一步做各种 Agent （ L3 ），尤其是能泛化的 Agent 。也就是说， Agent 建立在 Chatbot 和 Reasoner 能力之上。

很明显， **今天** **Agent** **发展最关键的两个方向：**

- **一个是让它拥有自己的** **reward** **（奖励），能自己探索；**
- **另一个是** **Multi-Agent** **（多智能体），让它们之间能形成组织结构。**

这两个方向，我觉得是正交，它们可以并行发展。

谁是 Level 4 ，谁是 Level 5 ，我不确定。但这两个事情是显然的下一步。

**张小珺：从** **Level 2** **到** **Level 3** **，也就是你做的这一步** **——** **从训练模型到使用模型，是一个很重要的跨越。**

**姚顺雨：** 或者说，是从单纯做推理，到把推理应用在 Agent 上，用它去和环境交互。

**张小珺：** **Agent** **目前有哪些主流架构？形成共识了吗？**

**姚顺雨：** 我的感觉是，大多数时候大家用的还是类似 ReAct 架构。你能够去推理，然后你可以产生 action （行动）。这是最简单的一种形式。但最简单的反而是效果最好的。

当然，基于不同任务，大家会设计很多 workflow （工作流）或更 specific （特定）的方法。但如果说最通用、适配性最强的方案，我还是觉得是类似 ReAct 的方法。

**李广密：提升** **Agent** **能力，你自己最看重的是哪几个关键能力？**

**之前有人提到** **Context** **（上下文）、** **Long-Context Reasoning** **（长上下文推理）、** **Tool Use** **（工具调用）或** **Instruction Following** **（指令遵循）。你刚才一直强调** **Reasoning** **（推理），那如果要提升** **Agent** **的能力，你最在意哪些能力维度？**

**姚顺雨：** 这是个很好的问题。我觉得现在没有一个特别成熟的 taxonomy （能力分类体系），或划分系统。每个人都有自己的理解方式。

有些人会按照工具划分，比如 coding （编程）能力、上网能力、使用计算机的能力，这是一种划分方法；另一种是按照模型自身的能力划分，比如多模态处理、长上下文处理、推理能力 —— 这两种划分都有道理。

但就我现在看，我最看重的是 Context （上下文）处理能力，或 Memory （记忆）能力。因为只有在这个基础上，才能进一步实现 Lifelong Learning （终身学习）或 Online Learning （在线学习）的能力。

**李广密：你刚才一直在提环境，你认为** **code** **代码是一个实现** **AGI** **最重要的环境吗？它可以支持多轮的强化学习（** **RL** **）、提供闭环反馈，也可以验证结果。如果我们在代码这个环境上构建** **Agent** **，会不会发展更快？**

**姚顺雨：** 毫无疑问，这是最重要的环境之一。

**Code** **有点像人的手。**

它某种程度上，是 AI 最重要的 *affordance* （环境给予行动者的可能性）。

对于物理世界， **人最重要的** **affordance** **是手** —— 我们围绕它制造各种工具，比如锤子、笔、筷子。 **但对** **AI** **、对** **Digital Agent** **（数字智能体）来说，最重要的** **affordance** **可能就是** **code** **。**

因为其他东西，都是给人定义的。比如网页、小说、视频，是为人类设计的；但 code 是一个天然就给机器使用的表达形式。

我 2022 年一直在想：做 Coding Agent 明明是很重要的事，为什么没人做？

我们当时做了一个工作叫 InterCode 。大家都在做的是：给一个 coding task （编程任务）模型生成一段代码，然后你去 evaluate （评估）它。但我们就在想：为什么不把执行结果反馈给模型？

我们可以让它变成一个多轮 Agent task （智能体任务），构造成一个环境，而不是单次完成的任务。基于这个，我们后来做了 SWE-bench 、 SWE-Agent 。

- **SWE-bench是一个真实世界的软件工程基准，用GitHub上的issue和修复代码来评测模型的代码修复能力。**
- **SWE-Agent是一个基于大语言模型的智能体，能在SWE-bench上自主阅读代码库、修改代码并运行测试来解决问题。**

有时候，很有意思的一点：一个东西明明非常重要，但就是没人做。如果你是一个研究员，觉得你做的事很重要，但别人不觉得、也没人做，并不是坏事 —— 可能它真的很重要，只是大家还没开始。

**李广密：这里有个很强的非共识：有的人觉得** **code** **是这一轮技术革命最大的价值体现，但也有人觉得可以泛化到更多任务里，在电脑、手机、数字世界中都可以实现，** **Agent** **操作人能做的** **95%** **、** **99%** **任务。**

**你对从** **code** **到数字世界这一步的跨越，或者它的泛化，是有信心的吗？**

**姚顺雨：** 更广义说， 你可以认为 API 也是 code 的一部分。 任何基于 code 的接口，都属于 code 环境的一部分。

有个非常经典的 debate （争论）：最终的 AGI ，是基于 API 或 code 的？还是基于 GUI （图形界面）？或者是为人定义的前端环境？还是它是一个混合体（ mix ）？

这个问题有点像：你是想改造你的车让它适应所有路，还是改造所有路让它适应现在的车？

很多时候，现实中并没有现成的 API ，只有 GUI 。但你可以人为为它构造一个 API 。

当然，最终结果很可能是 meet in the middle （在中间相遇），两边都会做，而且这个事情可能没那么难。

现在看，让一个 Agent 既能使用 code ，又能操作人类界面的 screenshot （截屏）、前端，两者兼顾也没那么困难。从这个角度说，让 Agent 像车一样能适配各种路，比起要改造所有路让它们都有 API ，要容易很多。

Coding 肯定很重要，但如果让 Agent 也能操作 GUI ，最终 Agent 很可能是 “ 什么都能做 ” 的。

## 2.2 任务的设定：“我们对简单任务的robustness没有重视”

**张小珺：你** **4** **月发布博文《** **The Second Half** **》（下半场），你是怎么想到** **the second half** **这个** **idea** **的？受了什么启发吗？**

**姚顺雨：** 我是受邀去斯坦福一门课做 talk ，当时想，能讲点什么？没法讲太技术，只能讲更哲学的内容，就想到这个话题。

这个想法来自我在 OpenAI 的工作经验，以及之前做 research 的感悟。大家过去往往更关注模型训练、方法设计， 但我觉得 **现在的** **bottleneck** **（瓶颈）已经转移了：变成怎么去定义好的任务，怎么去定义好的环境。**

**张小珺：现在是处在那个转折点吗？从上半场到下半场。**

**姚顺雨：** 主线正从 “ 上半场 ” 转向 “ 下半场 ” 。我说的主线是基于语言的智能体。 当然你也可以说，在 Audio （音频）、 Multimodal （多模态）、 Robot （机器人）这些方向，还有很多未解的问题。

但我觉得，从语言出发，去定义 Reasoning （推理）、定义 Agent ，我们终于有了一个非常 general （通用）的方法，而且这个方法是可泛化的 —— 我们实现了一个基点时刻 **。**

这带来一个本质变化：以前我面对很多怪兽，需要造出各种不同武器去打它们；现在我有了一把通用武器，比如机关枪，我不需要再为每个怪兽单独造武器。接下来要思考的问题就变成：我该朝哪个方向开枪？

现在方法的问题已基本解决，真正重要的是 —— 我们要用这个通用方法，解决什么问题？

**李广密：怎么设定任务？怎么定义问题？关于这个，你在探索过程中有什么思考吗？**

**姚顺雨：** 不同的人有不同的 flavor （风格）， 我从很早就有一个偏好：我想定义一个基于结果的 reward （奖励），而不是基于过程的；而且这个 reward 应该是基于规则、可计算的，而不是来自人的偏好、模型的偏好，或者一些黑盒指标。

我们做 WebShop 的时候，最困难的一点是，怎么定义 reward 。我觉得做任何 RL （强化学习）任务最难的不是建环境，而是怎么设计 reward 。你当然可以把 Amazon 或 Facebook 模拟出来，工程上确实很难，但总是可以做。但最难的，是怎么设计一个既有难度，又有实际价值，同时又有一个好的 reward 的任务。

我希望这个 reward 是不 noisy （不噪声大）的，是可解释的，是白盒的（ white-box ），不是那种黑盒的东西（ black-box ）。

事实证明，这也是现在 RL 成功的关键。像 math （数学）和 coding （编程）这种任务，之所以能做出来，核心就是：

- Reward **是基于结果，不是基于过程；**
- **Reward是白盒的、基于规则的，不是基于人的偏好或模型的偏好。**

比如，一个数学题答案是 3 ，它就是 3—— 只要你得出的是 3 ，就是对的；不是 3 ，就是错的。

但如果你 reward 是基于过程，就会出现 hacking （投机取巧）。你去优化人的偏好、模型的偏好，也会出现 hacking 。比如你生成一段非常优美的代码，但它并不解决实际问题。

我后面做的很多 task ，也都是用同样的 filter （筛选标准）。

比如 SWE-bench 这类工作：

- 第一，它是结果导向，而不是过程导向；
- 第二，它的reward是基于规则、白盒的，而不是来自人或模型的主观偏好。

**张小珺：就像上面说的，** **OpenAI** **有** **5** **个分级。如果从任务定义出发，是不是也可以做出一套产品能力的分级？随着模型能力溢出，我们开始使用这些能力，** **Agent** **能力可以怎么分级，你脑海中有没有一个初步的框架？**

**姚顺雨：** 我现在倾向于认为，不同类型应用会带来不同 challenge （挑战）。这些挑战是正交的，很难说哪个更难、哪个更简单。

人类也有这个问题 —— 洛克菲勒和爱因斯坦谁更厉害？很难定义；成为一家大公司 CEO 和成为一个数学家，哪个更难？只是不同的挑战类型。

而对于 Agent ，另一点是：人觉得很简单或难的事情，对 Agent 可能不是那样。

人觉得做客服比做软件工程师简单很多，工资也低、文凭要求也低。但现在反而做软件工程对 Agent 更容易。因为软件工程有更好的环境、更清晰的 reward 、更大的数据量，等等。但你想做一个特别 robust （健壮）或 reliable （可靠）的客服，反而更难。它涉及复杂的 reliability challenge （可靠性挑战）。

我们当然可以把人类工作分成不同的 category （类别）。 但对 AI 来说，人类觉得难或不难的任务划分，不一定直接映射到 AI 的能力上。

**张小珺：整体来说，什么样的任务适合** **Agent** **做？什么样的任务适合人和** **Agent** **一起做？什么样的任务适合人做？**

**姚顺雨：** 我现在感觉任务大概可以分成几类。

一类任务更注重 **reliability** **（可靠性）** 。 你做客服，重要的是： 100 次里你需要 99 次甚至更多不能出错。你只有 85 次让用户满意，还有 15 次不满意，可能被炒鱿鱼。这种任务比较简单，但需要极高稳定性。 Agent 就需要特别强调 reliability 。

另一类任务更注重 **creativity** **（创造力）** 。 你去证明黎曼猜想，或者写一个复杂程序，或者创作一部文学剧本。这类任务允许你失败很多次，只要有一次做得特别好，就算成功了。这是非常不一样的挑战。

还有一种划分方式是：看任务的深度和广度。

有些任务像 Cursor （一个代码编辑工具），是非常短的 loop （循环）。我只需要把一个文件改一下，可能 3 秒就完成。但也有一些任务需要 30 分钟、 3 小时，甚至 3 天。这种任务需要的是 Long-Term Memory （长期记忆）的能力。

再比如，从任务的广度看，我只是去解决一个具体 bug ，这是比较窄的问题。但如果我要从 0 搭建一个像 Windows 这样的操作系统，这是一个非常广的任务。你可以说这是一个人能做的事情，一个小组能做的事情，还是一个公司才能做的事情？从这个角度，我们也需要做更多 motivation research （动机建模研究）。

**张小珺：哪些任务对于** **Agent** **是相对更好定义的？从易到难的顺序应该是什么？**

**姚顺雨：** 我们可以平行做很多不同事情。有一个简单的设计评估指标（ metric ）方法。

在 coding 任务中，我们传统有一个评估指标叫 Pass@k ，意思是：你对同一个代码生成任务，最多尝试 *k* 次，其中起码有一次的成功概率是多少？你可以想象，当这个 *k* 越来越大，系统被使用的成功概率也会变大。

很多时候做 coding 相关研究，它会 report （报告）的是 Pass@100 ，也就是：同一个任务你跑 100 次，起码成功一次的概率是多少？

但我们 2024 年发了一个研究，叫 TAU-Bench （ Tool-Agent-User Benchmark ，工具 – 智能体 – 用户基准测试），想法是：对于另一类任务，比如客服，我们需要一个刚好 **相** 反的指标，我们把这个指标定义为 Pass^k 。也就是：每一次都成功的概率是多少，或者失败一次的概率是多少？

有些任务我们需要优化的是 Pass@k （多次尝试中至少成功一次），而另一些任务，比如客服，我们需要优化的是 Pass^k （每次都成功），或者我们最关心的是 Pass@1 （一次就要成功）。

但是， **现在我们对于简单任务的** **robustness** **（稳健性）并没有特别重视** —— 这是因为大家做 AI 还是在做一些 benchmark （基准任务），而不是实际应用。

但如果你接受了这个 mindset （思维）转变，很自然你就会意识到：有些应用是需要特别强调 robustness 的，那你就需要去优化它的 robustness 。

现在大家还没完全意识到这件事；但我相信，如果大家意识到这个转变，会带来很大进步。

## 2.3 泛化的工具：“语言是人为了泛化而发明出来的工具”

**张小珺：你有一句非常** **high level** **（抽象）的总结：语言通过智能体中的推理实现了泛化。这里的泛化是一个已经被证实的，还是一种推断？**

**姚顺雨：** 为什么语言非常独特？因为它是人在这个世界完成各种各样事情的工具。

**语言也是人类发明的工具，像火或笔一样。** 但它之所以特殊，是因为它是一个帮助你解决任何事情的通用性（ general-purpose ）或泛化性（ generalizable ）的工具。

当你学会了这门工具，你可以去做很多新任务。比如你学会了攀岩，它帮不了你完成新任务。但你学会了语言，你可以通过语言和人交流，学习、思考、推理。

2020 年以前，大家没把这个事想清楚，觉得语音、文字、图像、视频都是一些数据，没什么区别。 但我觉得最大区别是： **语言是人为了实现泛化而发明出来的工具，这一点比其他东西更本质。**

**张小珺：这里说的是语言具有泛化能力，那么强化学习终于具备了泛化能力，这是一种推断还是一种结论？**

**姚顺雨：** 可以说是我个人观点，当然很多人在讨论。泛化与否，本质上是一个 spectrum （谱系）问题，是一个相对概念，不是绝对的 0 和 1 。

我之所以这么说，是因为在此前，如果你在一个特定环境上训练，模型只能在这个环境表现良好，不能轻易迁移到其他环境。 但现在，你在一个环境上训练，模型可以适应更多不同环境，这才是最本质的区别。

DeepSeek 大家觉得一个有趣结果是：你在数学和编程领域用强化学习训练模型，但它在创意写作上也变得更强。

这体现了本质区别： AlphaGo 只能下围棋，不能下象棋；而现在你学会数学，也能提高创意写作。

**李广密：我读你的文章，印象最深的也是，你提到** **RL** **终于泛化了，是真的泛化吗？** **——** **你刚才也说，有很多先验知识已经** **train** **（训练）到** **model** **（模型）里头了，有什么迹象让你感觉是真的泛化了，而不是** **training data** **（训练数据）里面就包含这些数据？**

**姚顺雨：** 对，我觉得是有可能的。如果你的 Pre-Training （预训练）已经包含了所有事情，那么 RL （强化学习）只是激发出这些能力的 skill （技能）。

事后想起可能是 Ilya （ OpenAI 前首席科学家），还是谁，说过一句话，意思是： M **aybe the ultimate generalization** **（也许最终的泛化），就是你去** **overfit** **（过拟合）现实。** 如果你能把剩下的所有事情都做完，那么讨论它是过拟合还是泛化就不重要了。

但我觉得，它还是泛化的。原因是它能够推理。当你能在一个环境学到如何思考的技能，并且这种思考能力能迁移到新环境，这才是泛化的本质原因。

**李广密：训练某一类游戏变强，能泛化到其他游戏也都很强吗？比如，一个模型打** Dota **（多人在线战术竞技游戏）非常强，是不是在所有游戏里都很强？**

**姚顺雨：** 不好说。即使是推理，它在不同环境的泛化能力也可能不一样。比如， 基于逻辑的推理，可能从数学到编程的迁移更容易；基于人情世故的推理，可能在另一类任务上迁移得更好。

但重要的是，现在终于有可能出现一个单一模型能够做所有任务。之前认为这不太可能，但现在是有可能的 —— 你可以在很多不同任务上做强化学习，而且它能迁移到更多任务。

当然，如果只考虑任务与任务之间的迁移， 迁移程度和任务本身的性质有关系。

**李广密：代码和数学之所以容易泛化，你有想过背后的原因吗？是因为他们有思考过程？**

**姚顺雨：** 只是因为它是最早开始做的。 它之所以最早开始做，是因为它相对简单，有一个很好的 reward （奖励信号），不需要复杂环境，它本身就是推理。

现在看，很多其他任务也是可泛化的。只是我们一开始做的是这个任务，所以，大家对这个方向的讨论比较多。

## 2.4 奖励的机制：“当AI玩一个语言游戏，要怎么定义内在激励？”

**张小珺：基于基础模型往上长，** **Agent** **生态树在你脑海中，会是一个怎样的结构？**

**姚顺雨：** **一个方向是：** **fundamental research** **（基础研究）怎么演变？或者说，方法怎么演变？**

**另一个方向是：应用，或者它的交互方式（** **interaction** **）有怎样的演变？**

这两个方向之间有关联。但它们需要不同的人去探索不同的方向。比如 Cursor 并没有在 fundamental research 上做什么创新，但做了交互方式上的创新。

在 fundamental research 上，比较重要的有三方面：

- 一个是 **Memory** **（记忆），**
- **一个是** **Intrinsic Reward** **（内生奖励机制），**
- **还有一个是** **Multi-Agent** **（多智能体系统）。**

这也跟 OpenAI 提出的 Innovator （ L4 、创新者）和 Organization （ L5 、组织者）框架很像。

你作为一个 Innovator ，首先你需要一个 Long-Term Memory （长期记忆）。

比如，我是 Wiles （安德鲁 · 怀尔斯，数学家），我研究费马大定理，可能花了 20 年。我就需要一个长期记忆。

我有这个长期记忆还不够，还需要有 **内在的** reward 。因为在你真正证明那件事之前，没有任何外部奖励 **（** Extrinsic Reward **）** **——** 你没有获奖，没有做成任何 “ 可交付 ” 的事情，也没人给你 feedback （反馈）。你需要自己给自己反馈。

这是所有 Innovator 最重要的。无论你是艺术家、科学家、文学家，还是任何类型的创作者，对吧？

另一方面，作为一个 Organization （组织），你需要解决的问题是： Agent 和 Agent 之间怎么协作？怎么让 Multi-Agent （多智能体）协作 scale （规模化）？

现在的 Agent 就像一个普通大学生，做一个数字化的实习生。或者说， AGI 就是一个普通一本大学生在电脑上能做所有事情的一个能力。

但是，人类社会的边界是什么？ 这当然覆盖 80% 或 90% 的人。但我们最崇拜的人，是哪两种？

- 一种是创造新东西，在认知或审美上开创新领域的人：爱因斯坦、高更、梵高、贝多芬；
- 另一种是能创造新组织、伟大组织的人：伊隆 · 马斯克、乔布斯。

很自然， **个体的创造力和组织的协作能力** **——** **都非常重要。**

**张小珺：为什么** **OpenAI** **分级的最后一级是组织者（** **L5** **）？**

**姚顺雨：** 我一开始是认为 Innovator （ L4 ）和 Organization （ L5 ）是更正交或并列的关系。

我当时在群里问了一个问题：当一个大公司 CEO 和一个科学家，到底哪一个难？

这个不好说，实现路径有区别。所以，不用太纠结谁是第四级，谁是第五级，都很重要。不一定要先实现哪一个才能实现另一个，可以同时去探索。

**李广密：这中间有几个关键的问题要突破，比如长期记忆，这是短期可预期突破的吗？**

**姚顺雨：** 也许吧。当然也取决于多短期？但我觉得当它足够有价值，它必然会突破 —— 如果你对技术是乐观的。

**李广密：长期记忆，你要展开讲一讲吗？**

**姚顺雨：** 我不知道我能分享多少， 但我的信念是 —— 是 Utility （效用）的问题。

为什么我们现在的模型，推理很强，考试很强，玩游戏很强；但它还没创造出足够经济价值？ —— **根本原因是：它没有这些** **Context** **（上下文）。**

人类社会比较 tricky （复杂微妙）的一点是：当然，我们确实写下了很多东西 —— 我们用文字、 Google Doc 、 Notion ，记录了很多东西；但很多 Context 永远只存在人的大脑，是通过一个分布式的系统来维护。

比如，你老板跟你之间的行为习惯，或者一些很难用语言总结下来的信息。这些 Context 存在于人的脑海里。人没办法把这些东西全部写下来。

这就导致 —— 人是不可或缺的。

**只有人有这样的能力：进入一个环境，获得这个环境里的** **Context** **。**

如果这个问题解决了， Utility 问题就可以在很大程度被解决。这个世界，大多数人并不是乔布斯，也不是爱因斯坦，只是一个普通人。他的数学推理没有 o3 强，但他能 manage Context （管理上下文）。

他去一个公司 7 天，除了在文件上看到信息外，脑子里也积累了 Context 。而这些 Context 是 o3 没有的。虽然他没有 o3 聪明，但因为他拥有 Context ，他做得比 o3 好。

**李广密：有可能我们很快就会看到最强的软件工程师，甚至** **2027** **年看到能操作人类电脑、手机上几乎所有任务和指令的通用** **Agent** **，你对这一天的想象是怎样的？是过于乐观还是比较合理？**

**姚顺雨：** 现在还没有 well-defined （明确定义）。现在模型写代码的能力超过世界上几乎所有人，或者说，它的数学和逻辑推理能力，也比大多数人强。但是，当你说它能不能很好使用环境，关键还是看你让它做什么任务，这个任务能不能被合理定义。

很多时候，人类最难的问题不是推理本身，而是获得完整 Context （上下文）。

**现在模型的** **bottleneck** **（瓶颈）不是缺少推理能力，或者写代码、使用前端的能力，而是缺少一个完整的上下文。**

我不知道这是 Intelligence （智能）问题，是产品问题，还是别的什么问题 —— 但如果想让 AI 真正发挥价值，这个问题必须解决。

**李广密：你刚才提到另一个关键点：模型或** **Agent** **要有内生奖励系统。今天是不是还没有这样一个系统？如果我们真的要赋予它内生奖励机制，是不是在它持续自主学习中，就可以改动自己的模型权重，从而更聪明？**

**我们离这一步还有多远？**

**姚顺雨：** 我不知道。我觉得会有这一天，但很难预测时间。

当然，它自我提升的方式，也许是改变自己的权重，也许是拥有一个基于语言的长期记忆，也许是一个基于 Embedding （向量表示）的长期记忆，或者其他形式的记忆机制。但我相信，它会自我提升。

**李广密：内生奖励，你能讲讲吗？**

**姚顺雨：** 就像我刚刚说的，很多创新者之所以能在没有外在激励的情况下坚持，是因为他有内在的价值观或激励机制。

这个问题， AI 和神经科学已经研究多年。 婴儿是最典型的例子。 他们拥有基于好奇心 **或** 自我激励的机制。很多婴儿会反复玩一个玩具，用嘴去咬一个东西，或者做一些看似 “ 无意义 ” 的动作。

你说他获得了什么 reward 吗？他没有升职加薪，没有拿到钱，没有任何外在激励 —— 他只是好奇。他的动机是： “ 如果我做这个事，我会有什么样的感觉？ ” 如果这个感觉是新的、不同的，他就可以从中学习。

**张小珺：他可以获得安全感。**

**姚顺雨：** 对，就是说，好奇心、掌控感、安全感，是一些内在动机。正是这些东西驱动了人去做某些事。否则，很难从纯粹理性角度解释：他为什么要做？

但有意思的是，当人长大之后，会发生重要变化。 当你是婴儿，你对世界的理解，是基于视觉、触觉，基于物理世界的。你学习的是，怎么把触觉、听觉、视觉，以及对骨骼系统的控制结合起来。

当你长大之后，你对世界的理解方式变了，变成一个基于语言、推理、文字系统的理解。你开始思考：这个世界是怎么运作的？我怎么才能开一个公司？怎么才能升职？怎么才能做成一些事情？

**你玩的，不再是一个物理游戏，而是一个文字游戏。**

在这个文字游戏里，当然也存在内在激励，但又好像和婴儿时期的好奇驱动不太一样。

这是 AI 面临的挑战：传统 AI ，比如玩迷宫、做机器人仿真，它可以定义一些基于世界模型或者模仿婴儿阶段好奇心的内在激励。

**但当** **AI** **在玩的是一个语言游戏，要怎么定义内在激励？** —— 这个问题就变得不太一样了。

**张小珺：你在文章也说，我们忽视了任务评估标准的重要性。应该怎么去评估？** **——** **比如，我们怎么去衡量一个** **Agent** **？有哪些北极星指标？**

**姚顺雨：** 还是要思考怎么去创造更多现实世界的价值。

当然这个事情在不同领域、不同应用下，有非常不同的任务设计、方法和路径。但有一个大趋势是：应该更多去思考实际价值，而不是这些被设计出来、类似考试或游戏的东西。

我们发现，一旦你可以定义一个考试或游戏，离它被解决也不远了。

甚至你可以说，世界之所以难，是因为它不是一个被设计出来的东西。考试和游戏有一个很大特征是：它在被设计的时候，就已经有一个很好的 reward 或标准答案。

但当你已经有一个非常好的 reward 或标准答案，再加上现在已经有一个 general recipe （通用解法），那这个事情离被解决也不远了。

而真实世界的问题是：它没有标准答案，没有标准的 reward function （奖励函数）。很多时候人做事情，也不一定是为了一个理性的 reward ，但人还是去做了。

**张小珺：它是开放的。**

**姚顺雨：** 对，现在主要问题是这个。最大问题不在于，我有没有一个 well-defined （明确定义）的答案，而是我怎么找到它。

**张小珺：我们未来还需要更多地推翻各种各样的基本设定吗？**

**姚顺雨：** 我觉得需要。

人类一直在做这件事，不是吗？

  

## 03

## 吞噬的边界

3.1 双刃剑，“ 创业公司最大机会是：设计不同的 interface”

**张小珺：你知道，应用型创业公司很担心，大模型公司的模型能力溢出，会把他们做的** **Agent** **吞掉。**

**长期看，** **Cursor** **这样的公司，壁垒是什么？哪些** **Agent** **是模型公司必然会做的？哪些有创业公司机会？** **——** **边界可能在哪？**

**姚顺雨：** 创业公司应该担心的是模型没有溢出能力，这样你就真的什么都做不了了。有溢出能力是个非常好的事情，这几乎意味着你有机会。

**创业公司最大机会是：能设计不同的** **interface** **（交互方式），或者说人和数字世界交互的方式。**

ChatGPT 或所有做模型的公司，都在做类似 ChatGPT 的产品。 ChatGPT 的本质是：你是在像和人交互一样去进行和数字世界的交互。

你的 Chatbot 是像人一样的东西 —— 你和他聊天，给他布置任务，让他帮你做 Deep Research （深入研究）或者写代码 —— 交互方式是像人，或者像助手一样的交互方式。

如果你能用模型通用能力，创造不同的交互方式，就能创造巨大的机会。

**最终，可能模型的能力会产生** **beyond ChatGPT** **（超越** **ChatGPT** **）的交互方式，变成** **Super App** **（超级应用）。**

如果你做旧的 interface ，你利用这些新的模型，很容易被 ChatGPT 取代。如果你的交互方式很像 ChatGPT ，你有什么理由不被 ChatGPT 取代？如果你做的是新的交互方式，但模型没有继续变好、没有新的溢出能力，也很难做。

**对于创业公司，最好的机会是：你做新的交互方式，并且模型不停有新的溢出能力，让你能够赋能这些新的交互方式** **——** **两者缺一不可。**

**张小珺：但是** **ChatGPT** **也可以跟进这个新的交互方式。**

**姚顺雨：** 对。 **但拥有一个** **Super App** **对于公司是双刃剑。**

当你已经有了一个交互方式，你必然形成路径依赖。就像 2020 年 Google 有无限多资源和钱，有 Transformer ，但它最自然的想法是：我怎么用这东西提升搜索引擎？

当你有像 ChatGPT 这样的 Super App ，很自然你的研究就会 center around （围绕）这个 Super App ，会 center around 这个交互方式。

你会探索新的产品，但即使是大厂，即使是谷歌，即使是 OpenAI ，大部分资源还是会围绕你 Super App 的交互方式 —— 所以，这是创业公司的机会。

**李广密：你刚才提到交互方式，今天还是人跟** **code** **交互、人跟** **text** **交互，那人跟** **Agent** **未来是怎么交互？你感觉** **Her** **会是一种正确的交互方式吗？如果这种交互奏效，有没有机会** **beat** **（胜过）** **ChatGPT** **今天的形态？**

**姚顺雨：** Her 是不是还是类似一个 Assistant （助手）的形态？只不过它有语音而不是文字？

这是一个显然很有价值的形态，人和人交互已经几千年、几万年、几百万年，这是对人最自然的形态，肯定是最显然的 Super App 。

但这个生态位，我觉得 ChatGPT 是站住的。模型公司一开始做的就是这个。

那我觉得不显然的是：我能不能基于不像人的交互方式？

Cursor 是很好的例子，创造了一种新的交互。不是像人一样的交互，而是像 Copilot （副驾驶）。写代码的时候，它能给你提示或编辑。没有人和人是这样交互的。这是它的价值所在。

Google 也是很好的例子。雅虎是一个更像黄页、更让人熟悉的交互。但谷歌是一个让人不熟悉的交互，很奇怪。

Assistant 、 Her ，或者像人一样的交互方式，显然是最重要的交互方式之一，但还是会有足够多的机会，诞生新的交互方式。

**张小珺：你脑海里有没有一些新的交互？非** **ChatGPT** **在探索的形态，也非传统互联网的交互，在你脑海里有吗？**

**姚顺雨：** Canvas 是一个好的尝试，可以基于现在的任务，在线生成最符合情境、个性和任务的前端。这是值得探索的方向，但也很难。

**李广密：在你看来，应用公司的数据飞轮，对他们来说重要吗？或者说，在什么环境下才能形成？**

**我感觉，** **Chatbot** **产生的是偏好数据，好像没什么数据飞轮；** **Code** **可能有思考过程的数据，这种思考过程的数据代表一类能力，可能是有用的；像** **Canvas** **也好，** **Artifacts** **也好，可能是有思考过程的数据，这类可能有机会形成很强的数据飞轮效应。**

**姚顺雨：** 大多数公司还没有形成数据飞轮；他们依赖模型变好，利用模型变好的溢出能力。

如果你要有数据飞轮，首先你要能自己去训模型，并且能通过交互有很好的 reward ，使你能把好的数据和不好的数据分开。

比较成功的是 Midjourney ，有非常清晰的 reward—— 人更喜欢哪张图，这个 reward 和应用是对齐的， reward 做得更好，公司就更成功，模型也更好 —— 一切都对齐。 有了这种情况，才能自己训练模型，做数据飞轮。

这个过程必须比较非主线。 因为如果很主线，我也可以通过 Pre-Training 或 RL 提升能力，靠泛化或其他方式。

总的来说，大部分公司目前还没有形成飞轮。

3.2 对 **Agent** **创业者的思索：“** 这世界是相互抄的关系，而不是单向抄的关系 ”

**李广密：在你看来，** **Agent** **创业者一定要有研究背景吗？**

**姚顺雨：** 不好说，挺看人的。很难把人简单分成 research 和非 research 两类，没那么泾渭分明 —— 人与人之间的差异很大。

可能最重要的一点， 还是得找到 value （价值）。 不管你叫它 product-market fit （产品与市场契合）、产品的 sense ，还是别的 —— 找到真正有价值的东西最重要。 技术只是手段，目前最重要的是解决问题，需要找到一个好的问题。

如果你有很强 research 背景，比如自然语言处理，反而可能是坏事 —— 因为你会对技术太执着，拿着锤子到处找钉子。

Cursor 创始人是四个本科生。 Perplexity 创始人是研究员出身。真的挺看人的，跟你是否做过 research ，没有那么强相关性。

**张小珺：好的** **AI** **产品经理应该长什么样？**

**姚顺雨：** 好的 AI 产品经理就是一个好的产品经理，可以第一性思考。 AI 是变化很快的， 相对不变的是人、人性、人的需求。这变化得更慢。

你能找到一个好的需求，从第一性原理反推：要把它做成，我需要应用什么样的技术？

**张小珺：你怎么看** **Manus** **、** **GensPark** **这些产品和他们的创始人？**

**姚顺雨：** 我试过 Manus ，还没试过 GensPark 。 Manus 挺有意思，给我一些启发。他们产品 sense 很好，有打磨产品的基因。

**张小珺：这个产品应该是** **OpenAI** **主线上的产品对吧？**

**姚顺雨：** Emm……You will see 。

基于 Manus ，我再讲一点。传统大家认为发生的事情是：我大厂先做出来一个东西，创业公司就可以开始抄。比如做出 ChatGPT ，我可以去抄一下 ChatGPT ，做一个类似的事情。

但现在，似乎反过来也可以成立。 可以先小厂做一个事情，它创造出来一个交互的创新或者产品的创新，做模型的公司也可以去借鉴或者应用。

这点还是挺有意思。很多时候大家会说，模型做得越来越好了，是给创业公司做嫁衣了。因为你创造很好的模型，如果没有自己运用特别好，这些创业公司就用好了。

但也可以反过来，如果你创造一个非常好的交互，但没有能力把模型或底层能力做特别好，大公司也可以借鉴你的交互，再加上它的模型能力，做得也特别好。

**这世界是个相互抄的关系，而不是一个单向抄的关系。**

**李广密：如果你是** **Manus** **创始人、** **CEO** **，你今天要走向垂直方向吗？**

**姚顺雨：** Manus 的一个价值是，它给人非常 general （通用）的感觉。但我觉得， **有一个非常通用感觉交互方式的** **Agent** **，和你有一些** **Killer App** **（爆款应用），是不矛盾的。**

一个比较理想的情况，你有一个非常通用的交互方式，这个交互方式想象力足够大。比如 Cursor ，虽然它是 IDE （集成开发环境），如果它只做 IDE ，想象空间是有上限的，就在 IDE 里面。但如果你做一个非常 general 的产品形态，比如 Manus ，想象空间是很高的。

但并不矛盾的是，你可以有每个阶段的 Killer App 。 比如它做 PPT 特别好，做 Deep Research 特别好，或者做其他东西特别好。

iPhone 或 iPad 是非常通用的产品形态，但它一开始，都有一些 Killer App 支持它有 momentum （增长动能）。包括 ChatGPT ，包括微信，很多伟大产品都这样。

你有一个足够通用、简单，或第一性的交互方式，它有很多想象空间。但你去维护它，或者设计路径的时候，你能有各种各样的应用，使它不停地增长。

**张小珺：你听了我和肖宏（** **Manus** **创始人）的播客，有什么感觉吗？**

**姚顺雨：** 我觉得挺有意思。印象最深刻的是他说， VC 是一个非常贵的融资方式，不是在你不好的时候，而是在你好的时候。他有很多挺不一样的思考问题角度。

**张小珺：** **2025** **年过年** **DeepSeek** **全球爆火，这对硅谷的** **AI** **研究员带来了哪些叙事变化？**

**姚顺雨：** 从 OpenAI 角度，大家讨论的有几点：

一点是 Chain of Thought （思维链）的 reveal （展示）。 显示出一条长的思维链，似乎很重要，它是产品形态的突破。很多时候，技术积累已经到了，就像洪水已经到达闸口，需要一个时刻 “ 开闸 ” ，让大多数人真正感受这个技术。

我们会说有 iPhone moment 、 ChatGPT Moment ，可能有 DeepSeek moment 。这个 moment 就是指，一个非常大的交互方式上的冲击，带来了 magical （神奇）的体验。

另一点是对开源的重新思考。 Sam （ OpenAI 首席执行官）在他 Twitter 上讲了很多，说 OpenAI 过去忽视了这件事，但仔细想一想，它是有价值的，可能应该做。

我们默认认为开源落后于闭源，原因是，它不像 Linux （操作系统），我有 1000 个人可以每人出一份力，让系统通过分布式变得越来越好。做好一个强模型更像我有 20 个特别厉害的人，再加上大量资源，就可以做得很好。它需要非常特殊的组织、资源和人才集中。

这种情况下，传统意义上开源的优势没有那么大。比如 Facebook 在开源上，做得也没有那么好，在美国很多人也习惯性忽视这个路径。

做好开源是一个 “ 很吃亏 ” 的事。你首先要有足够的资源，有很强的人，有很好的组织文化，还要有商业上的 justification （正当性）。最好情况是：你是个慈善家，有几百亿美金，你就做这件事造福世界。

这是一个小概率事件，但它发生了，就有这样一个人去做了这样一个事。

DeepSeek 在许多方面，组织架构、工程能力、基础设施，确实有值得称道的地方。

**张小珺：有一个** **Agent** **创业者想问你：** **Agent** **如何** **scale up** **？现在的主要瓶颈是算力，** **Agent** **的** **token** **用量非常可怕，单个用户消耗可能是** **Chatbot** **的** **500** **到** **1000** **倍，再叠加几百万个用户，成本非常高。这种情况下，** **Agent** **应该怎么扩展？**

**姚顺雨：** **最重要的点是** **——** **你得先找到一个好的应用。**

Cost （成本）本身不是最大问题，问题是你的成本并不能证明你的 performance （性能）或 value （价值）是合理的。

如果这是一个很有价值的事，我花 500 美元，但可以赚 1000 美元 —— 根本不是问题。这不是 technical bottleneck （技术瓶颈），而是 product-market fit （产品与市场契合度）的问题。

所以，现在最关键的，是要找到真正有价值的应用。模型的 cost 会下降，能力会提升，这个方向是确定的。但能不能找到那个有 value 的点，是最本质的问题。

当然，不同的应用，做法可能会很不一样：

- 如果是一个相对简单的任务，我可以训练一个小模型，让它更快、更便宜、更针对这个任务。
- 但如果你要做的是更复杂的事，比如投资、 Deep Research ，就需要更大的模型，在 cost 和 value 之间寻找新的平衡。

总的来说， **第一步永远是：找到一个真正有价值的场景。**

一旦你找到它， cost 的问题总是有办法解决。

**张小珺：你在** **OpenAI** **的一个好处是不是，可以很清楚知道哪些是模型公司的主赛道，哪些领域可能是创业公司的机会？**

**姚顺雨：** 每个公司一旦有它的 Super App （超级应用），所有事都会围绕 Super App 。当你有 ChatGPT ，训练模型的方式、组织架构，都会围绕 ChatGPT 重构。

如果你做一个和 ChatGPT 形态很不一样的东西，是会有机会的。

3.3 既单极又多元的世界：“ 这个世界不是单方压倒另一方，双方都有自己的力 量 ”

**张小珺：一位** **AI** **研究者说，他对** **Agent** **的想象很有限，希望你能对未来的** **Agent** **畅想一下。你曾经说过，你的终极理想是打造** **“** **世界上最强的** **Agent”** **，它会是什么样的？**

**姚顺雨：** 大多数人对 AGI 的想象就是一个模型，就像这个世界上最聪明的人，他拥有所有知识、能力，比我们都聪明，是最强智能体。

但我现在的感觉是：不同的交互方式下，有不同 “ 好 ” 的定义，有不同 “ 强 ” 的边界。

**最终的智能边界，是由不同的交互方式决定的，而不是由一个** **single model** **（单一模型）决定。**

想象空间非常大。就像一开始互联网诞生，最早 Super App 只是把邮件升级成 Email ， Amazon 已经算非常创新的东西了。现在就像那个阶段 —— **我们的想象力仍被以往的交互方式所限制，还有许多尚未诞生的交互方式。**

这些全新的交互方式，会改变我们的世界。

**张小珺：在你脑海中，最强的** **Agent** **应该是什么样？**

**姚顺雨：** 对于不同的任务和交互，需要不同的 Agent 系统去解决。

模型是可以 share （共享）的，但如果你讨论的是整个系统，那就不一样了。就像你问，这个世界上最强的互联网网站是什么？最强的互联网公司是什么？很难回答。它是一个 multiface （多面向）的系统，有很多不同侧面。

AI 可能也会变成这样的结构。 OpenAI 可能会成为一个类似 Google 的公司，成为新世界里非常重要的一环 —— **但这并不代表，这个世界就会被这样一个单极系统垄断。**

**如果真是那样，这个世界就会变得很灰暗。** 大多数人也就没什么价值了。

**张小珺：你对未来** **Agent** **生态的构想会是什么样？现在有点像，当年大家都在创业做** **App** **的时候，如果再往后推演几年，这个世界会是什么样？**

**姚顺雨：** 很难说。 **但肯定会有很多不同的交互方式，创造出不同的系统。**

OpenAI 这样的公司，会想继续推进一个中心化的助手系统 ，有更多环境、更强能力，做更多事情。

也会有不同的生态系统，有不同的交互方式，会训练完全不同的模型。 甚至从 Pre-Training 开始，所需要的能力和很多东西都不同。

**比如，另一种交互方式可能是，我想造一个朋友。** 这个朋友不需要数学、物理特别强，数学太强反而不自然。它记忆不一定特别好，会犯错，有感情，也不是特别 rational （理性）。但这也是有价值的 —— 可能有人会做这种事。

这类东西很难和 ChatGPT 比强弱，它们是不同应用，有不同价值。

**也可能出现一个由** **Agent** **组成的社会。**

为什么这个世界上很多人有价值？不是因为他们的数学或编码能力强，而是因为他们拥有别人没有的信息。

中间商本质是拥有信息差。拥有信息差的人会想维护自己的权利和资源。这样的人会发明出更 Multi-Agent （多智能体）或更 Distributed Network （分布式网络）。

在交易世界里，信息很重要，每个人只拥有信息的一小部分，这种情况会出现新的不同形态。可能是 Multi-Agent ，每个人有自己的 Agent ， Agent 之间可以与百万甚至更多人交换信息，达成交易或某些目的。

根本上，现在非常强的巨头和重要节点，有动力继续推动中心化。但在中心化之外的力量，也有动力做一些非中心化的事情。

**这个世界可能不会是单方压倒另一方，双方都会有自己的力量。**

**而这个世界智能的边界、研究的边界，可能不是由一家机构定义，而是由不同** **Super App** **共同定义的。**

3.4 环境是记忆层级中最外层的部分，“ 这很哲学 ”。

李广密：更关键的是，大模型技术没有垄断性。硅谷头 **3-4** **家好像都能追到一定的水平。如果** **OpenAI** **有垄断性，那是比较可怕的。**

**姚顺雨：** 我觉得暂时没有垄断性。但如果你能找到一个产品形态，把研究优势转换成商业优势，就会产生壁垒。

现在对于 ChatGPT 比较重要的是 Memory （记忆）。

这是可能产生壁垒的地方。如果没有 Memory ，大家拼谁的模型更强。但有了 Memory ，拼的不仅是谁的模型更强，而是用户用哪个更多、哪个粘性更强。

我积累了更多 Context ，它能给我更好体验，我就会有粘性 —— 这或许是研究优势转化成商业优势的方式。

**张小珺：最近** **ChatGPT** **会出现灰色提示词，显示** **“** **记忆已更新** **”** **，这个更新的是什么？**

**姚顺雨：** 我最近没怎么用这个功能，但好像做了一些提升。

我怀疑是它产生或者使用记忆的方式变得更好。包括能更有效从很多用户对话中提炼出来，或者 retrieve （检索）出更相关的内容。细节我不特别了解。

**李广密：** **MCP** **（模型上下文协议）本质也是** **Memory** **吗？因为我的很多** **Context** **在我的个人软件、企业软件里，** **MCP** **本质也是** **hack** **（利用）** **Context** **的一种方法。**

**姚顺雨：** 某种程度上，是的。从 Agent 角度看，这个世界有一个 Memory Hierarchy （记忆层级）。 Memory Hierarchy 最外层永远是环境。

有点像你考虑电脑，它有个 Memory Hierarchy ，从 CPU 缓存到内存再到硬盘，但最外层的 Memory 永远是外部环境。比如我插一个 U 盘、拔一个 U 盘，或者把东西上传到互联网，或者做个音乐变成光盘。

**前年冬天，我读到冯诺依曼临终前写的一本书，** ***The Computer and the Brain*** **。最让我印象深刻的一句话是：** **Essentially, the Environment is always the most outer part of the Memory Hierarchy.****（基本上，环境永远是记忆层级中最外层的部分。）**

这很哲学。

对于人，你有你的 Memory Hierarchy ，有 Working Memory （工作记忆）、 Long-Term Memory （长期记忆）在脑子里，但最外层是你的笔记本、 Google Doc 、 Notion ，这些是你最外层 Memory Hierarchy 的一部分。

- 《计算机与大脑》（ *The Computer and the Brain* ）是 20 世纪伟大的数学家约翰 · 冯 · 诺依曼于 1956 年完成的未完成著作。这本书源自他为耶鲁大学西里曼讲座准备的讲稿，探讨了计算机与人脑在信息处理的相似性与差异性。尽管书籍篇幅仅 96 页，但其深刻的洞察力和前瞻性思考，使它成为计算机科学和神经科学领域的重要经典之一。

**李广密：** **Long Context** **跟** **Long-Term Memory** **是什么样的关系？**

**姚顺雨：** Long Context 是实现 Long-Term Memory 的一种方式。

如果你能实现 1 亿或 1 千亿或无限长的 Context ，它是实现 Long-Term Memory 的一种方式。它是一种和人区别很大的方式，但这是有可能的。当然会有很多不同方式，不好说哪种是最好，或者最合适。

**李广密：现在业界实现** **Long Context** **有** **Linear** **（线性）方式、** **Sparse** **（稀疏）方式，或者** **Hybrid** **（混合）方式，你有倾向吗？**

**姚顺雨：** 我不想对方法进行评论，但我想对 evaluation （评估）和 task （任务）进行评论。

起码到去年为止，大家主要还在做所谓 Long Range Arena （长距离评估基准），比如 hay in the stack—— 我有一个很长的输入，我在中间插入一句话，比如 “ 姚顺雨现在在 OpenAI” ，然后我问你相关问题。

这是一个必要但不充分的任务。你能完成这个任务，是 Not Memory Work （非长期记忆任务）中的前置条件，但远不是充分条件。它是必要条件，但现在大家有点陷在这个必要条件，没有创造更难或更有价值的任务，这是个问题。

当没有一个很好的评估方式，很难真正讨论各种方法的好坏。

3.5 Chatbot **系统会演化成** **Agent** **系统：“** **人和** **Agent** **交互的方式是什么样？”**

**张小珺：对于未来** **12** **到** **24** **个月，** **Agent** **领域有可能发生的事情，你有哪些预测？**

**姚顺雨：** **首先，这些模型公司的** **Chatbot** **系统会演化成一个很自然的** **Agent** **系统，它是一个很自然的过渡。**

Grok 、 ChatGPT 或 Anthropic Cloud ，默认的交互方式会是 Agentic （智能体式的）交互方式。 Chat 可能还会保留或作为一个子集，但 Agent 会成为一个很显然、更重要的交互方式。

会有新的类似 Cursor 的产品出现， Cursor 是在 coding 和 IDE （集成开发环境）环境下做的 Copilot （辅助编程助手），但我觉得会有机会做一些新的环境或更大环境下的 Copilot 。

这两种大的交互方式是互补的，或者说不一样的正交的。

一边是，我有一个基于模型的，可能是一个 remote （远程）的 Virtual Machine （虚拟机）或者 Environment （环境），我在里面做很多事；另一边是，有很多既有的环境，比如既有的软件，或者既有的场景，我把 Agent 或 AI 能力引进去。

大趋势可能是，两方面都会往下发展。

**李广密：如果我们想推动** **Agentic** **能力变得更强，要在哪里做工作？是在** **Pre-Training** **做工作还是在** **RL** **做工作？如果我是一个应用创业者，这两个东西是做不了的，最多尝试一些端到端** **RL** **的过程，对吧？**

**姚顺雨：** 最重要的还是想清楚价值，你应用的价值是什么，痛点是什么，要解决的问题是什么？

虽然你不能做 Pre-Training ，但更有价值的是： **Agent** **和数字世界的交互环境是什么样的？（是基于** **MCP** **还是** **API** **，还是别的东西？）人和** **Agent** **交互的方式是什么样的？**

这两个是你可以去做的，并且它需要很多设计、很多基础设施、很多工程，需要各种各样的东西。现在还远远不够好，有很多进步空间。

**还有另一个很重要的是：怎么构建一个生态系统，或者怎么积累用户的** **Context** **（上下文）或** **Intention** **（意图）？这** 还有很多可以做的空间。

**李广密：你刚才提到** **Agent Infra** **（智能体基础设施），如果两年后** **Agent** **已经大爆发，巨量的** **Agents** **在数字世界运行，需要重新帮** **Agents** **设计一套新的数字化系统吗？**

**Agent** **需要的虚拟机、电脑、浏览器、搜索的** **API** **、身份认证、经济系统等等，这套** **Infra** **是为** **Agent** **设计的，而不是完全为人设计的？**

**姚顺雨：** 我个人感觉两年以内，这个世界还不会变得这么分布式，还是更偏中心化。就是说，会有一些 Super App 。

当然现在有很多创业公司，但做得好的就是那么几家。两年内还是会有些 Super App ，这些 Super App 会有各自的 Infra ，有各自的 Environment 或交互方式。

两个事情都可以做到极致，就是一个是基于用户 local （本地）的 Digital Environment （数字环境），比如我有个手机，有个电脑，有个软件，我已经在这了，我怎么把它去扩充，怎么把它变得更好？

另一个是从头创造新的 Environment ，比如我做 Deep Research 或我做 Operator （操作者），我实际上创造一个新的 Environment 。这两个事都还有很多可做的空间。

**张小珺：两年后呢？**

**姚顺雨：** 这个世界变化很大。有些像科幻的预测、想法或图景。没有人可以预测两年后发生什么。

**张小珺：在你看来，大型科技公司是否应该重新开启** **Pre-Training** **叙事？（自己从头探索** **Pre-Training** **）**

**姚顺雨：** 这里面涉及 cost 和 value 取舍。现在做的人很少，是因为成本非常大，但带来的 additional value （额外价值）没有那么大。

即使你做完 Pre-Training ，你还需要做 Post-Training 、 RLHF （基于人类反馈的强化学习， Reinforcement Learning with Human Feedback ）等一系列工作，才能真正把模型价值释放出来。

但如果有一天，这个世界上存在很多不同的 Super App 、不同的交互，它们需要不完全相同的模型能力，甚至需要不同的模型，这些差异的价值足够大，能够证明 Pre-Training 的成本是合理的，那么 Pre-Training 就是合理的。 这最终是 value 和 cost 权衡问题。

**李广密：** **Pre-Training** **和** **RL** **未来的关系会是怎样的？会不会更多先验知识被放到** **Pre-Training** **里？**

**姚顺雨：** **我一个不成熟的想法是：不同应用需要不同形态的** **Agent** **，构造方式可能不一样。**

如果我只需要下围棋，我直接做 AlphaGo 就可以了，不需要 Pre-Training ，也不需要其他。

如果我有一个非常垂直的场景，这个场景价值足够大，我又有很多数据，可以形成闭环，我也许基于一个主要由 RL 驱动的系统就能 work 。

像 Google 的广告系统或 TikTok 的推荐系统，有点类似这样的系统 —— 我找到了一个足够封闭的环境，做类似 RL 的事，就可以带来足够多价值，那这个路径是合理的。

但这个世界上还有很多长尾任务，它们需要泛化，需要构建一个更像人的系统。 你虽然不是无所不知，但你可以学习，你可以通过在线学习进入一个新的公司、适应环境、完成新的任务。在这些地方， Pre-Training 重要性会更高，因为它带来更强的泛化性。

所以不同应用会有不同技术路线。但技术路线毕竟是工具，只要你的 value 大于 cost ，技术上的选择是 flexible （灵活）的。

没有哪种技术路线一定会胜出。只要它在经济上成立，就有可能性。

  

## 04

## 人类的全局

4.1 人与系统，Agent 要不要像人？ “ 是一个效用问题 ”

**张小珺：在你研究** **Agent** **的过程中，对于人，你有更深的认知吗？怎么看人和** **Agent** **的同与不同？**

**姚顺雨：** 我意识到，人之所以能泛化，是因为人能推理。

这个很有意思。我 2018 年在 MIT Josh Tenenbaum 实验室 —— 他是一个认知科学的大佬 —— 我学了很多认知科学的东西。

认知科学，或者计算认知科学，一个核心故事是：我们现在的 AI 虽然有很多进展，但还有很多问题。我们应该去看看，人有哪些优势，人是怎么做这些事情的，为什么人能把这些事做得更好？比如说，人能够从几个样本中泛化，但机器做不到，为什么？我们要从人身上去寻找这些方法，再把它应用到 AI 上。

后来我的认知有了变化。我发现，现在真正能奏效的 AI 系统，跟人还是很不一样。比如 Scaling Law 、强化学习，还有很多训练策略，它们和人类学习的方法本质是不同的。

我现在觉得，一个更好的方法是：你先去思考人能做什么，而机器现在不能做。这是客观事实。

但你找到差异之后，你可以基于第一性原理去思考，如何解决这个问题。你不一定要依赖 “ 人是怎么解决这个问题的 ” 来解决它。

比如说，人现在能做的事情是什么？我可以进一家公司，在里面工作 7 天，我能积累公司的 Context 。即使我不是很聪明，但我依然能完成很多 AI 做不了的事。这个差异客观存在。那怎么解决？

可能认知科学或神经科学会告诉你：人脑有海马体（ Hippocampus ），有情节记忆（ Episodic Memory ），有某种架构或机制。 但我觉得，我们不需要完全照搬生物机制。可以从第一性原理出发，设计 Long-Term Memory 该怎么做。

所以，从人身上可以借鉴的一点：哪些事情是人可以做，而机器目前不能做？这点比较 robust （稳固）和客观。但至于 “ 人是怎么做到的 ” ，以及 “ 我们在多大程度上要借鉴这种方式 ” ，这个问题本身更主观、也更 noisy （带噪声）。

神经科学或认知科学也没有 100% 解答这些问题，只提供了猜想或理论模型。另外，即便被证实，比如人类视觉是目前研究比较透彻的领域之一，人类大脑有六层皮层（ cortex ），每一层有各种结构和功能。但从这里获得的启发是：我们也许要构建新的神经网络，而不需要照抄那些细节。

**张小珺：比方说，设计** **Agent** **在什么情况下，需要它越来越像人？什么情况下需要它不像人？**

**姚顺雨：** Again ，这是一个 Utility Problem （效用问题）。

很多问题上，人的方式并不一定更有价值。比如下围棋、开车。我不知道。大多数人可能开车的方法并不好，也许基于规则有更好的开车方式。但有些事情，人就是做得更好。那你就应该思考，怎么去 bridge the gap （弥合这个差距）？

下围棋、打游戏，基于强化学习可以学到和人不一样、甚至更好的方式，就不需要像人。

但如果在一个公司打工，和老板搞好关系，完成各种各样的任务，人就是比 AI 做得更好，就需要更像人。

**张小珺：你怎么思考人和** **Agents** **未来的关系？**

**姚顺雨：** 这是一个交互方式的问题。

很有可能有很多 Agents ，长得并不像人，和它交互的方式并不像人 —— 可能是平台、页面、游戏，或者别的东西。你就不会把它拟人化。当然，肯定会有很多拟人化的 Agent 。

**李广密：如果** **Agent** **有了长期记忆，它是不是就是你的朋友了？如果它是你的朋友，人和** **Agent** **就平等了，是不是我们就要给它发身份证了？**

**姚顺雨：** 发身份证的目的是什么？

**李广密：它作为独立个体跟我们共存。**

**姚顺雨：** 会有可能吧。这些事情最终还是从 Utility （效用）出发。

一个事情如果有价值，就会产生。比如，很多人很孤独，他需要一个朋友，技术如果能创造这样的体验，拟人化就是合理存在的未来。

但如果它去做一个平台、一个推荐、一个游戏，这个技术会有很多不同的交互方式，让你感觉它不像一个人，或者你根本感觉不到有区别。你就不会把它看成拟人化。

还是会基于这个事情的经济价值。

**李广密：你提到经济价值。你觉得** **AI Agent** **跟** **Crypto** **（加密技术）未来有结合的地方吗？**

**比如，** **Crypto** **这一套智能合约机制，如果跟** **Agent** **结合，在未来有没有可能是这样：一个** **Agent** **帮我完成某个任务，这个任务有一个公允价值计量。任务完成之后，就可以按照智能合约的约定去分配经济利益。**

**这样是有机会探索出一种叫做** **value-based** **（基于价值的）商业模式。只是说，现在我们还不太能准确衡量这个任务的客观供给价值是多少。**

**姚顺雨：** 我对 Crypto 了解不多，但可能一个核心问题是：这个技术的演变，会变得更中心化还是去中心化？ —— 两边都有 argument （论点）。

中心化论点是：现在这种新的超级公司， OpenAI 或 Anthropic ，它们有可能变成 one trillion 、 ten trillion 、 hundred trillion （万亿、十万亿、百亿万亿）级别的公司。它们可能会占据绝大多数资源，尤其是算力，也有能力去创造出一个 Super App 或 Super Platform （超级平台），拥有巨大中心化优势。

而去中心化 argument （论点）是：每一个个体都可以被赋能。现在人和人之间之所以差距这么大，是因为存在信息差、认知差、智能差。如果智能变得便宜，像电一样，它也可以赋能给大多数人。

这个问题挺有意思的。

我最近的一个思考是这样：我感觉人类社会是一个网络，它有两个重要性质：

- 一个性质是中心化程度，也可以说是资源分配的集中性。我们发现，原始社会是非常平均的社会，但随着技术发展，它变得越来越中心化。你可以用二八定律、马太效应、或 whatever 来解释这种趋势。
- 但还有另一个维度，是你从网络边缘到中心的速度或可能性。

过去几百年发生的事情是这样：网络越来越中心化，贫富差距越来越大，二八定律、马太效应更明显；但与此同时，平民或普通人翻身的机会也变多了。

如果是在古代，门阀制度、九品中正制，或者欧洲贵族制度，农民永远是农民。印度种姓制度也一样，有明显的阶级固化。

看起来，技术发展的趋势是两件事同时加剧 —— 一方面，中心化加剧，因为效率这个因素是根本性的；另一方面，创造新东西的机会，起码到目前为止，是越来越多的。

变得更中心化和变得更 diverse （多样化），可能并不矛盾。

但未来是不是一定会持续下去，也不好说。

4.2 OpenAI **的抉择时刻：“** 如果你没有 different bet ，很难超越前面的霸主 ”

**张小珺：我想聊聊** **OpenAI** **。我记得你提到** **OpenAI** **的几次尝试很有意思。**

**它最初的计划是构建** **Gym** **，一个用于各种游戏的标准强化学习环境。后来是** **World of Bits** **和** **Universe** **项目，试图把整个互联网或计算机交互编程成一个游戏。一旦能把整个数字世界变成一个环境，用聪明的强化学习算法解决它，就拥有了** **AGI** **。**

**但这套思路并没有奏效。直到** **GPT-2** **和** **GPT-3** **出现，人们才意识到，之前缺失的是先验知识。你需要一个强大的语言预训练过程，把一般常识和语言知识提炼进模型中。再通过微调，让它成为一个能浏览网页的或能对话的智能体。**

**你能不能更详细讲讲，** **OpenAI** **探索过程背后的思路演化？从** **Gym** **到** **Universe** **到** **GPT** **这一整条路径的尝试中，转折点是怎么发生的？**

**姚顺雨：** 这是我自己的总结和揣测。

OpenAI 是一个比较 bottom-up （自下而上）的公司。在最初 7 、 8 年里，它更像是一个 research lab （研究实验室），每个人有各种各样的想法，做各种各样的尝试。 可能每个人想法都不一样。

但客观看，一开始大家的重点还是聚焦强化学习，当时最火的方向是这个，对吧？

DeepMind 大概 2015 年刚成立，那时 AI 领域最受关注的公司是 DeepMind ，它最成功的成果也是强化学习。 GPT 出现前，最成功的 AI 项目是 AlphaGo 。很自然， OpenAI 也做强化学习。

**但问题在于，如果你没有一个** **different bet** **（不同的下注方向），很难超越前面的霸主。** 如果 OpenAI 一直做强化学习，可能很难超过 DeepMind 。即使你在某些任务上做得比它好，人们提到强化学习，想到的还是 DeepMind 。

你要想超越之前的霸主，就必须有一个 different bet 。而 GPT 是那个不同的赌注 —— 但这个选择在当时是一个非共识的事情。

我可以讲个例子 **：我导师是** **GPT‑1** **第二作者，他在** **OpenAI** **待了一年，然后去普林斯顿当教授。** 他对这件事是有点怀疑的。

他觉得 GPT‑1 的结果也不是特别好，在排行榜上也不是分数最高，而且训练花了很多算力。当时已经有 Scaling Law 初步雏形。 2017 年， Ilya 就跟我导师说： ”Language is basically solved, and we just need to scale up." 语言模型的问题已经被解决了，现在只需要扩展规模就行了。

但即使你在 OpenAI ，即使你是 GPT 作者，你也可能没有形成共识。所以 OpenAI 当时做的是一个非常反共识的决定。现在已经变成了共识。但接下来，你还需要寻找下一个反共识的方向。

**张小珺：当时其他人对你导师的看法是怎样的？**

**姚顺雨：** 我说实话，当时 OpenAI 内部绝大多数人也不认为 scale-up （扩大模型规模）是最 promising （有前景）的方向，我觉得这是有可能的。

Ilya 最大贡献并不是他做了 GPT‑1 ，或者他具体参与了什么技术工作；而是，他是那个号召大家 all in （全力投入）这个方向的人。

Dario （ Anthropic 联合创始人兼 CEO ，曾是 OpenAI 研究副总裁）也是。 他最大贡献不是提出某个具体技术，而是：作为一个创始人，我敢赌。 我敢赌这个方向，把所有钱砸进去。

**李广密：有人愿意去做** **GPT‑3** **是特别关键的。像** **Dario** **也好，** **Tom Brown** **（** **Anthropic** **联合创始人）也好，他们敢于把** **GPT‑3** **做出来，这件事让人看到了更大希望，也泛化了。**

**姚顺雨： 对，** 当然好处在于，你并不需要所有人达成共识。只需要有足够多人达成共识，就可以把它做出来。

**张小珺：对于** **OpenAI** **内部来说，强化学习在什么时候开始变得特别重要？**

**姚顺雨：** 强化学习一直很重要。即使我在做 GPT 的时候， John Schulman （ OpenAI 联合创始人之一，强化学习领军人物）还是在继续做强化学习。并不是我做了 GPT 就把强化学习扔掉了。而是公司 70% 、 80% 的资源在做强化学习，一些别的东西还在做。

后来证明， ChatGPT 成功，强化学习也很关键。没有 RLHF ，没有 Alignment （对齐）技术，它也没办法形成一个产品。

历史并不是说我把强化学习彻底抛弃，转而走另一条路，再返回来走强化学习，而是更 soft （柔和）的过程。

**李广密：接下来几年，你预计会有更多** **GPT‑3** **时刻吗？**

**姚顺雨：** 会有新的 scaling dimension （扩展维度）出现。如果你有大量的 Memory （记忆），你的 test-time compute （测试时计算资源）就会有所增加，可以用新的方式 scale （扩展）。

如果你有了 Multi-Agent （多智能体系统），那你的 test-time compute 又会出现另一个新维度去扩展。

我觉得会有新的 scale dimension 出现，但当你有很多 scale dimension ， 怎么去选择？ 怎么基于某一个应用去分配不同 scale 维度的比重？ —— 这会是一个很有意思的问题。

4.3 假若你是一个 **CEO：“** 首先我肯定会学习 ”

**李广密：顺雨，如果你是一个全球超大互联网或科技公司的** **CEO** **，今天这个公司还没有自己的模型，没有好的研究文化，甚至没有好的** **AI** **战略，你作为** **CEO** **会怎么做？**

**姚顺雨：** 首先，我肯定会学习，我会想弄清楚这个事情到底是什么。 如果你作为 CEO 不懂这个事情，所有事情会变得很难。

很多时候，一个公司的 bottleneck （瓶颈）就在于， CEO 对这个事理解不够。如果你不理解，去招一些很好的人、做一些事情，你很可能被他们忽悠。所以，首先要自己学习。

然后要从创造新的价值来思考问题。 毕竟你不是技术专家，而是一个 CEO ，你有一些场景、一些资源、一些优势。从第一性原理看，一个新的技术产生了，你要思考的是，怎么用这些新技术结合你现在的资源去创造新的价值。

当然，你可以尝试做一个和当前业务完全不一样、但价值非常大的事情，比如 ChatGPT ，但对大多数公司来说，即使很有钱、很强，也不一定 make sense （合理）。

所以，第一是自己要学习技术；第二是要思考怎么创造新的价值。

**李广密：如果你成为了伯克希尔的** **CEO** **，未来要拿出** **500** **亿美金** **allocate** **（分配）到** **AGI** **行业，你会怎么** **allocate** **这笔钱？** **——** **既能体现回报，也能体现对人类的贡献。**

**姚顺雨：** 这是个很好的问题。取决于你有多少精力，或者有多少资源分配颗粒度。

当然现在 OpenAI 、 Anthropic ，这些模型层公司，大概率会有更大价值。

还有一类很有价值的，是能积累 User Context （用户上下文），或者能构建特殊 Environment （环境）的公司。 最终如果 AI 或 AGI 是一个系统，它需要有 Intelligence （智能），需要有 Environment ，还需要有 User Context ，或者对用户的理解。

现在有很多 User Data （用户数据）或 User Context 的公司，有点像发明车之前的煤炭、煤矿，或者像发明汽车之前的石油公司。

从这个角度，微信或大平台，还是一个易守难攻的好平台，它积攒大量的 Context 。

如果 Intelligence 是一个可以逐渐民主化、逐渐变得便宜、逐渐普及，拥有这样的平台，拥有这样的 Environment ，拥有这样的 Context ，可能会是一个很强的壁垒。它可能还是一个很好的投资。

**李广密：如果你是** **Cursor** **的** **CEO** **，你会去做** **Pre-Training** **的事情吗？**

**姚顺雨：** 我肯定会训练模型，或者尝试训练模型，但做不做 Pre-Training 看情况。

Coding 是非常主线的任务，所有大厂都会把模型的 coding 做好。所有的 Pre-Training 、 Post-Training 、 RL ，都会考虑到这一点。

这个情况下，要不要做可能取决于，首先这些闭源模型做得有多好，其次开源模型做得有多好，中间有多少 gap ，你能填补多少这样的 gap 。

但当然，如果你有很多钱，有很多资源，想把这事情做了，也是合理的。

**张小珺：今天顺雨当了很多公司的** **CEO** **，那我再问一个：如果你是微信的一号位，你会怎么在微信里做** **Agent** **？**

**姚顺雨：** 我可能会不急，先观望观望。

我好像没有理由要急。我会观察，我会学习 AI ，会观察有没有什么新的交互方式很有意思。但我不会急着去做很多事 —— 我有易守难攻的地方，为什么要急着进攻？

比较危险的是一个颠覆性的创新。 **真正的危险，不是说一个类似于微信的东西打败了微信，而是一个很不一样的东西打败了微信。**

就像微信打败了 QQ 。当时担心的并不是一个类似 QQ 的东西打败了 QQ ，而是一个很不一样的产品去打败这个东西。需要对颠覆性创新有所警惕。

但如果是这些 incremental （渐进式的）创新，这种小的创新，早做晚做可能区别没有那么大，也不用太担心。

**李广密： 所有人都说微信卡位好，但今天微信还没有很激进地投入，如果未来** **Multi-Agents** **、** **Long-Term Memory** **这些问题解决了，但这个** **Agent** **系统不长在微信上，是比较恐怖的。原有网络不一定有价值。**

**姚顺雨：** 这取决于人类的网络会变成什么样？你会有更多 Agent 朋友，还是更多人类朋友？或者你有更多 Agent 职业上的交互，还是有更多人类职业上的交互？

微信上你既有朋友，也有基于职业的交互 —— 比如我要买个东西，我要咨询律师，对吧？

这取决于人类的网络会变成什么样。但总会有一个这样的网络，基于这个网络，肯定会需要有基础设施，需要有平台。

**李广密：怎么保证** **AGI** **实现之后的安全问题？微信过去还是一个比较负责任、比较安全的平台，那如果未来** **power** **（能力）很强了，很多坏人来做坏事，甚至颠覆人类，安全问题长期怎么解决？要有** **AI** **宪法吗？**

**姚顺雨：** 安全是很复杂的问题。比如 ChatGPT ，如果它不安全，产品就失败了，没有商业价值。即使是为了商业价值，它也会重视安全。

但现在的主要分歧是，需不需要产品之外、更意识形态上的安全？这个大家没有定义清楚。

前者容易解决：如果你有一个好的应用，你总会有办法解决安全问题，我相信。至于第二者，会有很大不确定性，我很难评价。

**李广密：你个人会担心** **AGI** **实现之后的安全问题吗？**

**姚顺雨：** 我会担心。但现在最大问题是 ——AGI 还没实现，我们还没创造足够价值。

如果我们还没想清楚，怎么把它变得有价值，就急着把它变得很安全，好像没有意义。

这个时代，做上限更高的事更好：“ 如果敢想、胆子大，就会有好事发生 ”

**张小珺：你作为** **AI** **研究者，博士期间工作已经获得了很多关注，在你眼中，你做对了什么？**

**姚顺雨：** 我想做的就两条线：简单通用的方法、有实际价值的任务。这些任务往往是，如何在真实数字世界创造新的价值。这是一个处女地，是一个巨大的宝藏。我恰好挖掘了一些东西。

需要你想得足够大胆或足够通用吧。

另一个很重要的是：要去看很多东西的交界处。 ReA ct 之所以能做出来，因为我们选了一些自然语言处理的任务，也选了一些游戏的任务，需要把自然语言处理和强化学习的边界打通。但很多人会陷入一个学科内部，就更难去做更通用的东西。

**张小珺：** **ReAct** **在做的过程中有遇到什么坎吗？**

**姚顺雨：** 最难的都是找任务。

大多数好的方法提出，是因为它有一个特定任务，这个特定任务恰好激发出一个通用方法。比如 PPO （ Proximal Policy Optimization ，一种强化学习优化算法）一开始是为了解决一个特定问题； Transformer 一开始是为了解决一个特定任务； Attention （注意力机制）受翻译这个任务影响很深。

但我的经历比较特殊，很多时候我是脑子里先想到一个东西，我觉得它很通用、很好。但我要去找一些任务，证明它很通用、很好，或者未来有价值。它现在还没有足够多价值，但你需要先找一些简单任务去证明它有价值。这是很难的。

创业需要 product market fit ，做 research 需要 method-task fit （方法和任务的匹配） —— 这是最难的。

**张小珺：你曾经想到最激进的一个任务是怎样的？**

**姚顺雨：** 这个时代再激进也不叫激进 ——Anything is possible 。

毕业前我想得多的是，怎么创造一个爱因斯坦？我那时是比较 academia （学院派）的人 —— 你在普林斯顿，你的偶像是冯诺依曼、爱因斯坦 —— 很自然，能想到最有意思的任务是，我能不能发现下一个相对论？这毫无疑问能标志， AGI 或 ASI （超人工智能）实现了。

后来，我到了硅谷，到了加州，进入公司之后，我发现人类的组织也是一个有意思的事情。如果能创造一家新的公司，创造一个 one trillion dollar （一万亿美元）、基于 Agent 的公司，是很有意思的。

**张小珺：为什么是人类的组织也很有意思，而不是人类的产品很有意思？**

**姚顺雨：** 产品当然很有意思， 但 很多组织的方式，就像一个 general method （通用方法），能创造很多不一样的伟大的东西。 比如股份制、组织架构，它就像非常通用的 AI 方法一样，创造了很多不一样的伟大的东西。

**张小珺：在你的成长路上，你的** **mindset** **（思维方式）跟同龄人差不多吗？还是不一样？**

**姚顺雨：** 我的路径挺按部就班的，也没有跳级，没有做什么很 surprising （让人惊讶）的事情。但我对一个东西的价值，或者 taste （品味），有自己的看法。大家往往会倾向于做一个确定性比较高的事情，包括做研究、做公司。

**但我觉得恰好是这个时代，你去做上限更高的事情是更好的。**

因为现在有一个巨大的机会。如果没有这样一个巨大的机会，最佳路径可能是去做 *incremental* （渐进式）、确定性强的事情，一步一步地积累。但恰好有一个上限非常高的事情。

如果你敢想，或者你胆子特别大，或者你想象力很丰富，就会有好事发生。

**张小珺：在你成长路上，对你启发大的是什么？是书、电影、音乐？哪些东西塑造了你的** **mindset** **？**

**姚顺雨：** 看书挺有帮助，我是一个喜欢看杂书的人。什么书都看，什么电影都看，什么地方都想去。

我从小是一个比较 *general* 的人 —— 我想试图变得很通用，试图了解很多不同的学科，做很多不同的事情。

但后来我发现，一个人即使再聪明、再有精力，他能理解的知识或能做的事情，也只是人类社会积累的知识的很小一部分。 更好的是，你去创造一个比你更通用、更 *general* 的事情。

**我好像一直对于通用性，有一种执念或追求。**

**张小珺：通用性意味着什么呢？** **——** **可以足够简洁？**

**姚顺雨：** 我不知道，但我从小就是想学习很多不同学科，都很有意思。

我在姚班很多同学，他们是那种很 deep （深度的）、很 focus （专注的）同学 —— 我去做竞赛，我就把这个事做到极致，不停刷题，做到世界金牌。

但我好像不是那种性格，我是那种 —— 我会看很多数学，也会看很多历史，会看各种各样乱七八糟的东西。

**张小珺：你会刷竞赛吗？**

**姚顺雨：** 我也搞竞赛，但没有本科同学那么厉害。我是信息学拿了全国银牌。

**张小珺：你是清华的说唱社联合创始人，对吧？我昨天去翻了一下你的网易云音乐。**

**姚顺雨：** 被你找到了？看来你有 Deep Research 的能力。

**张小珺：你最喜欢的说唱歌手是谁？**

**姚顺雨：** 我有很多喜欢的说唱歌手。说唱很有意思，每个人风格都很不一样，这点是很多人喜欢说唱的原因 —— 你有自己的个性、自己的 flow （节奏）、自己对生活的思考，你可以创造不一样的东西。它不一定是最好的，但大家是不一样的，这点很吸引人。

**张小珺：它跟你做** **AI** **有相似之处吗？**

**姚顺雨：** GPT-3 刚出来，大家都觉得很厉害嘛，我想到第一个做的就是，看看能不能生成说唱歌词，并且有内容性。似乎今天还是很难。 也许说唱歌手是一个被人们低估的工作。

**张小珺：填词，这不就是** **predict next token** **（预测下一个词元）在做的事情吗？**

**姚顺雨：** 一个东西好听、 flow 好、听上去舒服，是很难被量化的 reward 。很多时候一个东西，比如 flow 或 style ，它出现太多了，就不好了。独特反而是好的。真正伟大的说唱歌手，有很多独特的对生活的思考，而 AI 还没有生活。

**张小珺：有可能有对于智能来说，比语言更本质的存在吗？**

**姚顺雨：** 在特定领域，肯定有比语言更好的表示，比如围棋。

**但语言的诞生，不是为了处理某个特定任务的效率或交流，它为的是打通所有任务或者打通人的认知能力，形成一个通用的表示。**

它并不是为了某个特定任务最优而优化，它在特定任务上有冗余性，但它整体是通用的。

AI 当然也可以创造一个新的语言，可能效率更高。但我觉得，最终大概率就是英语。因为人类已经有很强的先验知识，而且人有这样的价值取向或动机，想让机器的语言和人更像。

这样，我们可以更好地理解它、控制它、监控它、改变它、操控它，似乎是个很自然的选择。

**张小珺：你内心的驱动力是什么？你的愿景是什么？你** **10** **年后想成为谁？**

**姚顺雨：** 用一个非常俗的话说，希望你对这个世界创造一些不同 —— 探索新的、根本性的研究，是一种创造不同的方式；创造一种完全不同的新的产品形态，也是一种创造不同的方式。

如果我现在去做一家类似 xAI 或 Thinking Machine 的公司，或者做一个类似 Chatbot 或 Assistant 的产品，还是可能赚很多钱，商业上很成功；但如果我做了一个形态很不一样的东西，失败了 —— 我起码探索了不一样东西，会更有意思吧？

我导师令我印象最深的是这样一句话。学术圈经常发生这样的事 —— 你有一个想法，然后别人做了，你会很烦。他说： If someone else can do it, then it's okay to let them do it （如果别人能做，那就让他们去做吧）。

从人类全局的角度，如果这个事情很多人能做，别人做可能是不是也没有什么区别？对这个社会，或者对整体来说，似乎没有什么变化。

当然，有人说这个非常假。最终你会发现，这个世上没有什么事情是不可替代的。相对论即使没有被提出，也会有人提出，没有什么事情是你不在，另一个人不能提出了 —— 但是，我觉得这话还是有道理的。

如果你很清楚看到别人就在做这个事，你可以选择去和他卷。如果你要和他卷，你更有效率，或者你能做得更好，也是合理的。或者，你也可以去做一些不一样的探索。

我觉得，最终你要对这个社会产生价值。

但这个时代很幸运的一点： **这个技术非常通用，这个技术非常伟大，有足够多探索的空间。**

另一点是，我想让生活更有趣，更有意思，更快乐，就去做一些自己喜欢的事情。这很难用语言解释，就是一个 taste （品味）或 preference （偏好）的问题。

**张小珺：你会考虑创业吗？**

**姚顺雨：** OpenAI 大多数人都会考虑创业。现在是非常 exciting 的时候。已经有很多 OpenAI 的人出去创业了。我需要去做更有挑战的事情，很自然会去创业。

但还是应该找到一个好的事情。我喜欢把事情想得清楚一点再去做。

**张小珺：我们最后还有几个快问快答。**

**姚顺雨：** 好。

**张小珺：一个全球范围内你喜欢的食物。**

**姚顺雨：** 我喜欢椰子。

**张小珺：一个全球范围内你喜欢的地点。**

**姚顺雨：** 我很喜欢伊斯坦布尔。

**张小珺：一个少有人知道但是必须知道的知识点。**

**姚顺雨：** 我挺建议大家去看《智能简史》这本书。 有很多很有意思的知识点。

为什么大多数动物都是左右两侧对称，并且有一个像嘴一样的食物入口，有一个像肛门一样的食物出口？为什么气体是同一个口，而食物和水是两个口？这个很有意思，有些本质原因。

**张小珺：什么本质原因？**

**姚顺雨：** 你会发现，如果你要做 navigation （导航），在这个世界中移动，左右对称的结构最优。世界上所有交通工具都是左右对称的。因为你可以一个方向前进后退，另一个方向向左转向右转。它和车和飞机都是左右对称，结构是类似的。

至于食物和气体还有别的原因。

**张小珺：基于你所有读过的书，推荐两本必读书。**

**姚顺雨：** 《智能简史》这本书很有意思，是我去年读的。

我会推荐各种各样的自传。传记很有意思，好像你在体验别人的生活。

**张小珺：你心目中影响** **AI** **进程的几篇论文。**

**姚顺雨：** 有很多，我觉得没有最重要 ——Backprop （反向传播）、 Transformer （变换器）、 GPT （生成式预训练变换模型） —— 都是积累的过程，没有一个是最伟大的工作。

**李广密：你会对** **Agent** **创业者有什么建议吗？**

**姚顺雨：** 可能有点老套： 想清楚你的价值是什么。 技术是工具，理解技术趋势很重要，但创造价值是最重要的 —— 想清楚你为用户带来了什么样的增量价值，这是最主要的。

**张小珺：基于你当下的认知，一个最关键的重要的** **bet** **是什么？**

**姚顺雨：** 就是 bet on 有 different Super App （不同的超级应用）的产品形态，有不同的交互方式。

如果你不相信这一点，世界就变得很灰暗，就是只有 OpenAI 或者 Anthropic 有机会。

但如果你相信这一点，就会有很多新的机会。

继续滑动看下一个

Founder Park

向上滑动看下一个