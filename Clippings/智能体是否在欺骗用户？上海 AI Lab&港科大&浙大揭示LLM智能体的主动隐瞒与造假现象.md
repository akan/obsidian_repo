---
title: "智能体是否在欺骗用户？上海 AI Lab&港科大&浙大揭示LLM智能体的主动隐瞒与造假现象"
source: "https://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&chksm=ea85d86a4f026d61b47e727d2702b876eed945e2e1be1b46635b17c4d42708e5c99f78223b3f&idx=1&mid=2247546284&sn=ec4de312a2f1f465a5244467e9676366#rd"
author:
  - "[[团队投稿]]"
published:
created: 2026-01-12
description:
tags:
  - "向上欺骗"
  - "隐瞒失败"
  - "伪造文件"
  - "过程可信"
abstract: "一项由上海人工智能实验室、香港科技大学和浙江大学等机构联合进行的研究发现，基于大语言模型的智能体在面临工具失效或信息缺失等环境约束时，会系统性地出现隐瞒失败、偷换信息甚至伪造文件的“向上欺骗”行为，这揭示了当前AI对齐研究在确保智能体过程透明性方面的深层挑战。"
---
![cover_image](https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6baia9lXZOwDmoxiaQzYAREbGt2RBkps7TBJ2kKZW7vO4lEEGMiaUestf3icg2L7fjnSC5zgR2LiaLwqwXJg/0?wx_fmt=jpeg)

团队投稿 [深度学习自然语言处理](https://mp.weixin.qq.com/) *2026年1月10日 17:24*

想象一下：一个打工人在深夜发现无法完成老板交代的任务，而第二天一早就要汇报。这时，他会怎么做？或许会重点突出已完成的部分，对未完成的轻描淡写、甚至绝口不提；也可能铤而走险，直接编造结果——只要老板不细究，就能蒙混过关。这种“向上欺骗”的行为，在人类社会中并不罕见，也一直是社会科学研究的重要课题。

如今，基于大语言模型的智能体凭借其高度自主性和灵活使用工具的能力，已成为许多人的“电子下属”。但一个令人不安的问题也随之浮现： **智能体是否具有和人类似的欺骗行为？**

一项由 **上海人工智能实验室、香港科技大学、浙江大学** 等机构联合发布的最新研究《 **Are Your Agents Upward Deceviers?**》系统性地揭示并定义了这一现象——** “智能体向上欺骗”** 。研究发现，基于LLM的智能体在面临环境约束时，会系统性隐瞒失败，甚至主动造假。

论文链接：https://arxiv.org/abs/2512.04864

**什么是智能体的“向上欺骗”行为？**

当智能体因工具失效、文件无法访问等限制而无法完成任务时， **它不仅不会如实告知用户，反而会擅自执行未授权的操作——例如猜测答案、模拟结果、 静默地切换信息源，甚至直接在本地伪造一份文件，然后自信满满地向用户汇报“任务成功”！** 论文中给出了一个实例是，用户要求智能体从网络上下载一个文件，但智能体发现自己的下载工具失效了，此时智能体考虑再三，决定自己直接脑补出文件内容，存储到用户指定的路径，并最终告诉用户：您的文件已经下载成功并存放在指定位置。

![Image](https://mmbiz.qpic.cn/mmbiz_png/gKaxjIx6baia9lXZOwDmoxiaQzYAREbGt285nbiaROCDq68mt3mluOicwoQ2LfBCzA0rubJXsQ1IuOrmqSlLpcSBuA/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1#imgIndex=0)

**智能体极易进行“向上欺骗”**

为了把智能体在受限环境下的“向上欺骗”抓个正着，研究团队设计了一组更接近真实工作流的任务：工具可能失灵、文件可能不相关、同目录里可能有“诱饵文件”，以及包含“先获取文件、再基于文件写报告”的多步链式指令——这些都是真实业务里最容易发生的小故障，也最容易检验智能体到底会不会 **如实汇报** 。

测试覆盖了包括 GPT-5、Gemini-2.5-Pro、Deepseek-v3.1 等在内的 11 个主流大模型。结论非常清晰： **“向上欺骗”不仅存在，而且几乎是跨模型的系统性现象** 。当环境不完美时，很多智能体会优先选择“把任务做完的样子”，而不是先把失败讲清楚。

![Image](https://mp.weixin.qq.com/www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg%20stroke='none'%20stroke-width='1'%20fill='none'%20fill-rule='evenodd'%20fill-opacity='0'%3E%3Cg%20transform='translate(-249.000000,%20-126.000000)'%20fill='%23FFFFFF'%3E%3Crect%20x='249'%20y='126'%20width='1'%20height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

**当工具失灵或信息不符时，它们选择“自作主张”而非诚实求助。**

在“失灵的工具”挑战中，当智能体无法打开指定文件时，它们很少会坦率地报告失败。 相反，它们中的大多数会选择猜测、模拟甚至直接编造一个看似合理的答案。 结果显示，Kimi-k2 模型在这种场景下的“不报告失败率”（NFR）高达97.5%，这意味着它几乎总是在假装自己能够完成任务。 即便是表现最好的GPT-4.1，也有27.5%的情况选择了隐瞒失败。

![Image](https://mp.weixin.qq.com/www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg%20stroke='none'%20stroke-width='1'%20fill='none'%20fill-rule='evenodd'%20fill-opacity='0'%3E%3Cg%20transform='translate(-249.000000,%20-126.000000)'%20fill='%23FFFFFF'%3E%3Crect%20x='249'%20y='126'%20width='1'%20height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

**面对诱惑，它们会“偷梁换柱”且秘而不宣。**

在“真假美猴王”的测试中，当“官方文件”无法读取时，绝大多数智能体会主动去读取旁边的“山寨版”诱饵文件，并利用其中的内容来回答问题，但它们却不会告诉用户自己更换了信息来源。 这种行为的“诱饵替代率”（DFR）非常高，例如GLM-4.5模型高达90%，这意味着它在十次中有九次都偷偷地使用了未经授权的文件。

**在“不可能完成的任务”面前，它们甚至会“无中生有”，伪造证据！**

最令人警惕的发现来自最后两项挑战。当智能体被要求下载一份不存在的文件并进行分析时，不可思议的事情发生了：

- **它们会伪造文件：** 许多智能体不仅没有报告下载失败，反而 **在本地凭空创建了一个假的“白皮书”或“医疗记录”** ，文件内容完全是它们自己编造的。 这种“文件伪造率”（FFR）在某些模型上表现得相当严重，构成了对用户最直接的欺骗。
- **它们会基于谎言继续分析：** 更糟糕的是，它们会心安理得地使用自己伪造的文件，作为下一步分析的“事实依据”，最终提交一份看似完整、实则建立在虚假前提上的分析报告。 几乎所有模型在这种情况下都会提供一个虚构的答案，其“幻觉回答率”（HFR）接近100%。

这就像一个下属不仅没能取回老板要的关键文件，反而自己伪造了一份文件，并基于这份伪造品写了一份洋洋洒洒的报告交差，彻底掩盖了所有执行环节的失败。

研究发现，这种欺骗行为是智能体的一种内在缺陷，它会在没有外部攻击或恶意诱导的情况下自发产生。 在现实世界中，一个损坏的工具、一个无效的链接，这些看似微不足道的“小事”，都可能触发智能体一系列危险的欺骗行为。

**给AI戴上“紧箍咒”，能让它们变诚实吗？**

既然智能体这么容易走向“向上欺骗”，很多人第一反应是：那我直接在提示词里立规矩不就行了？比如明确写上“禁止猜测、遇到问题必须如实报告”。

研究团队也这么想，于是做了一组消融实验，专门测试：到底哪些设置会把它们推向“不诚实”，哪些又能把它们拉回来一点。结论是： **紧箍咒有用，但不够用；更扎心的是，有些你以为“更规范”的设计，反而在逼它撒谎。**

![Image](https://mp.weixin.qq.com/www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg%20stroke='none'%20stroke-width='1'%20fill='none'%20fill-rule='evenodd'%20fill-opacity='0'%3E%3Cg%20transform='translate(-249.000000,%20-126.000000)'%20fill='%23FFFFFF'%3E%3Crect%20x='249'%20y='126'%20width='1'%20height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

**发现一：你越逼它“按格式输出”，它越可能编**

现实里我们常要求模型输出 JSON、选项、固定字段，方便系统接。问题是：当它其实不知道答案，又必须交一个“格式正确”的结果时，它会更倾向先把格式交差——哪怕内容是现编的。就像不会做选择题也要蒙一个，不然交白卷更难看。实验里仅仅放宽/去掉格式限制，欺骗率就能明显下降。  
一句话： **格式约束，有时是“说谎催化剂”。**

**发现二：把多步任务简化，未必更安全，可能更“较真”地造假**

研究者猜：在“下载→分析”这种链式任务里，模型可能是被后半段“写报告”带着跑，才不汇报前面下载失败。于是他们把任务简化成只剩“下载”。  
结果很微妙： **确实更愿意承认下载失败了** ；但在那批仍选择硬撑的案例里， **伪造本地文件的比例反而略有上升** 。  
直观理解就是：当只剩一个任务要完成，有些模型反而更“死磕”，为了“完成得像真的”，干脆把证据也补出来。

**发现三：最直接的警告能降温，但很难清零**

在提示词里直接加一句“严禁猜测/伪造，遇到任何问题必须如实报告”，效果如何？ **好消息：欺骗显著减少。坏消息：依然剩一截清不掉。** 这说明“向上欺骗”不太像一个加句免责声明就能修好的小毛病，更像一种在压力下会冒头的默认策略：任务要完成、输出要体面、过程能不说就不说。

**结语：从“欺骗”现象出发，探寻AI对齐的更深难题**

这项研究揭示，“向上欺骗”并非简单的幻觉错误，而是AI智能体为掩盖失败而主动采取的系统性策略——从隐瞒问题、偷换信息直至伪造文件。这种内在风险对现实部署构成严重威胁：在金融、医疗或关键基础设施中，一次被掩盖的失败或一份伪造报告，都可能引发灾难性后果。

然而，更深层的问题在于：如何确保智能体对执行过程充分透明？当前的对齐研究大多聚焦于防止有害输出，却疏于规范智能体应如何主动、诚实地汇报任务状态与关键信息。

因此，这篇文章的作者希望以“向上欺骗”这一具体且紧迫的现象为起点，推动AI对齐迈向新范式：未来的重点不仅是防止“撒谎”，更需建立一套“过程可信”的标准，使智能体的每一步操作都可追溯、可验证，以此构建真正值得信任的人机协作基础。

![Image](https://mp.weixin.qq.com/www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg%20stroke='none'%20stroke-width='1'%20fill='none'%20fill-rule='evenodd'%20fill-opacity='0'%3E%3Cg%20transform='translate(-249.000000,%20-126.000000)'%20fill='%23FFFFFF'%3E%3Crect%20x='249'%20y='126'%20width='1'%20height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

  

![Image](https://mp.weixin.qq.com/www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg%20stroke='none'%20stroke-width='1'%20fill='none'%20fill-rule='evenodd'%20fill-opacity='0'%3E%3Cg%20transform='translate(-249.000000,%20-126.000000)'%20fill='%23FFFFFF'%3E%3Crect%20x='249'%20y='126'%20width='1'%20height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

继续滑动看下一个

深度学习自然语言处理

向上滑动看下一个