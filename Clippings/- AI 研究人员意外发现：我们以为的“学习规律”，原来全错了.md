---
title: "AI 研究人员意外发现：我们以为的“学习规律”，原来全错了"
source: "https://mp.weixin.qq.com/s/mGZAZOvuei55RfvjB5j3bg"
author:
  - "[[AI范儿]]"
published:
created: 2025-08-20
description: "智能，不是记住所有答案，而是发现规律的能力。大规模参数带来的不是储存空间，而是搜索空间。"
tags:
  - "双重下降"
  - "彩票假说"
  - "偏差方差权衡"
abstract: "AI研究人员发现传统机器学习中的偏差-方差权衡理论在超大模型中失效，提出彩票假说解释大模型泛化能力。"
---
![cover_image](https://mmbiz.qpic.cn/mmbiz_jpg/OdmYSS49h7GvqB3YL6Kjca66Uictvia5k7mXVGp7htB8c8VCYwia1wJbPLuqg3Cl68LWApWmBBpAia8KwH5LbMliaRg/0?wx_fmt=jpeg)

Original AI范儿 [AI范儿](https://mp.weixin.qq.com/s/) *2025年08月20日 09:47*

![Image](https://mmbiz.qpic.cn/mmbiz_jpg/OdmYSS49h7GvqB3YL6Kjca66Uictvia5k73CB4pbiaGZ9Qzo7GaH0v0sdJpyeK2ibxWiaCP5XjayEXUo10NXwGVRFoQ/640?wx_fmt=jpeg&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1)

【本文翻译自 nearlyright.com 】

还记得那条人人遵守的“机器学习铁律”吗？模型太大必然过拟合，只会死记硬背、学不到东西。这不是传说，而是写进教科书的定律——三个世纪以来的统计理论都这么说。

直到有一天，这条“神圣不可侵犯”的规则被打破了。

现在，拥有数千亿甚至数万亿参数的神经网络驱动着 ChatGPT，破解蛋白质结构，还引发了一场高达数千亿美元的全球 AI 军备竞赛。而真正发生转变的，不只是算力，而是我们对“学习本质”的重新理解。

这个故事揭示了一项划时代的 AI 突破，它的诞生源自一群敢于挑战领域基础假设的“异端”。

---

  

**300 年的偏差-方差法则，是怎么倒下的**

在很长一段时间里，偏差-方差权衡（bias-variance tradeoff）被视为学习理论的基石：模型太简单，看不到重点；模型太复杂，记住的只是噪声。

想象一个小学生在学加法。他有两个选择：要么理解进位规则和位值原理，要么死记硬背所有题目和答案。后者在做作业时可能无懈可击，但一到考试就全军覆没。

传统理论认为，神经网络尤其容易“走歪路”。参数越多，越容易记住全部训练数据而无法泛化。于是学术界的共识变成了：模型要小巧精致，结构要严控，正则化必须到位。谁要是提出“加点参数”来解决问题，简直是学术异端。

---

  

**异端的反叛，撞出了“双重下降”**

但就在 2019 年，一群研究者反其道而行之。他们不仅不在模型“刚好”学会时停下，反而继续扩大规模，一路冲向“理论高危区”。

结果呢？模型不仅没崩塌，反而“第二次变强”。

这一现象被称为“double descent”（双重下降）：模型先是如理论所预言地过拟合，误差上升，但接着神奇地误差又开始下降，仿佛它穿越了过拟合的深渊，达到了新境界。提出者 Mikhail Belkin 等人坦言，这与经典偏差-方差理论“背道而驰”。

OpenAI 后续的研究显示，这种好处不止出现在一个模型里，而是普遍存在于不同任务和规模上。大模型不仅学得多，还开始具备“举一反三”的能力——只看几个例子就能学会一项新任务。

从那一刻起，整个行业彻底掉头：Google、Microsoft、Meta、OpenAI 纷纷豪掷数十亿美元，一路堆参数。GPT 模型从 1.17 亿扩张到 1750 亿。“模型越大越好”从昔日的笑谈，变成今天的行业信条。

但问题来了：这一切，为什么行得通？

---

  

**“彩票假说”：为什么大模型反而学得更好**

谜底来自 MIT 的 Jonathan Frankle 和 Michael Carbin。他们研究的是“剪枝”技术——也就是在训练之后删掉那些没用的权重。没想到，他们发现了大模型的秘密。

他们发现，每一个大模型中都藏着一个“中奖小模型”（winning ticket）——一小部分神经元构成的子网络，性能可以媲美整个大模型。他们甚至能删掉 96% 的参数而不损失精度。换句话说，大多数参数根本没用。

关键在于，这些“中奖票”只有在初始随机权重保持不变时才有效。一旦你换了初始化方式，即便保留架构，也会一败涂地。

于是，“lottery ticket hypothesis”（彩票假说）诞生了：大模型不是靠复杂解取胜，而是靠参数数量带来的机会空间。每一个子网络就是一张彩票，大部分注定落空，但参数足够多，总有一张是赢家。

整个训练过程，就是一场规模空前的彩票抽奖：模型里已有成千上万个不同起点的小网络，训练过程挑出那张“天选之票”，其余统统淘汰。

这个观点让传统学习理论与实践奇迹和解了：大模型之所以有效，不是因为它违反了规则，而是它在一个我们原本没意识到的层次上“玩转了规则”。奥卡姆剃刀依然成立——最简单的解释才是最优的。规模只是为了更高效地找到这些简单解。

---

  

**重新定义“智能”**

如果“学习”就是在广袤的可能空间中寻找最优雅的解释，那“智能”本身也就可以被重新定义了。

人脑有 860 亿个神经元、万亿级连接，怎么看都是“过拟合怪兽”。但我们却能凭几个例子学会一个全新概念，并推而广之。彩票假说指出，人脑的这种“冗余”正是为了让你有更高的几率抽中那个最简洁的解法。

智能，不是记住所有答案，而是发现规律的能力。大规模参数带来的不是储存空间，而是搜索空间。

---

  

**科学进步的本质：敢于怀疑“看似永恒”的法则**

这场关于神经网络的发现，也提供了科学哲学上的启发：我们很多时候不是在推翻原理，而是在揭示它们运行得比我们以为的更深、更复杂。

想想大陆漂移，最初被视为荒唐，直到板块构造解释一切；量子力学看似不合常理，直到实验证明一切真实存在。最深刻的进展，往往始于“敢于尝试看起来不应该成功的事”。

彩票假说也没有推翻传统学习理论，而是赋予它新的维度。偏差-方差权衡还在，只是我们现在知道，它是在更高维度上发生作用。

这也意味着，当前的“大模型路线”是有效的，但也不是无限扩展的万能钥匙。越往后，抽中大奖的边际收益越小。像 Yann LeCun 这样的大牛也提醒我们，仅靠规模，语言模型也许永远无法真正理解人类语言，因为它们缺乏根本性的架构突破。

---

  

**AI 革命的真相：原来最重要的不是算力，而是怀疑精神**

AI 的这次进化，不只是一次技术飞跃，更是一次思想解放。

真正带来改变的，是那群“明知不可为而为之”的研究者。他们挑战了教科书、质疑了公理，最终发现：所谓的规则，其实并没有错，只是它们的运行方式远比我们设想得更优雅、更复杂。

在这个构建于数学确定性的学科里，最重要的突破，却来自于对“未知”的勇敢拥抱。

  

---

  

【注：】

尽管“偏差-方差权衡”这一术语直到 20 世纪后半叶才逐渐标准化，但它背后的数学思想，早在 **18 世纪** 就已萌芽。1763 年， **Bayes 定理** 为后来的统计推理奠定了基础，使我们能够用数据来更新信念；而 **Laplace** 在 1780 年代至 1810 年代之间的研究，则首次明确提出：模型若过于复杂，容易捕捉噪声而非信号——必须在拟合度与简洁性之间保持平衡。

这一思想，正是现代“偏差-方差权衡”原则的雏形。从那个时代开始，统计学就逐渐形成了这样一种哲学：最好的模型不是记住所有细节，而是 **用最简洁的方式解释世界** 。

也就是说，当 AI 研究人员在 2019 年挑战“模型越大越糟”的共识时，他们不是在反对几百年来的统计智慧，而是在揭示这些原则在更复杂系统中 **依旧成立，只是以更微妙的方式运行** 。

  

📮 本文由「AI 范儿」出品

我每天都在更新，如果你觉得这些内容对你有用，

那我们就加个关注、交个朋友。

📬 商务合作 / 培训邀请 / 专访约稿

👉 请关注公众号并回复 ****【合作】**** 获取联系通道

![Image](https://mp.weixin.qq.com/s/www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg%20stroke='none'%20stroke-width='1'%20fill='none'%20fill-rule='evenodd'%20fill-opacity='0'%3E%3Cg%20transform='translate(-249.000000,%20-126.000000)'%20fill='%23FFFFFF'%3E%3Crect%20x='249'%20y='126'%20width='1'%20height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

  

[Read more](https://mp.weixin.qq.com/s/)

继续滑动看下一个

AI范儿

向上滑动看下一个