---
title: "大模型推理成本每年降低10倍的秘密：一文了解vLLM、SGLang等6款主流推理引擎前段时间，“源神”DeepSeek  - 掘金"
source: "https://juejin.cn/post/7500977298608455691"
author:
published: 2025-05-06
created: 2025-05-08
description: "前段时间，“源神”DeepSeek 又在 Github 上宣布开源计划了，这次将开源的是 DeepSeek 自研的推理引擎，也是更早之前 DeepSeek 开源周最后一天发布的“One More Th"
tags:
  - "clippings"
---
![横幅](https://p9-piu.byteimg.com/tos-cn-i-8jisjyls3a/80e551ec95e54d3e94bf0f1cdad71e51~tplv-8jisjyls3a-image.image) ![](https://p3-piu.byteimg.com/tos-cn-i-8jisjyls3a/ef1b479729b54febacdf28345ebe61af~tplv-8jisjyls3a-image.image)

![image.png](https://p3-xtjj-sign.byteimg.com/tos-cn-i-73owjymdk6/37b1812b87354ffa832265a6dce2d181~tplv-73owjymdk6-jj-mark-v1:0:0:0:0:5o6Y6YeR5oqA5pyv56S-5Yy6IEAgUFBJT-a0vuasp-S6kQ==:q75.awebp?rk3s=f64ab15b&x-expires=1747129216&x-signature=2k3WE%2BftP5cWLAXftwvHt8ft%2FpE%3D)

前段时间，“源神”DeepSeek 又在 Github 上宣布开源计划了，这次将开源的是 DeepSeek 自研的推理引擎，也是更早之前 DeepSeek 开源周最后一天发布的“One More Thing”。

DeepSeek 并不会选择直接开其内部完整且高度定制化的代码库，而是将采取一种更侧重协作、更具可持续性的策略，将其核心优化成果贡献给现有的开源项目，比如 vLLM 与 SGLang——这是两家业内领先的开源推理引擎项目。

推理引擎对于提高推理效率、降低推理成本有着重要意义，是大模型产品化部署的核心基础设施。著名投资机构 a16z 曾预测，大模型价格的下降速度甚至比个人电脑革命时期的计算成本或互联网泡沫时期的带宽成本下降速度还要快， **同等性能的大模型成本每年下降10倍** 。

本文将介绍业内主流的六大推理引擎，以及他们分别解决的核心技术问题。

点击 [PPIO官网](https://link.juejin.cn/?target=https%3A%2F%2Fppinfra.com%2Fuser%2Fregister "https://ppinfra.com/user/register") ，填写邀请码【JUEJIN】注册，即可得获取15元算力代金券，畅享DeepSeek V3/R1以及最新上线的Prover2、Qwen3系列、GLM-4-0414等模型

1. ## 为什么需要推理引擎？

推理引擎，顾名思义解决的是大模型的推理问题。大模型推理（Inference）是指在大语言模型或多模态模型训练（Training）完成之后，将其应用于实际任务（如聊天机器人、搜索增强生成、代码补全、AI助手等）中，输入数据并生成输出结果的过程。

从技术角度看，推理过程包括输入预处理（如分词）、模型执行（神经网络计算）、输出解码，以及性能优化（如 KV 缓存、并行调度、量化加速等）。相比训练，推理更关注效率、延迟、吞吐量和部署可控性，是生产环境中性能、成本和用户体验的关键驱动力。

**推理引擎，就是专门用于高效运行大模型推理任务的软件系统** ， **在保持模型输出准确性的前提下，最大化推理速度、吞吐量和资源利用率，典型代表有 vLLM、** **TGI** **、SGLang、TensorRT 等。推理引擎是大模型产品化部署的基础设施核心。**

推理引擎的发展大概分为两个阶段，第一个阶段是在 2023 年之前的早期阶段。此时，行业内还并没有专业的推理引擎，大家最常用的推理工具，是 Hugging Face 在 2019 年发布的通用推理框架 Transformers。

Hugging Face Transformers 是一个流行的开源 Python 库，最早主要支持 BERT 类模型的推理与微调。到 2020-2021 年 GPT-2/GPT-3 等模型流行，Transformers 成为几乎唯一一个能加载上百个模型架构，且能用几行代码就跑出结果的框架。这是大多数人“第一次跑大模型”的起点。

```
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained("gpt2")
tokenizer = AutoTokenizer.from_pretrained("gpt2")

inputs = tokenizer("Hello, how are you?", return_tensors="pt")
outputs = model.generate(**inputs)
print(tokenizer.decode(outputs[0]))
```

Hugging Face Transformers 是最早承担推理任务的框架，使用范围广泛。然而，它并非专门为推理效率而设计的。随着模型规模和用户请求增加，Transformers 在推理上暴露出很多性能瓶颈。这些问题主要包括：

🐢 推理速度慢：每次生成都要走完整的Transformer流程，低效

💣 内存吃紧：显存常常爆满，尤其多个请求时

🧍♂️不能高效多用户并发：多个用户一起用会卡顿

🔁 不能复用缓存：每次生成都重新算前面部分

⏳ 长上下文处理困难：输入太长会显存爆炸

🤯 代码复杂难部署：原生transformers推理代码臃肿，难调

这些复杂的技术问题最终都会指向一个结果——大模型的推理效率太低、推理成本太高。

于是，从 2023 年开始——也就是 ChatGPT 发布之后，大模型推理进入第二阶段，专门面向推理效率优化的推理引擎应运而生！

1. ## 推理引擎百家争鸣

随着大型语言模型（LLM）的参数规模不断攀升，如何高效地进行推理成为业界关注的焦点。各种开源和商业的推理引擎纷纷涌现。业内主流推理引擎包括：

1. ### TGI（Text Generation Inference）

TGI 由 Hugging Face 在 2023 年初公开发布，支持主流的开源模型架构（如LLaMA、Falcon、BLOOM、StarCoder等）的一键部署。2023年之后，TGI 快速发展并广泛应用于云服务（如AWS、Azure）和企业平台。2025年初，TGI 引入多种后端支持（如vLLM、TensorRT-LLM），成为一个通用的推理框架。

作为成熟的工业方案，TGI 对开发者极为友好，与Hugging Face模型库无缝连接，支持多种硬件平台，适用于需要快速搭建分布式模型服务的场景，是第一个面向生产环境、开源易用的推理引擎。

TGI的典型应用场景包括互联网公司、AI创业公司搭建聊天机器人、内容生成服务，广泛用于中小型企业快速上线AI服务。

1. ### vLLM

vLLM 最初由加州大学伯克利分校的天空计算实验室开发，于2023年6月首次发布， 是 LLM 推理领域的重要分水岭。

vLLM的革命性创新在于PagedAttention技术，vLLM 在不改动模型结构的情况下，大幅压缩显存开销、提高服务并发，是第一个在系统层面对 LLM 推理进行全面重构的开源引擎。

今天，vLLM 已发展成为一个社区驱动的项目，得到了学术界和工业界的广泛贡献，GitHub 上获得了数万星标。2025年1月，vLLM 发布了v1 alpha版本，这是一个重要的架构升级，旨在进一步提升性能 。v1版本引入了优化的执行循环、简单且灵活的调度器以及零开销的前缀缓存等新特性。

vLLM 适合对推理性能要求高、但成本敏感的企业，比如提供在线聊天服务的初创公司和研究团队。

1. ### TensorRT-LLM（NVIDIA）

为了进一步挖掘硬件潜力，NVIDIA 在2023年下半年推出了 TensorRT-LLM。

TensorRT-LLM专门为 A100、H100 等 GPU 做了深度定制，支持流式输出、超大模型多卡部署等，是面向极致性能场景的工业级推理引擎。

TensorRT-LLM 当前主要用于需要在 NVIDIA 硬件上部署 LLM 的 **公司** ，如 Meta、Cohere 等都与 NVIDIA 合作利用该库优化其模型推理。该工具巩固了 NVIDIA 在高端 LLM 推理市场的技术优势。

1. ### SGLang

SGLang 由伯克利的 LMSYS.org（Chatbot Arena平台的创建者）团队开发，最初于 2024 年 1 月发布 。SGLang 的设计目标是成为一个通用的、高性能 LLM/VLM 服务引擎，不仅提高性能，还支持复杂的 LLM 程序化调用 **，** 包括多轮对话、代码执行/工具接入和结构化输出等。

SGLang 强调推理过程的灵活编排和多模态支持，2025年后逐渐获得行业内头部公司关注，如字节跳动、xAI 等公司。

SGLang 适用于多模态应用（如看图对话），需要复杂逻辑的AI助理和Agent场景，适合希望深度定制推理流程的团队。

1. ### llama.cpp

与上述主要针对GPU的引擎不同，llama.cpp 主打“在普通电脑甚至手机上运行大模型”。它由社区开发者 Georgi Gerganov 在2023年推出，项目一经推出便快速走红，在个人开发者和小型公司中得到广泛采用。

llama.cpp 的特色在于超低的硬件门槛，无需GPU也能运行，同时完全开源，跨平台（Windows、Linux、Mac、手机等）使用简单，适用于个人开发者、学生、小公司等场景，用于快速验证AI创意或离线环境的推理需求。

1. ### 微软 DeepSpeed

微软于2020年推出DeepSpeed，起初侧重于训练环节的优化。2021年后逐步增强推理功能，尤其擅长大规模模型在分布式系统中的高效推理。如今是学术界与工业界普遍采用的解决方案。

DeepSpeed的特色在于支持超大规模模型的高效推理（千亿参数以上），提供从训练到推理的一站式优化方案。

DeepSpeed适合大型科技公司或研究机构，在云端和集群上运行数百亿乃至千亿参数级别的大模型，特别是需要高效利用多GPU资源的场合。

1. ## 推理引擎如何实现高效推理？

推理引擎旨在优化生产环境中 LLM 的内存使用和性能。它们通过帮助实现高吞吐量和低延迟来保证 LLM 能够处理大量的请求并快速响应。推理引擎的核心技术主要包括 *KV 缓存管理* 、 *分页机制* 、 *连续批处理* 、 *结构化输出* 、 *张量* *并行* 、 *流水线并行* 、 *专家并行* 和 *低比特量化等* 。

3.1. KV 缓存管理：高效的计算复用机制

在自回归推理场景中，模型每生成一个新 token，都需计算并保存对应的 KV（键-值）缓存，以供后续的 token 解码重复使用。如何管理并最大化复用这些缓存，成为推理性能优化的重要方向。

首个广泛落地这一技术的是 vLLM 项目。vLLM 引入了名为 **PagedAttention** 的机制，通过精细的分页缓存管理，动态按需加载和共享前缀缓存，从而大幅降低了重复计算和显存占用。

随后，SGLang 提出 **RadixAttention** ，基于前缀树结构进一步提高了缓存命中率，尤其适用于多轮对话场景。Hugging Face 的 Text Generation Inference（TGI）也在 2024 年发布的 v3.0 版本中新版本中加入类似机制，使长上下文推理场景显著提速。

3.2. 分页机制：实现显存的精细化管理

传统的 KV 缓存分配通常以最大上下文长度为单位进行整体分配，导致严重的显存浪费。为解决该问题，vLLM 率先提出了基于分页机制（PagedAttention）的精细化内存管理方法。这种方法借鉴操作系统虚拟内存分页的理念，将 KV 缓存分割为固定大小的“小页”，允许动态、非连续存储，从而大幅降低显存碎片率，并显著提升显存利用率。

这种设计随后得到了广泛认可，TGI 和 DeepSpeed-Inference 也在各自的引擎中加入了类似分页式的缓存管理方案，使得该技术成为了主流推理引擎的标配机制。

3.3. 连续批处理：实现动态批量调度

推理场景下的批处理机制传统上为静态批量，即等待批量请求齐全再开始计算，导致 GPU 资源闲置率较高。vLLM 项目首次提出并实现了“连续批处理（Continuous Batching）”的概念，这种机制通过动态地将不同用户的推理请求实时插入到当前运行的批次中，从而最大限度减少 GPU 空转，显著提高硬件利用率与推理吞吐量。

连续批处理将推理过程拆分为两个阶段： **预填充阶段（Prefill）和解码阶段（Decode），并** 将预填充与解码解耦，使得模型在处理长上下文输入（预填充耗时长）和后续逐字生成（单步耗时短）时都能各自批量并行，避免资源闲置。这种PD分离的架构也被月之暗面的 Mooncake 所采用。

目前，连续批处理已广泛被其他推理引擎如 TGI 和 SGLang 等采纳，各项目在此基础上进一步优化了实时拼批策略，推动其成为业界标准做法。

3.4. 结构化输出：规范模型生成内容

模型生成的自由文本在特定应用场景下往往难以直接使用，如 API 调用或 JSON 响应。结构化输出技术通过在生成过程中施加约束（例如正则约束、有限状态机FSM约束），使模型能够直接产出符合预定义格式的结构化数据。

这一技术由 SGLang 项目首创，推出了名为 xGrammar 的约束解码模块，实现对 JSON 和函数调用等格式的强约束，极大简化了应用端的解析工作。随后，vLLM 与 Hugging Face 也分别推出了相似的结构化输出功能，并逐步将这一特性推广为推理引擎的常规功能之一。OpenAI 在其 API 中引入了 “函数调用” 机制，也是一种结构化输出思路。

3.5. 张量并行与流水线并行：多设备模型部署的标准方案

张量并行（Tensor Parallelism, TP）和流水线并行（Pipeline Parallelism, PP）是目前模型规模扩展至数十亿乃至千亿级别时常用的推理部署方法。

TP 将单层网络参数张量分割并分布到多张 GPU 上同时计算，从而突破单 GPU 显存瓶颈；PP 则将模型不同层分割到不同 GPU 上，流水线式执行推理，使单个 GPU 承担的模型层数减少，从而支持更大的模型。

这两种并行策略由 NVIDIA 的 Megatron-LM 首先提出并推广，随后微软的 DeepSpeed 和开源引擎 vLLM、SGLang 等也广泛采纳，形成了面向超大模型推理场景的行业标准解决方案。

3.6. 专家并行：支持稀疏激活的超大模型

专家并行（Expert Parallelism, EP）技术伴随 Mixture-of-Experts (MoE) 模型架构的兴起而受到关注，并在 DeepSeek 的火爆传播中受到更多的重视。MoE 模型通过路由网络选择激活部分专家子模型，从而实现稀疏激活和极大规模参数扩展。EP 将不同的专家分配到不同 GPU 上并行运行，从而有效实现稀疏计算与内存高效管理。

微软 DeepSpeed-MoE 在推理端首次规模化实现了这一技术，使万亿参数级别的 MoE 模型具备实际可用的推理效率。此后，NVIDIA 的 Megatron-Core、NeMo 等平台迅速跟进并在大规模场景中验证了该技术的有效性。vLLM 则计划于 2025 Q2 在主干中正式支持专家并行，进一步促进 MoE 推理的普及。

3.7. 低比特量化：实现模型的高效压缩

低比特量化（如 INT8 或 INT4）是降低大模型推理成本的主流方案。通过精细的权重量化技术，在几乎不损失精度的情况下，大幅压缩模型体积并降低显存占用，使得推理服务的部署成本和硬件门槛显著下降。

开创性的 GPTQ 算法首次实现了百亿至千亿参数模型的 INT4 量化。后续 Hugging Face、vLLM 和 llama.cpp 等主流推理引擎均支持 GPTQ 或类似的量化方案，推动 INT8 和 INT4 量化模型成为业界推理部署的主流模式。

1. ## 结语

大语言模型推理引擎的优化技术百花齐放，从底层内存管理到高层并行策略，各有侧重又相辅相成。这些技术最早由不同项目创新提出，现如今正逐步融合到主流的开源推理引擎中。

对于开发者而言，理解并善用这些优化手段，将有助于在实际AI应用中以更低成本、更高效率部署强大的大语言模型。这场围绕LLM高效推理的持续演进，还将随着模型规模的扩大和新硬件的出现而不断前行。各项优化技术的交融与创新，终将推动大模型走向更加经济高效、稳定可靠的应用阶段。

作为一站式 AIGC 云服务平台，PPIO派欧云提供高性能的 API 服务，涵盖了最新的 DeepSeek R1/V3 Turbo、Qwen 等系列模型，仅需一行代码即可调用；并支持在 Chatbox、AngthingLLM、Ragflow 等 20+ 主流第三方平台调用。

PPInfer 是派欧算力云基于 vLLM 进行二次开发和深度优化的推理引擎。在常规的推理加速技术之上，自研全链路 FP8 量化、KV Cache 稀疏压缩、投机采样等技术，显著提升 LLM 推理性能。PPIO 通过 2024 年的实践，已经实现大模型推理的 10 倍 + 降本，并在成本与性能之间找到最佳平衡。

点击 [PPIO官网](https://link.juejin.cn/?target=https%3A%2F%2Fppinfra.com%2Fuser%2Fregister "https://ppinfra.com/user/register") ，填写邀请码【JUEJIN】注册，即可得获取15元算力代金券，畅享DeepSeek V3/R1以及最新上线的Prover2、Qwen3系列、GLM-4-0414等模型。

评论 0

暂无评论数据