---
title: "56.7倍LLM推理加速！港大联合华为诺亚提出异步测试时扩展框架A1"
source: "https://mp.weixin.qq.com/s/tzjanRlD6JJYzv3A8My8eA"
author:
  - "[[没方]]"
published:
created: 2025-09-26
description: "引入三大技术突破测试时扩展性能瓶颈~"
tags:
  - "异步推理"
  - "测试时扩展"
  - "共形预测"
  - "LLM加速"
abstract: "香港大学与华为诺亚方舟实验室提出异步测试时扩展框架A1，通过共形预测实现56.7倍LLM推理加速。"
---
![cover_image](https://mmbiz.qpic.cn/sz_mmbiz_jpg/DPAHibibAl3vQew0PxDS0fLCh4eyQTwBia9mV8e2MicibO8NEGZHCJhkP6c5JuU1xYichPiaMiaMQ1XcATp8u4HqU2xKSA/0?wx_fmt=jpeg)

Original 没方 [智猩猩GenAI](https://mp.weixin.qq.com/s/) *2025年09月26日 11:41*

智猩猩GenAI整理

编辑：没方

  

大型语言模型（LLM）得益于测试时扩展(test-time scaling)，但现有方法面临诸多挑战，包括严重的同步开销、内存瓶颈以及延迟问题，尤其是在进行长推理链 (Long Reasoning Chains)的推测解码（speculative decoding ）时尤为突出。

  

为了解决上述挑战，香港大学联合华为诺亚方舟实验室等研究者们提出了一种免训练、无损加速的自适应推理框架A1（Asynchronous Test-Time Scaling）。 A1通过优化计算强度分析指出同步开销是主要瓶颈，提出在线校准策略实现异步推理，并设计支持串行与并行扩展的三阶段拒绝采样流程。在MATH、AMC23、AIME24和AIME25数据集上的实验表明，针对多种草稿-目标模型组合，A1在test-time scaling中实现了56.7倍的推理加速和4.14倍的吞吐量提升。

  

![Image](https://mmbiz.qpic.cn/sz_mmbiz_png/DPAHibibAl3vQew0PxDS0fLCh4eyQTwBia9vEG33iaONsc1xRNtgribBuibCVvVuWdic1ILUhVOBuicIevTQER9EZjYkzg/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1#imgIndex=0)

  

- 论文标题：
	A1: Asynchronous Test-Time Scaling via Conformal Prediction
- 论文链接：  
	https://arxiv.org/pdf/2509.15148

  

***01***

**方法**

  

为了识别性能瓶颈，研究团队关注算术强度，它量化了算术单元的利用率。算术强度定义为：

  

![Image](https://mp.weixin.qq.com/s/www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg%20stroke='none'%20stroke-width='1'%20fill='none'%20fill-rule='evenodd'%20fill-opacity='0'%3E%3Cg%20transform='translate(-249.000000,%20-126.000000)'%20fill='%23FFFFFF'%3E%3Crect%20x='249'%20y='126'%20width='1'%20height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

  

其中 ， *F* 表示浮点运算次数（FLOPs），而 *B* 表示访问的字节数。

  

**（1）细化算术强度**

推测解码通过将计算与推测内存访问重叠来加速推理，使多个草稿token能够并行验证。其主要瓶颈是并行评分计算，该过程受计算限制。

  

![Image](https://mp.weixin.qq.com/s/www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg%20stroke='none'%20stroke-width='1'%20fill='none'%20fill-rule='evenodd'%20fill-opacity='0'%3E%3Cg%20transform='translate(-249.000000,%20-126.000000)'%20fill='%23FFFFFF'%3E%3Crect%20x='249'%20y='126'%20width='1'%20height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E) ![Image](https://mp.weixin.qq.com/s/www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg%20stroke='none'%20stroke-width='1'%20fill='none'%20fill-rule='evenodd'%20fill-opacity='0'%3E%3Cg%20transform='translate(-249.000000,%20-126.000000)'%20fill='%23FFFFFF'%3E%3Crect%20x='249'%20y='126'%20width='1'%20height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

  

基于上述观点，异步扩展可被视为一种更为激进的并行化策略。目标模型在并行验证的token数量远超推测解码，这加剧了预填充瓶颈，导致总计算时间远超内存访问时间（如图4a中绿色与黄色线条的对比所示）。此外，由于不同草稿步骤中采样速度的差异引起的同步延迟会直接降低计算资源利用率，而上述算术强度公式没有涵盖这一点。如图3b所示，同步开销随着采样轮数呈指数增长。在并行扩展设置（图3a）中，这种开销则随并发路径的数量呈线性增长。

  

研究团队观察图4a发现，增加采样规模会自然提升计算强度（此时内存访问时间可忽略不计）。但由于同步开销的存在，系统整体推理时间在增加。 **为将同步成本纳入计算强度考量，研究团队定义了一个异步算术强度** *r* ：

  

![Image](https://mp.weixin.qq.com/s/www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg%20stroke='none'%20stroke-width='1'%20fill='none'%20fill-rule='evenodd'%20fill-opacity='0'%3E%3Cg%20transform='translate(-249.000000,%20-126.000000)'%20fill='%23FFFFFF'%3E%3Crect%20x='249'%20y='126'%20width='1'%20height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

  

其中， 是计算时间， 是内存访问时间， 和 分别是计算和内存访问的单位成本。

  

从图4b可以发现，在同步设置下，随着采样的增加异步算术强度 *r* 会减少， **表明在由推测解码支持的test-time scaling中，同步开销成为了主要瓶颈。**

**（2）在线校准策略实现异步推理**

自适应共形预测（Adaptive conformal prediction）通常依赖预留的校准集来确定阈值 ，以保证覆盖率。然而，在test-time scaling设置中，推理期间通常无法获得保留的示例，因此预留一个专用校准集是不切实际的。为了解决这一限制，研究团队提出了一种在线校准策略。

  

对测试集中每个输入 预采样 m 个输出，得到 。放宽一致性分数总和约束，定义为：

  

![Image](https://mp.weixin.qq.com/s/www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg%20stroke='none'%20stroke-width='1'%20fill='none'%20fill-rule='evenodd'%20fill-opacity='0'%3E%3Cg%20transform='translate(-249.000000,%20-126.000000)'%20fill='%23FFFFFF'%3E%3Crect%20x='249'%20y='126'%20width='1'%20height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

  

用于拒绝采样的边际一致性 p 值计算：

  

![Image](https://mp.weixin.qq.com/s/www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg%20stroke='none'%20stroke-width='1'%20fill='none'%20fill-rule='evenodd'%20fill-opacity='0'%3E%3Cg%20transform='translate(-249.000000,%20-126.000000)'%20fill='%23FFFFFF'%3E%3Crect%20x='249'%20y='126'%20width='1'%20height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

  

在此公式中， 表示测试时候选样本 的一致性分数，其中， 代表测试集上第 ξ个输入得到的第 k个样本， 是校准集 的得分。指标函数1(·)在条件满足时返回1。最终得到的一致性分数 p 值控制拒绝采样：如果满足 ，则接受候选样本，确保仅保留高置信度输出。

  

通过选择第 ⌈p⋅n⋅m⌉小的分数来保证1−α水平的边际覆盖（marginalcoverage）；选择第 ⌈p⋅m⌉ 小的分数作为阈值，可实现条件覆盖（conditional coverage）。

  

预算预测：设 *B* 表示预定义的预算（即需要拒绝的候选数）。给定一个test-time输入 ，采样m个候选思维链 并计算对应的p值 。

  

此采样和评估过程是异步进行的：每个候选 独立生成并评估p值，无需与其他候选同步。因此，输出的候选结果会隐式地呈降序排列：

  

![Image](https://mp.weixin.qq.com/s/www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg%20stroke='none'%20stroke-width='1'%20fill='none'%20fill-rule='evenodd'%20fill-opacity='0'%3E%3Cg%20transform='translate(-249.000000,%20-126.000000)'%20fill='%23FFFFFF'%3E%3Crect%20x='249'%20y='126'%20width='1'%20height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

  

通过直接比较每个候选的 p 值与误覆盖率(miscoverage rate)阈值α，对有序集进行划分，从而构建出预测集：

  

![Image](https://mp.weixin.qq.com/s/www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg%20stroke='none'%20stroke-width='1'%20fill='none'%20fill-rule='evenodd'%20fill-opacity='0'%3E%3Cg%20transform='translate(-249.000000,%20-126.000000)'%20fill='%23FFFFFF'%3E%3Crect%20x='249'%20y='126'%20width='1'%20height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

  

该公式确保选中的候选满足共形预测覆盖准则（conformal coverage criterion），可安全保留。

**（3） 三阶段拒绝采样流程**

![Image](https://mp.weixin.qq.com/s/www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg%20stroke='none'%20stroke-width='1'%20fill='none'%20fill-rule='evenodd'%20fill-opacity='0'%3E%3Cg%20transform='translate(-249.000000,%20-126.000000)'%20fill='%23FFFFFF'%3E%3Crect%20x='249'%20y='126'%20width='1'%20height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

  

研究团队采用一个三阶段采样流程，如图5所示，以实现目标拒绝率α的拒绝采样。

  

**草稿模型采样** ： 给定输入tokens ，草稿模型通过从相同的草稿分布 中采样，提出 m个长度为 的候选延续序列，表示为：

  

![Image](https://mp.weixin.qq.com/s/www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg%20stroke='none'%20stroke-width='1'%20fill='none'%20fill-rule='evenodd'%20fill-opacity='0'%3E%3Cg%20transform='translate(-249.000000,%20-126.000000)'%20fill='%23FFFFFF'%3E%3Crect%20x='249'%20y='126'%20width='1'%20height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

  

**验证** ： 对于每个候选采样 ，研究团队在目标模型 下对其进行评分，通过计算logits/似然(likelihood) 并转换为一致性得分。使用校准集，为每个候选计算 p 值，采样在 中被拒绝，而采样在 之外则被接受。

  

**目标模型采样** ：为了节省token预算，在每个回合中，每轮目标侧的token预算是 ：对于 中每个被接受的候选，让目标模型 使用该候选作为前缀继续生成，最多生成 个token，如果遇到结束token则提前停止。

  

**迭代上述阶段，直到检测到最终答案、达到最大轮数或超出整体 token 限制** 。 **如图 5 所示，增加轮数可以实现顺序测试时扩展（蓝色框），而增加每轮候选数可以实现并行测试时扩展（绿色框）。**

*****02*****

**评估**

![Image](https://mp.weixin.qq.com/s/www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg%20stroke='none'%20stroke-width='1'%20fill='none'%20fill-rule='evenodd'%20fill-opacity='0'%3E%3Cg%20transform='translate(-249.000000,%20-126.000000)'%20fill='%23FFFFFF'%3E%3Crect%20x='249'%20y='126'%20width='1'%20height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

  

表1报告了当草稿模型（DM）和目标模型（TM）来自不同模型家族时，异步测试时扩展的实验结果。

  

研究团队在两种不同设置下评估性能，边际覆盖（Mar Acc），用于衡量预定义的预算和预测集是否在测试输入上实现平均对齐；条件覆盖（Con Acc），每个独立的输入实例均能满足该覆盖要求。 可以发现：

  

- 推测解码能够达到与目标模型自身相当的性能。该方法在保持高质量输出的同时有效降低了计算开销，为推理扩展提供了一个可行的解决方案。
- 最具挑战性的数据集AIME24/25在边际覆盖设置下表现出强劲性能，而另外两个数据集（MATH100和AMC23）则在条件覆盖设置下展现出更优异的结果。这表明，边际覆盖能有效地将更多计算资源分配给任务中最困难的部分；而条件覆盖则在单个输入层面确保了可靠的结果，尤其在较简单的任务中能保证每个问题都得到正确解答。
- 尽管推理模型在推理过程中通常会消耗更多token，但 **在相同的token预算限制下，使用推理模型作为草稿模型相比非推理模型能实现更强的扩展性能** 。

  

![Image](https://mp.weixin.qq.com/s/www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg%20stroke='none'%20stroke-width='1'%20fill='none'%20fill-rule='evenodd'%20fill-opacity='0'%3E%3Cg%20transform='translate(-249.000000,%20-126.000000)'%20fill='%23FFFFFF'%3E%3Crect%20x='249'%20y='126'%20width='1'%20height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

  

表2报告了当草稿模型（DM）和目标模型（TM）来自相同模型家族时的性能表现，并将其与仅适用于同系列模型的基线方法（如Speculative Thinking，记为SPECTHINK）进行比较。可以发现：

  

- 当草稿模型和目标模型属于同一家族时，较大的目标模型始终优于其较小的对应模型，证实了家族内部一致性的优势。
- 在不同数据集上，DeepSeek 和 Skywork 在具有挑战性的基准（AIME、AMC）上表现出显著的提升，而 Qwen2.5 在 MATH100 上表现具有竞争力但在更难的任务上明显落后。
- s1.1 的表现通常优于 Qwen，但尚未达到 Skywork 或 DeepSeek 同等水平。
- SpecReason 和 SpecThink的整体表现普遍不如目标模型，表明它们在推理密集场景下的有效性仍然有限。

![Image](https://mp.weixin.qq.com/s/www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg%20stroke='none'%20stroke-width='1'%20fill='none'%20fill-rule='evenodd'%20fill-opacity='0'%3E%3Cg%20transform='translate(-249.000000,%20-126.000000)'%20fill='%23FFFFFF'%3E%3Crect%20x='249'%20y='126'%20width='1'%20height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

  

在边际覆盖和条件覆盖设置下的预算预测准确率，可以验证A1能否在拒绝采样中准确控制目标模型的干预次数，直接反映了共形预测在估计拒绝率方面的精度。

  

在边际覆盖设置下（即于数据集层面预测拒绝率）， 如图6b所示，在整个数据集上的预算预测准确率很高，尤其是在64样本配置下，其绝对误差保持在5%以内。当校准阶段使用 \=500个token且采样阶段也使用 \=500时，误差可进一步控制在2%以内。这直接凸显了构建多样化校准集对于维持高共形预测准确性的重要性。

  

在条件覆盖设置下（其目标是为每个批次提供精确的预算控制）， 如图6a所示，在拒绝率为25%的在线校准设置下，当校准与采样阶段均采用 \=500时，16样本与64样本配置的准确率相近。然而，当校准阶段使用 \=500而采样阶段使用 \=700时，64样本配置能实现显著更高的预算预测准确率。这表明当采样token预算与校准token预算不匹配时，增加并行样本数量可有效提升预算预测的准确率。

  

![Image](https://mp.weixin.qq.com/s/www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg%20stroke='none'%20stroke-width='1'%20fill='none'%20fill-rule='evenodd'%20fill-opacity='0'%3E%3Cg%20transform='translate(-249.000000,%20-126.000000)'%20fill='%23FFFFFF'%3E%3Crect%20x='249'%20y='126'%20width='1'%20height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

  

不同采样设置和token预算下的延迟与吞吐量分析如图8所示，可以看出：

  
i) 如图8(a)所示，SpecReason和SpecThinking的延迟随着采样数量的增加而持续上升，凸显了扩大采样规模的成本。相比之下， **A1方法显著降低了采样延迟，并在条件覆盖设置下实现了最低的推理延迟** 。

  

ii) 在线校准的开销几乎可以忽略不计，尤其在较大样本量时更为明显。

  

iii) 图8(b)展示了SpecReason 和 SpecThinking 方法的吞吐量如何随每个样本的token预算变化，当token预算增大时存在收益递减效应。 **A1方法即使在超大预算下仍能保持高吞吐量，尤其在边际覆盖设置下表现突出。**

  

在16个样本且每个样本最大token预算为8192的设置下， **A1相比基线方法实现了56.7倍的推理加速和4.14倍的吞吐量提升。**

  

![Image](https://mp.weixin.qq.com/s/www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg%20stroke='none'%20stroke-width='1'%20fill='none'%20fill-rule='evenodd'%20fill-opacity='0'%3E%3Cg%20transform='translate(-249.000000,%20-126.000000)'%20fill='%23FFFFFF'%3E%3Crect%20x='249'%20y='126'%20width='1'%20height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

  

图7展示了16个样本设置下的token消耗情况。这包括仅由目标模型或草稿模型执行的采样，以及在条件覆盖和边际覆盖设置下的异步采样。与草稿模型和目标模型这两个基线相比， **A1能够显著降低token消耗，尤其在条件覆盖设置下效果更为突出** ，因为该方法支持在实例层面进行预算预测。

  

**END**

  

**点击下方名片 即刻关注我们**

继续滑动看下一个

智猩猩GenAI

向上滑动看下一个