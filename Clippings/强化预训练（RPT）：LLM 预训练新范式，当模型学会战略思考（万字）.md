---
title: 强化预训练（RPT）：LLM 预训练新范式，当模型学会战略思考（万字）
source: https://mp.weixin.qq.com/s/cNc4QtiSmphD15MXfEEEpA
author:
  - "[[肆零柒]]"
published: 
created: 2025-06-12
description: RPT通过将传统的预测 token 重构为定义任务，交付结果。这种推理范式的转变，让模型不仅是在预测 token，而在自己定义“任务”，推理“任务”。这一过程赋予了模型类似人类的战略思考模式。它不再只是机械地记忆和预测，而是在直接交付结果！
tags:
  - RPT
  - 强化学习预训练
---
Original 肆零柒 [觉察流](https://mp.weixin.qq.com/s/)

*2025年06月12日 08:15* *上海*

*点击👇🏻可关注，文章来自*

🙋‍♂️ 想加入社群的朋友，可看文末方法，进群交流。

---

  

**“** RPT 通过将传统的 next-token-prediction 重构为 next-token-reasoning 任务，并引入强化学习机制，这种推理范式转化，显著提升了模型的推理能力和语言建模性能。让模型不仅是在预测 token，而在自己定义“任务”，推理“任务”。这一过程赋予了模型类似人类的战略思考模式。它不再只是机械地记忆和预测，而是在直接交付结果！”

  

大家好，我是肆〇柒。在当下，大型语言模型（LLM）正以其卓越的能力在诸多任务中引人瞩目。这些能力的提升，很大程度上得益于在大规模文本数据上的 next-token-prediction 自监督学习范式。通过预测文本中的下一个 token，LLM 能够学习到语言的复杂模式和语义关系，从而在文本生成、问答系统、机器翻译等任务中取得显著成果。

然而，随着对模型性能要求的不断提高，强化学习（RL）逐渐成为微调 LLM 的关键技术。它能够使模型更好地对齐人类偏好，或者提升特定技能，如复杂推理等。但 RL 在实际应用中面临诸多挑战：一方面，基于人类反馈的强化学习（RLHF）严重依赖昂贵的人类偏好数据，这不仅限制了其可扩展性，还可能导致奖励劫持问题；另一方面，基于可验证奖励的强化学习（RLVR）虽能避免一些奖励劫持问题，却因标注数据的稀缺性，难以在通用预训练场景中广泛应用。（扩展阅读👉《[IBM 研究：可验证奖励强化学习（RLVR）通过 GRPO 提升模型推理能力（万字）](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247488749&idx=1&sn=bc48976b6afc36ffddfa20c086c252b9&scene=21#wechat_redirect)》）

我曾在觉察流的社区群里提到过，o3 的一些能力非常强，甚至在跨领域知识的问答方面也很出彩。当时我有一个猜想，它怎么做到的？我的答案是，它可能在预测 CoT，而今天看到了这篇来自微软研究院、北京大学和清华大学研究团队的论文《Reinforcement Pre-Training》，让我对这一点猜测又确信了几分。

![Image](https://mmbiz.qpic.cn/sz_mmbiz_png/E1DQUYfcS0Jt019yb5asRlnB0q4ZUa0iciajhNUia6sqHINzNrCe6zwhJ9TNR1v3mkpFHNqJCUweBshj2ab7YOkdg/640?wx_fmt=png&from=appmsg&watermark=1&tp=webp&wxfrom=5&wx_lazy=1)

鉴于刚才所提到的挑战，这篇论文所论述的强化预训练（Reinforcement Pre-Training，RPT）被提出。RPT 为了弥合可扩展自监督预训练与强化学习优势之间的差距，创新性地将 next-token-prediction 任务重构为 next-token-reasoning 任务，利用大量无标注文本数据进行通用目的强化学习。这既能够显著提升语言建模的准确性，也为后续的强化微调奠定了坚实基础，有望推动 LLM 向更智能、更通用的方向发展。

![Image](https://mmbiz.qpic.cn/sz_mmbiz_png/E1DQUYfcS0Jt019yb5asRlnB0q4ZUa0icMJHzvMicic2Uu0dyfUKucAf5MmYOsBlBSeor0GoO2tRB1v30vKqxagFA/640?wx_fmt=png&from=appmsg&watermark=1&tp=webp&wxfrom=5&wx_lazy=1)

# RPT 的核心概念：从预测到推理的认知进化

### RPT 基本原理：next-token-prediction 的重构

RPT 的核心思想在于对传统的 next-token-prediction 进行重构，将其转变为一个推理任务。在常规的 next-token-prediction 中，模型仅仅是基于前面的文本信息直接预测下一个 token，这种方式主要侧重于学习文本表面的模式和关联。然而，RPT 引入了截然不同的机制，它要求模型必须先生成一个推理序列，再进行下一个 token 的预测。这一过程极具创新性，类似于人类在面对问题时的思考过程，即先分析已有的信息，进行一系列的推理和思考，然后再得出结论。

例如，当模型在处理一段关于物理定律的文本时，若要预测下一个 token，它并非直接根据已有的词频统计等简单模式来进行预测，而是需要先理解前面文本中提到的物理概念、定律的适用条件等关键信息。然后，基于这些理解，生成诸如 “考虑到作用在物体上的力与质量的关系，接下来可能会提及加速度” 等推理序列，最终再确定下一个 token 是 “加速度” 或其他相关词汇。通过这种方式，模型能够深入挖掘文本背后的语义和知识，而不仅仅停留在表面的 token 级相关性上。

### RPT 的多重优势

**可扩展性和通用性** ：RPT 实现了在无标注文本数据上的通用目的强化学习，这是一次重大的突破。传统上，强化学习在 LLM 中的应用往往受限于数据的标注要求，无论是基于人类反馈的数据还是带有可验证答案的标注数据，都难以大规模获取。然而，RPT 完全依赖于无标注的文本数据，这意味着它可以充分利用互联网上海量的文本资源。无论是新闻报道、学术文献，还是社交媒体上的帖子等各种文本数据，都可以成为 RPT 的训练素材。这极大地拓展了模型训练的数据来源，使其能够涵盖各种不同的领域、主题和语言风格，为 LLM 的通用性提供了坚实的数据基础。例如，利用大量的文学作品训练 RPT 模型，使其能够更好地理解和生成具有文学风格的文本，如小说创作、诗歌生成等；在技术文档领域的应用，则有助于模型准确地理解和生成复杂的代码文档、技术报告等内容。

**降低奖励劫持风险** ：在强化学习中，奖励劫持一直是一个令人头疼的问题。一些复杂的、基于学习的奖励模型可能会出现漏洞，模型可能会利用这些漏洞来获取高奖励，而并非通过真正有价值的学习行为。RPT 则巧妙地避免了这一问题，它采用直接的规则奖励信号，即根据预测的正确性给予奖励。这种奖励机制简单而有效，只关心模型预测的下一个 token 是否与实际文本匹配，而不涉及复杂的、容易被操纵的奖励模型。例如，在预测数学问题解答文本中的下一个 token 时，只有当模型准确地预测出正确的数学符号或概念词汇时，才会获得奖励。这使得模型能够专注于学习真正有价值的推理和预测能力，确保训练过程的稳定性和目标的准确性。

**促进泛化而非死记硬背** ：RPT 通过鼓励模型生成推理序列，促使模型深入理解文本背后的逻辑和知识。这种方式使得模型能够跳出单纯的记忆模式，转而培养起强大的泛化能力。在传统的训练方式下，模型可能会倾向于记忆训练数据中的常见表达模式和 token 顺序，从而在面对新的、未见过的文本时表现不佳。而 RPT 则引导模型在训练过程中主动思考文本的内在逻辑和语义关系，探索多种可能的推理路径。例如，在处理一段关于历史事件的文本时，模型不会仅仅记住某个历史事件的固定表述，而是会思考事件发生的原因、影响等相关因素。这样一来，当遇到关于同一历史时期但不同具体事件的文本时，模型也能够基于已有的知识和推理能力进行准确的预测和理解，大大增强了其在不同文本场景下的适应性和泛化性能。

**提升推理过程中的计算资源分配效率** ：RPT 在预训练阶段就巧妙地引入了推理过程，这相当于为模型分配了更多的 “思考” 时间。在传统的训练方式中，模型主要侧重于对下一个 token 的直接预测，而在 RPT 中，模型需要先进行推理序列的生成，然后再进行预测。这个过程使得模型在训练时就能够充分地利用计算资源，对每个 token 的预测进行更深入的思考和分析。类似于在推理时扩展（inference-time scaling）中为每个预测分配更多的计算资源来提升准确性，RPT 将这种计算资源的高效利用提前到了训练阶段。例如，在训练过程中，模型可能会花费更多的计算资源来分析上下文中的关键信息、探索多种可能的推理路径等，从而在训练完成后，能够在推理过程中更快速、更准确地进行预测，提高整体的性能表现。

# RPT 的方法论：构建智能模型的精巧架构

### 预训练任务：next-token-reasoning

#### 任务定义与推理序列生成算法

在 RPT 框架下，next-token-reasoning 任务彻底革新了传统语言模型的训练方式。其核心在于，不再仅仅把下一个 token 的预测当作一个简单的概率估计问题，而是将其转化为一个充满思考过程的推理任务。给定输入序列 ，模型需要将前缀  视为上下文，并生成一个推理序列 ，最终预测下一个 token 。这一过程，就像为模型配备了一个 “思考引擎”，使其能够在预测之前，先对已有的信息进行深度加工和分析。

具体生成算法如下：

1. 1. **初始化** ：以输入序列为起点，模型首先对上下文进行编码，提取关键语义信息。这一步骤就像人类在阅读一篇文章时，先快速浏览开头部分，对文章的主题和大致方向有一个初步的把握。例如，当输入的上下文是关于物理定律的描述时，模型会识别出其中涉及的物理概念、定律的名称等关键信息，为后续的推理和预测奠定基础。
2. 2. **迭代推理** ：在每一步推理中，模型基于当前上下文和已生成的推理序列，生成下一个推理 token。这一过程会考虑语义连贯性、语法正确性以及与最终预测目标的相关性。例如，在处理数学问题时，模型可能会生成诸如 “考虑变量之间的关系”“应用定理公式” 等推理 token。这就好比人类在解决数学问题时，会一步步地分析问题的条件、应用相关的数学定理和公式，逐步向答案靠近。每一个推理 token 都是模型思考过程中的一个 “脚印”，记录着它对问题的逐步深入理解。
3. 3. **预测生成** ：在完成推理序列后，模型基于推理序列和原始上下文，生成对下一个 token 的预测。预测过程会综合推理序列中的信息，以确定最可能的 token。以数学问题为例，经过一系列的推理 token 后，模型可能会预测下一个 token 是某个数学符号或特定的数值，这个预测结果是基于前面的推理过程得出的，具有较高的可信度和准确性。
4. 4. **算法终止** ：当达到预设的推理序列长度或满足特定终止条件（如预测置信度超过阈值）时，算法终止，输出推理序列和预测 token。终止条件的设置是为了在推理的充分性和计算效率之间取得平衡，确保模型能够在合理的时间和计算资源内完成任务。

我们可以通过一个形象的比喻来理解 RPT 的这一创新过程：如果传统的 next-token-prediction 是在黑暗中直接猜测下一步的位置，那么 RPT 的 next-token-reasoning 就像是在黑暗中先点亮一棵“树形”的路径，照亮周围的环境，分析路径的可能性，然后再迈出下一步。这种转变，提升了模型预测的准确性，赋予了模型更接近人类思考方式的能力（战略思考）。下图所示，RPT 通过强化学习激励模型进行推理并准确预测下一个 token，使得强化学习能够扩展应用于大规模网络文本语料库。

![Image](https://mmbiz.qpic.cn/sz_mmbiz_png/E1DQUYfcS0Jt019yb5asRlnB0q4ZUa0ic4RvaJUiblFaYObnCOTscSAGgHPiaGWJkXjeNPKALWwcbeEacfbCz2FCQ/640?wx_fmt=png&from=appmsg&watermark=1&tp=webp&wxfrom=5&wx_lazy=1)

强化预训练（Reinforcement Pre-Training, RPT）将下一个token预测重新定义为一个推理任务，其中语言模型通过强化学习（Reinforcement Learning, RL）被激励去推理并正确预测下一个token。所提出的这种方法使得强化学习能够扩展到网络文本语料库。樱桃蛋糕顶部的图片取自LeCun的PPT

上图展示了 RPT 如何将 next-token-prediction 重新定义为一个推理任务，通过强化学习激励模型进行推理并正确预测下一个 token。这种设计使得 RPT 能够扩展强化学习的应用范围，使其适用于大规模网络文本语料库。

### 推理模式原理与实现机制

模型实现联想、假设验证等推理模式的关键在于其内部的算法设计和结构优化。以联想模式为例，模型通过构建语义关联图谱，将上下文中的关键词与知识库中的相关信息进行链接。当处理一段关于 “气候变化” 的文本时，模型会基于上下文中的关键词 “温室气体”“全球变暖” 等，联想出相关的概念如 “碳排放”“极地冰川融化” 等。这一过程通过模型内部的注意力机制和语义嵌入层实现，注意力机制能够捕捉上下文中的关键信息，并将其与知识库中的内容进行匹配，而语义嵌入层则将这些信息映射到同一语义空间，便于模型进行联想和关联。

假设验证模式则依赖于模型的概率推理模块。模型会根据当前上下文和推理序列，生成多个可能的假设，然后通过计算每个假设的置信度来验证其合理性。例如，在处理一段历史事件的文本时，模型可能会生成 “该事件的起因可能是经济因素”“该事件的起因可能是政治因素” 等多个假设。通过对上下文信息的深度分析和对历史知识的调用，模型计算每个假设成立的概率，从而筛选出最合理的假设作为后续推理的基础。这一过程涉及到模型内部复杂的概率计算和逻辑判断机制，确保假设验证的准确性和有效性。

为了更直观地理解标准 next-token-prediction 和 RPT 的 next-token-reasoning 之间的差异，我们可以参考下图。

![Image](https://mmbiz.qpic.cn/sz_mmbiz_png/E1DQUYfcS0Jt019yb5asRlnB0q4ZUa0icAechGXee5hvvsSszBKAicBbxvDd5r6zCcM3bypJyaCq6TsnElS3toug/640?wx_fmt=png&from=appmsg&watermark=1&tp=webp&wxfrom=5&wx_lazy=1)

next-token prediction 与 next-token reasoning 的范式比较。标准的 next-token predict 直接估计预训练语料库中的下一个token，而next-token reasoning 则在进行预测之前会对多个token进行推理

标准 next-token-prediction 直接估计预训练语料库中的下一个 token，而 RPT 的 next-token-reasoning 则在进行预测之前，先在多个 token 上进行推理。这种差异使得 RPT 能够更深入地理解文本的语义结构，从而提高预测的准确性。

### 推理序列长度与复杂度调整策略及量化关系

RPT 模型根据不同场景灵活调整推理序列的长度和复杂度，以适应各种不同的任务需求。这种调整并非随意为之，而是基于严谨的量化关系和评估指标。

在任务需求分析方面，引入文本语义复杂度（）和上下文语义连贯性（）作为评估指标。文本语义复杂度通过分析文本中概念的抽象程度、语义关系的层次结构等进行量化，例如，数学证明文本的语义复杂度通常较高，因为其中包含多个抽象的数学概念和复杂的逻辑关系。上下文语义连贯性则通过衡量上下文之间的语义关联紧密程度和逻辑一致性来确定，如日常对话文本的语义连贯性相对较高。

在模型性能评估方面，建立模型性能（）与推理序列参数之间的量化关系模型。通过大量的实验数据，发现模型性能与推理序列长度（）和复杂度（）之间存在显著的正相关关系。那么，随着推理序列长度和复杂度的增加，模型在复杂任务上的性能呈现明显的提升趋势，但在简单任务上可能会出现性能饱和甚至轻微下降的情况。这是因为在简单任务中，过长或过于复杂的推理序列可能会引入不必要的噪声，影响模型的预测效率和准确性。

基于上述量化关系，提出了以下调整策略：

- • 对于语义复杂度高（）且语义连贯性低（）的任务，如数学证明、科学文献分析等，增加推理序列的长度至 （α 为调整系数）和复杂度至 （β 为调整系数）。例如，当处理一篇复杂的数学研究论文时，模型会生成较长且复杂的推理序列，以充分挖掘文本中的深层语义和逻辑关系。
- • 对于语义复杂度低（）且语义连贯性高（）的任务，如日常对话、简单叙述性文本等，适当缩短推理序列的长度至 <svg style="display: initial;flex-shrink: 0;max-width: 300vw !important;width: 24.815ex;" aria-hidden="true" viewBox="0 -750 10968.3 1037.2" role="img" height="2.347ex" xmlns="http://www.w3.org/2000/svg"><g transform="scale(1,-1)" stroke-width="0" fill="currentColor" stroke="currentColor"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z" data-c="1D43F"></path></g><g data-mjx-texclass="ORD" transform="translate(714,-150) scale(0.707)" data-mml-node="TeXAtom"><g data-mml-node="mi"><path d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z" data-c="1D460"></path></g><g transform="translate(469,0)" data-mml-node="mi"><path d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z" data-c="1D452"></path></g><g transform="translate(935,0)" data-mml-node="mi"><path d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z" data-c="1D45E"></path></g></g></g><g transform="translate(2028.2,0)" data-mml-node="mo"><path d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z" data-c="3D"></path></g><g transform="translate(3084,0)" data-mml-node="mi"><path d="M31 249Q11 249 11 258Q11 275 26 304T66 365T129 418T206 441Q233 441 239 440Q287 429 318 386T371 255Q385 195 385 170Q385 166 386 166L398 193Q418 244 443 300T486 391T508 430Q510 431 524 431H537Q543 425 543 422Q543 418 522 378T463 251T391 71Q385 55 378 6T357 -100Q341 -165 330 -190T303 -216Q286 -216 286 -188Q286 -138 340 32L346 51L347 69Q348 79 348 100Q348 257 291 317Q251 355 196 355Q148 355 108 329T51 260Q49 251 47 251Q45 249 31 249Z" data-c="1D6FE"></path></g><g transform="translate(3849.2,0)" data-mml-node="mo"><path d="M229 286Q216 420 216 436Q216 454 240 464Q241 464 245 464T251 465Q263 464 273 456T283 436Q283 419 277 356T270 286L328 328Q384 369 389 372T399 375Q412 375 423 365T435 338Q435 325 425 315Q420 312 357 282T289 250L355 219L425 184Q434 175 434 161Q434 146 425 136T401 125Q393 125 383 131T328 171L270 213Q283 79 283 63Q283 53 276 44T250 35Q231 35 224 44T216 63Q216 80 222 143T229 213L171 171Q115 130 110 127Q106 124 100 124Q87 124 76 134T64 161Q64 166 64 169T67 175T72 181T81 188T94 195T113 204T138 215T170 230T210 250L74 315Q65 324 65 338Q65 353 74 363T98 374Q106 374 116 368T171 328L229 286Z" data-c="2217"></path></g><g transform="translate(4571.4,0)" data-mml-node="msub"><g data-mml-node="mi"><path d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z" data-c="1D436"></path></g><g data-mjx-texclass="ORD" transform="translate(748,-150) scale(0.707)" data-mml-node="TeXAtom"><g data-mml-node="mi"><path d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z" data-c="1D461"></path></g><g transform="translate(361,0)" data-mml-node="mi"><path d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z" data-c="1D452"></path></g><g transform="translate(827,0)" data-mml-node="mi"><path d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z" data-c="1D465"></path></g><g transform="translate(1399,0)" data-mml-node="mi"><path d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z" data-c="1D461"></path></g></g></g><g transform="translate(6613.9,0)" data-mml-node="mi"><text font-style="italic" font-family="serif" font-size="884px" transform="scale(1,-1)" data-variant="italic"><tspan leaf="">（</tspan></text></g><g transform="translate(7567.8,0)" data-mml-node="mi"><path d="M31 249Q11 249 11 258Q11 275 26 304T66 365T129 418T206 441Q233 441 239 440Q287 429 318 386T371 255Q385 195 385 170Q385 166 386 166L398 193Q418 244 443 300T486 391T508 430Q510 431 524 431H537Q543 425 543 422Q543 418 522 378T463 251T391 71Q385 55 378 6T357 -100Q341 -165 330 -190T303 -216Q286 -216 286 -188Q286 -138 340 32L346 51L347 69Q348 79 348 100Q348 257 291 317Q251 355 196 355Q148 355 108 329T51 260Q49 251 47 251Q45 249 31 249Z" data-c="1D6FE"></path></g><g transform="translate(8388.5,0)" data-mml-node="mo"><path d="M694 -11T694 -19T688 -33T678 -40Q671 -40 524 29T234 166L90 235Q83 240 83 250Q83 261 91 266Q664 540 678 540Q681 540 687 534T694 519T687 505Q686 504 417 376L151 250L417 124Q686 -4 687 -5Q694 -11 694 -19Z" data-c="3C"></path></g><g transform="translate(9444.3,0)" data-mml-node="mi"><path d="M34 156Q34 270 120 356T309 442Q379 442 421 402T478 304Q484 275 485 237V208Q534 282 560 374Q564 388 566 390T582 393Q603 393 603 385Q603 376 594 346T558 261T497 161L486 147L487 123Q489 67 495 47T514 26Q528 28 540 37T557 60Q559 67 562 68T577 70Q597 70 597 62Q597 56 591 43Q579 19 556 5T512 -10H505Q438 -10 414 62L411 69L400 61Q390 53 370 41T325 18T267 -2T203 -11Q124 -11 79 39T34 156ZM208 26Q257 26 306 47T379 90L403 112Q401 255 396 290Q382 405 304 405Q235 405 183 332Q156 292 139 224T121 120Q121 71 146 49T208 26Z" data-c="1D6FC"></path></g><g transform="translate(10084.3,0)" data-mml-node="mi"><text font-style="italic" font-family="serif" font-size="884px" transform="scale(1,-1)" data-variant="italic"><tspan leaf="">）</tspan></text></g></g></g></svg>和降低复杂度至 <svg style="display: initial;flex-shrink: 0;max-width: 300vw !important;width: 24.532ex;" aria-hidden="true" viewBox="0 -750 10843.3 1037.2" role="img" height="2.347ex" xmlns="http://www.w3.org/2000/svg"><g transform="scale(1,-1)" stroke-width="0" fill="currentColor" stroke="currentColor"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path d="M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z" data-c="1D437"></path></g><g data-mjx-texclass="ORD" transform="translate(861,-150) scale(0.707)" data-mml-node="TeXAtom"><g data-mml-node="mi"><path d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z" data-c="1D460"></path></g><g transform="translate(469,0)" data-mml-node="mi"><path d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z" data-c="1D452"></path></g><g transform="translate(935,0)" data-mml-node="mi"><path d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z" data-c="1D45E"></path></g></g></g><g transform="translate(2175.2,0)" data-mml-node="mo"><path d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z" data-c="3D"></path></g><g transform="translate(3231,0)" data-mml-node="mi"><path d="M195 609Q195 656 227 686T302 717Q319 716 351 709T407 697T433 690Q451 682 451 662Q451 644 438 628T403 612Q382 612 348 641T288 671T249 657T235 628Q235 584 334 463Q401 379 401 292Q401 169 340 80T205 -10H198Q127 -10 83 36T36 153Q36 286 151 382Q191 413 252 434Q252 435 245 449T230 481T214 521T201 566T195 609ZM112 130Q112 83 136 55T204 27Q233 27 256 51T291 111T309 178T316 232Q316 267 309 298T295 344T269 400L259 396Q215 381 183 342T137 256T118 179T112 130Z" data-c="1D6FF"></path></g><g transform="translate(3897.2,0)" data-mml-node="mo"><path d="M229 286Q216 420 216 436Q216 454 240 464Q241 464 245 464T251 465Q263 464 273 456T283 436Q283 419 277 356T270 286L328 328Q384 369 389 372T399 375Q412 375 423 365T435 338Q435 325 425 315Q420 312 357 282T289 250L355 219L425 184Q434 175 434 161Q434 146 425 136T401 125Q393 125 383 131T328 171L270 213Q283 79 283 63Q283 53 276 44T250 35Q231 35 224 44T216 63Q216 80 222 143T229 213L171 171Q115 130 110 127Q106 124 100 124Q87 124 76 134T64 161Q64 166 64 169T67 175T72 181T81 188T94 195T113 204T138 215T170 230T210 250L74 315Q65 324 65 338Q65 353 74 363T98 374Q106 374 116 368T171 328L229 286Z" data-c="2217"></path></g><g transform="translate(4619.4,0)" data-mml-node="msub"><g data-mml-node="mi"><path d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z" data-c="1D436"></path></g><g data-mjx-texclass="ORD" transform="translate(748,-150) scale(0.707)" data-mml-node="TeXAtom"><g data-mml-node="mi"><path d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z" data-c="1D461"></path></g><g transform="translate(361,0)" data-mml-node="mi"><path d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z" data-c="1D452"></path></g><g transform="translate(827,0)" data-mml-node="mi"><path d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z" data-c="1D465"></path></g><g transform="translate(1399,0)" data-mml-node="mi"><path d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z" data-c="1D461"></path></g></g></g><g transform="translate(6661.9,0)" data-mml-node="mi"><text font-style="italic" font-family="serif" font-size="884px" transform="scale(1,-1)" data-variant="italic"><tspan leaf="">（</tspan></text></g><g transform="translate(7615.8,0)" data-mml-node="mi"><path d="M195 609Q195 656 227 686T302 717Q319 716 351 709T407 697T433 690Q451 682 451 662Q451 644 438 628T403 612Q382 612 348 641T288 671T249 657T235 628Q235 584 334 463Q401 379 401 292Q401 169 340 80T205 -10H198Q127 -10 83 36T36 153Q36 286 151 382Q191 413 252 434Q252 435 245 449T230 481T214 521T201 566T195 609ZM112 130Q112 83 136 55T204 27Q233 27 256 51T291 111T309 178T316 232Q316 267 309 298T295 344T269 400L259 396Q215 381 183 342T137 256T118 179T112 130Z" data-c="1D6FF"></path></g><g transform="translate(8337.5,0)" data-mml-node="mo"><path d="M694 -11T694 -19T688 -33T678 -40Q671 -40 524 29T234 166L90 235Q83 240 83 250Q83 261 91 266Q664 540 678 540Q681 540 687 534T694 519T687 505Q686 504 417 376L151 250L417 124Q686 -4 687 -5Q694 -11 694 -19Z" data-c="3C"></path></g><g transform="translate(9393.3,0)" data-mml-node="mi"><path d="M29 -194Q23 -188 23 -186Q23 -183 102 134T186 465Q208 533 243 584T309 658Q365 705 429 705H431Q493 705 533 667T573 570Q573 465 469 396L482 383Q533 332 533 252Q533 139 448 65T257 -10Q227 -10 203 -2T165 17T143 40T131 59T126 65L62 -188Q60 -194 42 -194H29ZM353 431Q392 431 427 419L432 422Q436 426 439 429T449 439T461 453T472 471T484 495T493 524T501 560Q503 569 503 593Q503 611 502 616Q487 667 426 667Q384 667 347 643T286 582T247 514T224 455Q219 439 186 308T152 168Q151 163 151 147Q151 99 173 68Q204 26 260 26Q302 26 349 51T425 137Q441 171 449 214T457 279Q457 337 422 372Q380 358 347 358H337Q258 358 258 389Q258 396 261 403Q275 431 353 431Z" data-c="1D6FD"></path></g><g transform="translate(9959.3,0)" data-mml-node="mi"><text font-style="italic" font-family="serif" font-size="884px" transform="scale(1,-1)" data-variant="italic"><tspan leaf="">）</tspan></text></g></g></g></svg>。这样可以提高训练效率，同时避免过度推理带来的性能损耗。例如，在处理日常对话文本时，模型会生成较短且简单的推理序列，快速地基于上下文信息进行预测。

### 基于强化学习的预训练（RPT：数据与模型的协同优化）

#### 训练过程与参数更新机制的数学解析

强化学习在 RPT 中的运用是提升模型性能的关键所在。对于给定的前缀 x<t，模型使用当前策略生成 G 个响应（包括推理序列和最终预测序列）。通过前缀匹配奖励验证预测的正确性，利用奖励信号更新模型参数。这一过程就像一个导师在指导学生思考和回答问题，根据学生的回答是否正确给予反馈，并帮助学生不断改进自己的思考方式和答案准确性。

模型在每个训练步骤中会生成多个不同的思考轨迹，每个轨迹包含一个推理序列和对应的下一个 token 预测。然后，对比预测结果与真实值来计算奖励，并根据奖励值调整模型参数，鼓励生成更准确的预测结果。参数更新采用 PPO 算法，其数学原理和参数更新机制如下：

1. 1. **初始化参数** ：设置初始学习率（如 1e-6）、PPO 小批次大小（如 256）等超参数。学习率决定了模型参数更新的步长，较大的学习率可能导致模型在训练过程中发散，而较小的学习率则会使训练过程缓慢。PPO 小批次大小决定了每次更新时使用的样本数量，合适的批次大小能够保证模型更新的稳定性和效率。
2. 2. **生成样本** ：在当前策略下，对每个前缀生成 G 个响应样本 ，其中 ，表示推理序列，表示预测的 token。这些样本代表了模型在当前状态下对不同思考路径的探索，通过生成多个样本，模型能够更全面地了解不同推理序列和预测结果的可能性。
3. 3. **计算奖励** ：根据预测结果与真实值的匹配情况，计算每个样本的奖励 。奖励机制定义为：
	其中， 表示真实的下一个 token， 表示预测 token 的字节长度，和  分别为有效字节长度的上下界。奖励机制是强化学习的核心部分之一，它决定了模型的学习方向和目标。在 RPT 中，只有当模型的预测结果与真实值完全匹配，并且满足一定的条件（如字节长度符合有效边界）时，才会给予正向奖励，否则给予零奖励。这种奖励机制简洁而明确，引导模型专注于生成准确的预测结果。
4. 4. **计算优势函数**：为了更有效地更新模型参数，引入优势函数，它表示在状态下，当前策略相对于基准策略的优势。优势函数的计算公式为：  
	  
	其中，是状态-动作值函数，表示在状态下执行当前策略的期望累积奖励；是状态值函数，表示在状态下的期望累积奖励。
5. 5. **更新参数** ：利用 PPO 算法，基于奖励信号和优势函数更新模型参数 θ。PPO 算法通过优化目标函数来更新模型参数，目标函数为：

其中， 表示旧策略，ε 是一个超参数，用于控制策略更新的范围，防止策略更新过大而导致训练不稳定。通过最大化目标函数 ，模型能够逐步改进策略，提高预测的准确性。PPO 算法通过优化目标函数，使模型在当前位置的策略得到改进，提高预测的准确性。通过多个迭代周期，模型不断学习和调整自己的策略，逐步提升性能。

![Image](https://mmbiz.qpic.cn/sz_mmbiz_png/E1DQUYfcS0Jt019yb5asRlnB0q4ZUa0icXmULnp6Ftoreoj5P5jSVuy0vMmUqyf8s2Q80T3hgMvPEdoEcZvXJAw/640?wx_fmt=png&from=appmsg&watermark=1&tp=webp&wxfrom=5&wx_lazy=1)

强化预训练（RPT）的示例说明

上图直观地展示了 RPT 的训练过程。给定一个带有缺失后续内容的上下文，LLM 执行策略性展开以生成 G 个不同的思考轨迹。每个轨迹包含一个中间推理步骤和一个对下一个 token 的最终预测。如果预测与真实 token 匹配，则赋予正向奖励；否则，奖励为零。这一奖励信号用于更新 LLM，鼓励生成更准确的延续内容。

### 奖励机制优化方法与动态调整策略

奖励机制在强化学习中起着至关重要的作用。在 RPT 中，奖励的定义基于累计字节长度和有效边界。对于预测结果与真实值完全匹配且字节长度符合有效边界的情况，给予正向奖励；否则给予零奖励。这种设计确保奖励的准确性和合理性。然而，为了进一步优化奖励机制，可以采用以下方法：

1. 1. **动态调整奖励阈值** ：根据训练进度和模型性能，动态调整奖励的匹配严格程度。在训练初期，模型可能还处于探索阶段，此时适当放宽匹配条件，例如允许预测结果与真实值在一定范围内相似即可获得部分奖励，鼓励模型积极探索多种推理路径。随着训练的进行，模型逐渐具备更强的推理能力，此时逐步提高匹配严格度，要求预测结果必须与真实值完全一致才能获得奖励，从而确保模型生成的预测结果更加精确。具体调整策略如下：  
	在训练的前 30% 迭代周期内，设置奖励阈值为 ，其中  是一个容差值，允许预测 token 的字节长度在一定范围内波动。随着训练的进行，逐渐减小，到训练后期（后 30% 迭代周期），ΔL 趋近于 0，确保预测结果的精确性。
2. 2. **引入奖励塑造** ：在原始奖励基础上，结合模型的中间推理结果，设计奖励塑造函数。例如，对于推理序列中包含有效逻辑步骤的情况，给予额外的奖励激励。奖励塑造能够引导模型生成更高质量的推理过程，而不仅仅关注最终的预测结果。具体奖励塑造函数定义为：  
	  
	其中， 是原始奖励， 是一个指示函数，用于判断推理序列第 t 个 token 是否符合逻辑（如正确应用数学定理、逻辑推导合理等），γ 是折扣因子，用于平衡即时奖励和延迟奖励的影响。如，在推理序列中，如果模型能够正确地应用某个关键的数学定理或者逻辑规则，即使最终预测结果不完全正确，也会因为这个有效的推理步骤而获得一定的奖励，这样可以促使模型在推理过程中更加注重逻辑的合理性和正确性。

### 预训练设置：数据与模型的协同优化

**数据集选择与预处理优化** ：RPT 使用 OmniMATH 数据集进行训练，该数据集包含 4,428 道竞赛级数学问题和解决方案，来源于 AoPS Wiki 和 AoPS 论坛等。在预处理过程中，利用 DeepseekR1-Distill-Qwen-1.5B 小代理模型对 token 进行熵计算，设置熵阈值以过滤掉低熵位置，优先训练具有挑战性的 token。为了进一步优化数据预处理，可以采用以下方法：

- • **多级熵过滤** ：除了基于单个 token 的熵过滤，还可以考虑上下文窗口内的 token 熵分布，进行多级过滤。例如，对于连续多个低熵 token 的区域，可能表示简单的重复或常见短语，可以整体过滤掉，从而更高效地聚焦于具有挑战性的文本部分。
- • **数据增强** ：对过滤后的数据进行数据增强操作，如同义词替换、语句重组等，增加数据的多样性和模型的泛化能力。

**模型与训练参数优化** ：基础模型选择为 Deepseek-R1-Distill-Qwen-14B。训练框架采用 verl 库，推理使用 vllm 库，训练算法为 GRPO。关键训练参数包括学习率 1e-6、批次大小 256、零 KL 惩罚等。详细的超参数设置如下表所示：

  

![Image](https://mmbiz.qpic.cn/sz_mmbiz_png/E1DQUYfcS0Jt019yb5asRlnB0q4ZUa0ic6iaNwUYVDkaS9dbKiaLXOMdEAQic82gml4LGc2lCyw1uvef9qq9ibxEHLw/640?wx_fmt=png&from=appmsg&watermark=1&tp=webp&wxfrom=5&wx_lazy=1)

用于强化预训练的超参数  

根据实际训练需求和资源限制，可以对这些参数进行优化。例如，适当调整学习率可以平衡模型的收敛速度和稳定性；确定合适的批次大小可以充分利用计算资源，同时避免内存溢出等问题。此外，还可以探索以下优化方向：

- • **自适应学习率调整** ：根据训练过程中的损失变化和奖励趋势，动态调整学习率。当模型性能提升较快时，适当降低学习率以精细调整参数；当性能提升停滞时，适当提高学习率以跳出局部最优。
- • **超参数搜索算法** ：采用贝叶斯优化等超参数搜索算法，在大规模参数空间中寻找更优的超参数组合，进一步提升模型性能。

# 实验设计与评估：验证 RPT 的强大性能

### 预训练模型评估：语言建模与扩展性分析

**语言建模性能评估与分析** ：利用 OmniMATH 验证集，评估模型在不同难度 token 位置上的 next-token-prediction 准确率。结果表明，RPT 模型在所有难度级别上均优于 R1-Distill-Qwen-14B 的标准 next-token-prediction 和基于推理的预测。例如，在高难度 token 位置上，RPT 模型的准确率提升显著，这可能是因为 RPT 更注重挖掘 token 之间的深层语义关系和推理逻辑，从而在复杂场景下表现出更强的预测能力。具体数值显示，RPT-14B 在高难度 token 位置上的准确率达到 23.75%，相较于 R1-Distill-Qwen-14B 的 20.43% 有明显提升，这归功于 RPT 在预训练阶段对推理能力的强化训练，使得模型能够更好地理解和预测复杂的文本内容。

![Image](https://mmbiz.qpic.cn/sz_mmbiz_png/E1DQUYfcS0Jt019yb5asRlnB0q4ZUa0icS4N7GlGtqQiaLiaGrrlpQpJ27ObE22XWaicDER2PN6d14niaGqr0KQdbfQ/640?wx_fmt=png&from=appmsg&watermark=1&tp=webp&wxfrom=5&wx_lazy=1)

![Image](https://mmbiz.qpic.cn/sz_mmbiz_png/E1DQUYfcS0Jt019yb5asRlnB0q4ZUa0icS4N7GlGtqQiaLiaGrrlpQpJ27ObE22XWaicDER2PN6d14niaGqr0KQdbfQ/640?wx_fmt=png&from=appmsg&watermark=1&tp=webp&wxfrom=5&wx_lazy=1)  
准确率在三个不同难度的基准测试划分中的 Next-token prediction 表现。RPT在标准的Next-token prediction基线和基于推理的预测基线方面均表现更优

上表提供了不同模型在三种不同难度测试集上的 next-token-prediction 准确率对比。从表中可以看出，RPT 在所有难度级别上都优于标准 next-token-prediction 基线和基于推理的预测基线，这进一步证明了 RPT 的有效性和优越性。

**扩展性分析与大规模训练策略** ：通过幂律衰减模型拟合 next - token - prediction 准确率与训练计算的关系，发现 RPT 的性能随着训练计算量的增加而持续提升。这表明 RPT 具有良好的扩展性，能够在增加训练资源时获得更优的性能。例如，在训练计算量增加到 1000 步时，模型的准确率相较于初始阶段有了显著提高，且拟合曲线的高 R² 值（如易难度为 0.995，中等难度为 0.997，高难度为 0.989）表明模型对数据的拟合效果非常好。为了进一步优化扩展性，可以采用以下策略：

- • **分布式训练架构** ：构建高效的分布式训练架构，将模型训练任务分配到多个计算节点上并行执行。通过优化通信机制和数据分片策略，减少节点间通信开销，提高训练效率。
- • **混合精度训练** ：结合 FP16 和 FP32 精度，在保证模型精度的前提下，减少内存占用和计算量，加速训练过程。同时，采用梯度累积等技术，确保在大规模并行训练中的梯度更新稳定性。
	![Image](https://mmbiz.qpic.cn/sz_mmbiz_png/E1DQUYfcS0Jt019yb5asRlnB0q4ZUa0icZqUddvcvStdibGeLkrhiak9MKibc7hH8IZWfVm9VP6aPJmsy5Ew8Yej3g/640?wx_fmt=png&from=appmsg&watermark=1&tp=webp&wxfrom=5&wx_lazy=1)

不同难度水平数据上的 next-token prediction 准确率平均表现

上图展示了 R1-Qwen-14B/32B 在不同难度数据上的平均 next-token-prediction 准确率。从图中可以看出，RPT 模型在不同难度级别上均表现出较高的准确率，进一步证明了其在语言建模性能上的优势。

下图则展示了 RPT 的 next-token-prediction 准确率随着训练计算量的增加而一致提升的情况。拟合曲线的高决定系数表明预测值与观测值之间的一致性较高，这表明 RPT 具有良好的扩展性，能够随着训练资源的增加而持续提升性能。

![Image](https://mmbiz.qpic.cn/sz_mmbiz_png/E1DQUYfcS0Jt019yb5asRlnB0q4ZUa0icHqSx3BzpVGHqsf61llibiaB0LfumhiaBt14rPHC1AiaQtssZuC2zuD7C1w/640?wx_fmt=png&from=appmsg&watermark=1&tp=webp&wxfrom=5&wx_lazy=1)

在强化预训练中，随着训练计算量的增加，Next-token prediction accuracy 在所有数据难度下均持续提高。拟合曲线显示出较高的决定系数，表明预测值与观测值之间的一致性

### 强化微调实验：挖掘 RPT 的深层潜力

在对强化预训练（RPT）进行全面评估的过程中，强化微调实验扮演着至关重要的角色。这一实验环节验证了 RPT 模型在进一步强化学习微调中的表现，也深入揭示了 RPT 相较于传统预训练模型的优势所在，为 RPT 在实际应用中的有效性提供了有力支撑。（扩展阅读👉《[强化微调 ReFT：开启大语言模型推理新范式](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247488810&idx=1&sn=82048561f00266e1a701551e60668af7&scene=21#wechat_redirect)》）

#### 实验设置：精准筛选与严谨配置

强化微调实验的设置遵循科学严谨的原则，从数据采样到参数配置，每一个步骤都经过精心设计。实验数据源自 Skywork-OR1，从中随机采样 256 个问题用于训练，200 个问题用于测试，确保数据样本具有足够的多样性和代表性。数据筛选流程借鉴 SkyworkOR1 的数据过滤管道，利用 R1-Distill-Qwen-32B 模型识别具有挑战性的实例，从而为模型训练提供更具价值的数据素材。

在训练参数方面，训练批次大小和 PPO 小批次大小均设置为 64，训练周期数为 15。这些参数的选择基于对模型学习效率与资源消耗的综合考量，这是为了实现模型性能提升与计算成本控制的平衡。评估设置中，验证时的最大 token 数设定为 32,000，温度参数设定为 0.6，这些参数配置为模型性能的准确评估提供了标准化的测试环境。

#### 对比分析：显著优势与深层原因剖析

对比分析结果令人瞩目。RPT 模型在仅使用 RLVR 进一步训练前后的性能提升显著，而持续使用标准 next-token-prediction 目标训练后的性能提升则相对有限。RPT-14B 模型在进一步 RL 训练后，性能从 56.3 提升到 58.3，而 R1-Distill-Qwen-14B 仅从 51.2 提升到 52.7。这一对比鲜明的结果有力地证明了 RPT 为后续 RL 训练提供了更为坚实的基础。

这一性能差异的背后，源于 RPT 在预训练阶段所培养的推理能力。RPT 通过强化学习激励模型生成推理序列，使得模型在微调阶段能够更快地适应任务特定的逻辑要求。例如，在面对复杂的数学推理任务时，RPT 模型能够在微调过程中迅速抓住问题的关键逻辑，基于预训练阶段形成的推理模式，高效地学习任务特定的解题策略。而传统预训练模型由于缺乏这种推理能力的培养，在微调阶段需要花费更多的时间和计算资源来理解和适应任务逻辑，导致其性能提升较为缓慢。

![Image](https://mmbiz.qpic.cn/sz_mmbiz_png/E1DQUYfcS0Jt019yb5asRlnB0q4ZUa0icBHOJHd8yNUdCkeRbwiachXOVgiatzjiaVAevQxgrazZk67QvRXzziblgeg/640?wx_fmt=png&from=appmsg&watermark=1&tp=webp&wxfrom=5&wx_lazy=1)

不同模型的强化微调性能。“持续NTP训练”指的是在与RPT-14B相同的语料库上，使用标准的 next-token prediction 目标进行持续预训练。RPT为后续的强化学习训练提供了更强大的基础

上表展示了不同模型的强化微调性能对比。可以看出，RPT 在进一步 RL 训练后的性能提升明显优于仅使用标准 next-token-prediction 目标训练的模型，这进一步证明了 RPT 的优势。

#### 推理能力迁移：从预训练到微调的桥梁

RPT 的强化微调实验还深入探讨了推理能力从预训练到微调的迁移机制。在预训练阶段，RPT 模型通过生成推理序列来预测下一个 token，这一过程促使模型深入理解文本背后的逻辑结构和语义关系。当进入微调阶段时，这种推理能力成为了模型快速适应新任务的有力武器。

以数学问题求解任务为例，在预训练过程中，RPT 模型已经学会了如何分析数学概念之间的关系、如何应用数学定理进行推理等。在微调阶段，面对具体的数学问题，模型能够将预训练阶段形成的推理模式迁移到新任务中，迅速生成针对问题的推理路径，如 “已知条件是什么”“需要求解的目标是什么”“可以应用哪些数学公式或定理” 等。这种推理能力的迁移使得模型在微调过程中能够以更少的训练数据和计算资源达到更高的性能水平，展现出更强的任务适应性和学习效率。

#### 性能提升的多维度影响：泛化与稳定性的增强

强化微调实验还从多个维度分析了 RPT 模型性能提升的影响。在泛化能力方面，RPT 模型在微调后展现出更强的泛化性能。它能够在不同类型的数学问题上保持稳定的高准确率，无论是代数问题、几何问题还是概率统计问题等。这表明 RPT 模型在预训练阶段所学习到的推理能力具有较强的通用性，能够跨越具体任务的差异，为模型提供广泛适用的推理框架。

在稳定性方面，RPT 模型在微调过程中的训练曲线表现出更少的波动和更快的收敛速度。这得益于 RPT 在预训练阶段通过强化学习所形成的稳定推理机制。模型在微调过程中能够更有效地利用训练数据，避免因数据噪声或任务特定偏差而导致的训练不稳定问题。这种稳定性有助于提高模型的最终性能，使得模型在实际应用中的表现更加可靠和可预测。

### 零样本任务性能评估：跨领域的强大泛化能力

**评估基准与设置优化** ：选择 MMLU - Pro 和 SuperGPQA 作为评估基准。下表展示了不同模型在 SuperGPQA 基准测试中的详细零样本性能表现：

![Image](https://mmbiz.qpic.cn/sz_mmbiz_png/E1DQUYfcS0Jt019yb5asRlnB0q4ZUa0icnaibEibc5JOzleiaCicQAyshhvE9pzrBfmBoN06VSJBOp5aZYFvZk4SHmA/640?wx_fmt=png&from=appmsg&watermark=1&tp=webp&wxfrom=5&wx_lazy=1)

在 SuperGPQA 上的零样本性能  

下表则展示了不同模型在 MMLU-Pro 基准测试中的详细零样本性能表现：

  

![Image](https://mmbiz.qpic.cn/sz_mmbiz_png/E1DQUYfcS0Jt019yb5asRlnB0q4ZUa0iclBprEj6fEUWcicwFFyDxIc232mIlTT6dmPdD4Jhia0BEUCvV3LKPpoMQ/640?wx_fmt=png&from=appmsg&watermark=1&tp=webp&wxfrom=5&wx_lazy=1)

在 MMLU-Pro 上的零样本性能  

在评估时，MMLU - Pro 的 token 数量设置为 12,288，温度为 0.8；SuperGPQA 的 token 数量设置为 12,288，温度同样为 0.8，并采用多选题格式进行评估。为了更全面地评估模型性能，可以增加评估基准的多样性，涵盖更多领域和任务类型，如逻辑推理、创意写作等。

**性能对比与原因分析** ：RPT-14B 在推理模式下的性能显著优于 R1-Distill-Qwen-14B（标准 next-token-prediction 模式和推理模式）以及 R1-Distill-Qwen-32B（标准 next-token-prediction 模式）。例如，在 SuperGPQA 基准测试中，RPT-14B 的准确率为 39.0%，而 R1-Distill-Qwen-14B 为 32.0%，R1-Distill-Qwen-32B 为 37.2%。这表明 RPT 的推理模式能够帮助模型更好地理解和生成复杂推理任务所需的逻辑连贯、语义准确的文本。其原因在于 RPT 的推理序列生成过程促使模型深入理解文本背后的逻辑结构和语义关系，使得模型在面对不同领域的零样本任务时，能够基于已有的知识和推理能力进行有效的预测和推理。

![Image](https://mmbiz.qpic.cn/sz_mmbiz_png/E1DQUYfcS0Jt019yb5asRlnB0q4ZUa0icURSnkRF6LgShGmjPbvorC1IibdJhgickic0C6o414WeUiaYEHZVTOib0EdQ/640?wx_fmt=png&from=appmsg&watermark=1&tp=webp&wxfrom=5&wx_lazy=1)

零样本在通用领域终端任务上的表现。RPT-14B在推理模式下始终优于14B和32B基线模型

上表展示了不同模型在零样本任务上的性能对比。可以看出，RPT-14B 在推理模式下在多个基准测试中均优于基线模型，这进一步证明了 RPT 的有效性和优势。

### next-token-reasoning 模式分析：推理背后的思维

为了更清晰地展示推理模式的差异，对推理模式进行了分类，并提取了相应的关键词，具体分类和关键词如下表所示：

![Image](https://mmbiz.qpic.cn/sz_mmbiz_png/E1DQUYfcS0Jt019yb5asRlnB0q4ZUa0icGS8BsNB9WpiaD8boyxS3AjxwMSibj8wT6qOXy0PCEbrOKnRKCficYCuiaA/640?wx_fmt=png&from=appmsg&watermark=1&tp=webp&wxfrom=5&wx_lazy=1)

![Image](https://mmbiz.qpic.cn/sz_mmbiz_png/E1DQUYfcS0Jt019yb5asRlnB0q4ZUa0icGS8BsNB9WpiaD8boyxS3AjxwMSibj8wT6qOXy0PCEbrOKnRKCficYCuiaA/640?wx_fmt=png&from=appmsg&watermark=1&tp=webp&wxfrom=5&wx_lazy=1)  
模式组和关键词

下表展示了七种提示模板的具体内容：

![Image](https://mmbiz.qpic.cn/sz_mmbiz_png/E1DQUYfcS0Jt019yb5asRlnB0q4ZUa0icjKN3qgNjT1aLzb6AaugEgXhBRJabY03ZnYKhoxicxQZU1IKIhsjumpQ/640?wx_fmt=png&from=appmsg&watermark=1&tp=webp&wxfrom=5&wx_lazy=1)

![Image](https://mmbiz.qpic.cn/sz_mmbiz_png/E1DQUYfcS0Jt019yb5asRlnB0q4ZUa0icjKN3qgNjT1aLzb6AaugEgXhBRJabY03ZnYKhoxicxQZU1IKIhsjumpQ/640?wx_fmt=png&from=appmsg&watermark=1&tp=webp&wxfrom=5&wx_lazy=1)  
七个针对 next-token 任务推理的提示模板

**推理模式差异统计与解读** ：对 R1-Distill-Qwen-14B 和 RPT-14B 的推理响应进行分类统计发现，RPT-14B 在假设模式和演绎推理模式上的使用比例显著更高。假设模式帮助模型探索多种可能的预测路径，而演绎推理模式基于已知信息进行逻辑推导，得出更准确的预测结果。例如，在处理一段关于物理现象的文本时，RPT-14B 会生成多种假设，如“假设接下来会解释物理现象的原因”“假设会给出实验验证方法”，然后通过演绎推理，结合上下文信息，确定最合理的预测路径，从而生成准确的下一个 token。

**实例分析与推理过程解析** ：以具体的推理实例来看，RPT-14B 在预测下一个 token 时，会对语义上下文进行深入分析。例如，在处理一段关于向量大小计算的文本时，模型会分析上下文中的关键词，如 “magnitude”“formulas” 等，然后提出假设：“接下来可能会解释如何计算向量的大小，或者给出具体的计算步骤。” 接着，它会探索多种可能性，如 “可能是列举计算步骤，也可能是直接给出公式。” 最后，通过自我反思和调整，结合上下文的结构和语义信息，确定最可能的下一个 token。这一过程中，模型的推理序列体现了其对文本深层语义的理解和逻辑推理能力，这是其性能优于传统模型的关键所在。

![Image](https://mmbiz.qpic.cn/sz_mmbiz_png/E1DQUYfcS0Jt019yb5asRlnB0q4ZUa0icjERvzA1kgcL4IJrPogCtPyqXEhLAMoHV5FhmwToM0aYKickfgDVr1LA/640?wx_fmt=png&from=appmsg&watermark=1&tp=webp&wxfrom=5&wx_lazy=1)

R1-Distill-Qwen-14B 用于问题求解的推理模式统计，以及 RPT-14B 用于 next-token reasoning 的推理模式统计

上图展示了 R1-Distill-Qwen-14B 和 RPT-14B 在问题解决和 next-token-reasoning 中的推理模式统计。可以看出，RPT-14B 在假设模式和演绎推理模式上的使用比例显著更高，这表明 RPT 更倾向于通过多种推理方式来探索和确定下一个 token。下表则提供了一个 RPT 推理模式的具体示例，展示了 RPT 如何在预测过程中生成推理序列，从而得出更准确的预测结果。

![Image](https://mmbiz.qpic.cn/sz_mmbiz_png/E1DQUYfcS0Jt019yb5asRlnB0q4ZUa0icicDwhp1F90hqZUrqO5CD3hjU8OoXibia101ialNiaMjYGPIaNsqSHrmZ0pg/640?wx_fmt=png&from=appmsg&watermark=1&tp=webp&wxfrom=5&wx_lazy=1)

RPT的推理模式示例  

# RPT与传统范式的比较

### 大型语言模型的扩展范式

传统 LLM 的扩展范式主要集中在训练时间和测试时间两个维度。训练时间扩展通过增加模型参数和训练数据来提升性能，而测试时间扩展则通过延长推理计算来增强模型的推理能力。RPT 独特地将每个 next-token-prediction 视为推理任务进行扩展，结合了训练时间和测试时间扩展的优点。与传统范式相比，RPT 在技术实现上更加注重推理过程的构建和优化，在应用场景上更适用于复杂推理任务，在性能表现上能够更有效地提升模型的准确性和泛化能力。例如，与仅通过增加模型参数进行扩展的方式相比，RPT 在不大幅增加模型规模的情况下，通过强化推理训练，使得模型在数学推理等复杂任务上的性能得到了显著提升，这体现了 RPT 在扩展范式上的创新性和优势。

### 大型语言模型的强化学习：RPT 的独特优势与创新

强化学习在 LLM 后训练阶段的应用已取得显著成果。例如，基于人类反馈的强化学习能够使模型更好地对齐人类偏好，而大规模 RL 则提升了模型的推理能力。RPT 与以往工作的主要区别在于采用基于 next-token 预测正确性的规则奖励信号，有效避免了奖励劫持问题。这种创新的奖励机制使得 RPT 在强化学习方法上更具有优势，能够更稳定地提升模型的性能和应用效果。与传统的基于奖励模型的 RL 方法相比，RPT 的规则奖励信号更加直接和客观，减少了因奖励模型偏差导致的训练不稳定问题，从而提高了模型训练的效率和最终性能。

# 总结：洞察与展望

强化预训练（RPT）的提出，无疑是大型语言模型（LLM）预训练领域的一次重大革新。它在技术层面实现了突破，更深刻地改变了我们对模型认知方式的理解。从传统的 next-token-prediction 到创新性的 next-token-reasoning，这一转变，已经不再是简单的任务重构，而是模型认知模式的一次质的飞跃。

在 next-token-prediction 中，模型的角色类似于一位机械的记忆者，仅凭统计规律预测下一个 token。然而，RPT 将这一过程进化为 next-token-reasoning，赋予了模型战略 “思考” 的能力。在 RPT 框架下，模型不再是简单地根据已有的词频统计等模式进行预测，而是需要先理解上下文中的关键信息，生成推理序列，再进行预测。以数学问题为例，模型不再死记硬背数学符号的顺序，而是通过推理序列，如 “考虑变量之间的关系”“应用定理公式” 等，逐步推导出下一个 token。这种类似于人类思考的过程，使模型能够深入挖掘文本背后的语义和逻辑，而非停留在表面的 token 级相关性上。

这种进化带来的影响是深远而多维的。RPT 模型在语言建模性能上的提升就是最直观的体现。比如实验数据显示，RPT-14B 在高难度 token 位置上的准确率达到 23.75%，相较于 R1-Distill-Qwen-14B 的 20.43% 有明显提升。这表明 RPT 模型能够更精准地把握文本的深层结构和复杂语义关系。这种性能提升不会是偶然的，而是源于 RPT 对模型认知能力的重塑。RPT 通过强化学习机制，激励模型生成推理序列，使模型在训练过程中主动思考文本的内在逻辑和语义关系，探索多种可能的推理路径。这种学习方式使模型能够跳出单纯的记忆模式，转而培养起强大的泛化能力，从而在面对新的、未见过的文本时表现更佳。

更为重要的是，RPT 的推理能力具有强大的泛化潜力。它使模型在面对不同领域和类型的文本时，能够灵活运用推理策略，而非依赖于对特定数据模式的死记硬背。在数学领域，RPT 模型能够通过推理序列深入理解数学概念和定理之间的关系；在文学领域，它则可以分析文本的情感脉络和修辞手法。这种泛化能力的提升，无疑为 LLM 在更多领域的应用开辟了广阔的空间。例如，在教育领域，RPT 可以帮助构建更智能的教育辅导系统，根据学生的学习进度和知识掌握情况，生成个性化的学习材料和问题解答；在科研领域，RPT 能够辅助研究人员进行文献分析和科学发现，提升科研效率。

当然，我们必须认识到 RPT 当前的局限性。实验主要基于 14B 参数模型，预训练语料以数学文档为主，这使得模型在非数学领域的推理能力略显不足。此外，训练起点依赖推理模型，可能会影响 RPT 的普适性和灵活性。如果初始推理模型在某些特定任务上存在缺陷或偏差，可能会对 RPT 的训练效果产生负面影响，限制其在不同场景下的应用范围。

  

RPT 的提出，我认为是 LLM 预训练领域的一座新里程碑。它为我们提供了当下极具价值的技术方案。我的理解在于，论文中提到的推理范式的转变，大家不觉得有点类似于从 CoT（Chain-of-Thought，思维链）式思考转向了 ToT（Tree-of-Thought，思维树）式思考吗。以下我做了简单的对比。

#### CoT 与 ToT 的基本概念

- • **CoT（思维链）**：强调模型在生成答案之前进行逐步的推理，像是逐步思考和探索的链式过程。例如，面对一个数学问题，CoT 方法会要求模型先进行分解问题、尝试不同解决步骤等，最后得到答案。
- • **ToT（思维树）**：在 CoT 的基础上，增加了探索的广度，即生成多个可能的推理路径，形成一个“树”状的思考结构。它不仅关注纵向的推理深度，还注重横向的多种可能性探索。

#### RPT 与 CoT、ToT 的关系

- • **与 CoT 的相似性**：RPT 的核心思想是将 next - token - prediction 重构为 next - token - reasoning 任务，要求模型生成推理序列。这类似于 CoT 的逐步推理方式，强调在生成答案之前进行深入的思考和分析。
- • **与 ToT 的相似性**：RPT 中模型需要生成多个不同的思考轨迹（如 G 个响应），每个轨迹包含一个推理序列和对应的下一个 token 预测。这一过程类似于 ToT 的多路径探索，模型在多个可能的推理路径中进行尝试和评估，从而提高预测的准确性和泛化能力。

#### RPT 的独特优势

- • **更灵活的推理模式**：RPT 不仅包含 CoT 的链式推理，还融合了 ToT 的树状探索，使模型能够从多个角度思考问题，提升了推理的全面性和深入性。例如，在文本生成任务中，模型不仅能按部就班地进行逻辑推理，还能同时尝试多种可能的表述方式，选择最合适的一种。
- • **强化学习的激励作用**：RPT 利用强化学习直接根据预测的正确性给予奖励，引导模型优化推理过程。这种激励机制促使模型在推理过程中更加注重思考的准确性和有效性，使模型能够更好地学习如何进行合理的推理。
- • **在预训练阶段的深度整合**：RPT 将推理能力的培养深度融入预训练过程，使其成为模型内在的基础能力。这与 CoT 和 ToT 通常在模型生成阶段的运用不同，为模型提供了更坚实、更通用的推理基础。

所以基于以上的理解，RPT 的推理范式转变，是否是吸收了 CoT 和 ToT 的优点，并通过强化学习等机制进一步优化了推理过程，为大型语言模型的预训练和性能提升带来了新的突破。

我在看这篇论文的时候非常兴奋，这不仅是因为 RPT 范式为大型语言模型的预训练带来了全新的思路，还因为这让我又一次看到了模型向上吞噬的能力。可能有小伙伴会诧异或者叫担心，模型向上吞噬，那这不是意味着应用又薄了吗？那么做工程应用的饭碗又浅了一些？模型能力的不断提升，是否会挤压工程应用的生存空间，甚至让人类在技术面前变得无足轻重？

然而，我坚信并非如此。因为现实世界的知识、人类的能力、认知向 AI 压缩的过程，会进一步提升应用智能的飞跃。想想看，AI 的发展从通用模型到如今广泛涌现的 Reason Model，这是模型通过 RL 范式的一次向上吞噬，或许这里用吞噬并不合适，因为这是人类的智慧让模型发生了跃迁。如今 RPT 范式所展现的类 ToT（Tree of Thought）战略思考模式，这每一次模型的进化，都并非是对人类的替代，而是对人类能力的有力补充。所带来的直接收益就是，上层 AI 应用性能的增强，AI 应用的推理准确性得到提升，自动化智能化的效能更高，最终受益的还是人！

RPT 范式的提出，可以看作是基于之前训练范式的又一次飞跃，这一次模型几乎是基于 ToT 的战略思考模式来“定义任务”，而不单纯是“推理任务”。回想一下开篇我提到的 Openai 推理模型 o3 的能力，或许它的背后已应用了这样的战略思考范式，所以才可以实现模型的“跨域”知识的推理。因为模型“眼界”宽了，“看”到的东西不一样了，认知就会有“高度”，综合推理的水平就会上升，Knowledge is power！那么在此，我们思考一下，应用可以基于这样已进化的推理模型做哪些事情？一个单一具体的假设，也许，顶层的 Agent 对于 Graph 的推理可能不再是难题。因为模型已经高效的应用了类 ToT 的思考模式，这可能将极大地推动 Agent 在 GoT（Graph of Thought）实现算法上的进化，使其更加高效和易于实现。同时，这也意味着 AI Agent 的能力基于模型能力进化的现实，也会带来协同进化的增强效应。这也正是我阅读这篇论文让我兴奋的原因，说不定再过半年，类似“战略思考”的推理范式就会落地并普及。

在此，RL范式再次彰显了其强大的力量。当我们面对模型的进化时，无需恐惧或退缩。在人类文明的历史中，每一次人类发明的工具实现进化，人类与这些进化工具的交互都会推动整个文明的跃迁。我们应当正视这一现象，并基于工具的进化，挖掘人类更深层的创造力。人一定是这场技术革命的核心，人类也必须是这场技术革命的主宰者！各位，看过此文有什么感想？如有其他想法可以在评论区留言，我们聊聊。或者加入“觉察流”社区群，与群里的小伙伴一起学习、交流。加入方法，私信回复“入群”“加群”即可。

如果你对“模型训练”相关话题感兴趣，可以点击订阅👉“[模型训练](https://mp.weixin.qq.com/mp/appmsgalbum?action=getalbum&__biz=Mzk2NDA0MzcxNw==&scene=1&album_id=3819545931764203521&count=3)”主题。

  

![图片](https://mp.weixin.qq.com/s/www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg%20stroke='none'%20stroke-width='1'%20fill='none'%20fill-rule='evenodd'%20fill-opacity='0'%3E%3Cg%20transform='translate(-249.000000,%20-126.000000)'%20fill='%23FFFFFF'%3E%3Crect%20x='249'%20y='126'%20width='1'%20height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

参考资料

- • Reinforcement Pre-Training  
	https://arxiv.org/pdf/2506.08007

  

#觉察流 #AI全栈 #AI论文 #AI社区 #开源 #开源项目 #RPT #强化学习 #强化学习预训练 #LLM #大型语言模型 #NextTokenReasoning #语言建模性能  

  

关联阅读

◆🔥[Qwen3 嵌入与重排序（技术报告）：复杂指令下的文本处理新能手（万字）](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247489205&idx=1&sn=731e40806e0d37e14752846541e90db7&scene=21#wechat_redirect)

◆🔥[Test-Time Scaling：挖掘大型语言模型推理潜能（3万字综述）](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247489178&idx=1&sn=95339e919429ac35cdccd1a695d375f7&scene=21#wechat_redirect)

◆[ALPHAONE（α1）：LRM 自适应推理效率与准确性的平衡之道](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247489002&idx=1&sn=af6673bdafe5e9806df1349de5435fb8&scene=21#wechat_redirect)

◆[NVIDIA 新成果：ProRL 拓展 LLM 推理边界（万字）](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247488996&idx=2&sn=553a25785e32a5ebe24d8af70359e2f3&scene=21#wechat_redirect)

◆🔥[LLM 强化学习的开源新力量：字节跳动 DAPO 算法](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247488951&idx=1&sn=ff5d7c72409e0cc29501b1ce4eab3381&scene=21#wechat_redirect)

◆🔥[AutoRefine：RL加持RAG，边想边搜并精炼，革新LLM推理（万字）](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247488880&idx=1&sn=22118f024c33eae24f678f82a1e80a6d&scene=21#wechat_redirect)

◆🔥[定义任务 + 合成数据：智能训练的高效引擎 —— Synthetic Data RL（万字）](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247488830&idx=1&sn=ff0a9457c9375ee6b8597c50799e56a4&scene=21#wechat_redirect)

◆[MARFT：多智能体协作与强化学习微调的协同进化（万字）](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247488815&idx=1&sn=ce82cd744938cdc911ab84764dedf26c&scene=21#wechat_redirect)

◆[强化微调 ReFT：开启大语言模型推理新范式](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247488810&idx=1&sn=82048561f00266e1a701551e60668af7&scene=21#wechat_redirect)

◆🔥[当异常奖励遇上 AI 推理：一场意料之外的智力提升（万字）](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247488755&idx=1&sn=b3ae45fce4299ce3fe99e6994c7fe3af&scene=21#wechat_redirect)

◆[IBM 研究：可验证奖励强化学习（RLVR）通过 GRPO 提升模型推理能力（万字）](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247488749&idx=1&sn=bc48976b6afc36ffddfa20c086c252b9&scene=21#wechat_redirect)

◆[奖励推理模型（RRM）：革新奖励模型的新范式（万字）](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247488709&idx=1&sn=fbb4808882401794f7206c76ce4e3a99&scene=21#wechat_redirect)  

◆[DeepSeek-V3：硬件与模型的完美统协，奏响 AI 高效新乐章（万字）](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247488661&idx=2&sn=af99598ca3265cd4b7f52f992a0951c6&scene=21#wechat_redirect)

◆[MMaDA：多模态大型扩散语言模型的创新突破（万字）](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247488661&idx=1&sn=3e3d8c98cc0d98bddfabdb4892466f97&scene=21#wechat_redirect)

◆🔥[混合推理模型（LHRM）：平衡效率与推理能力的新范式](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247488638&idx=1&sn=cd2a0a9a970c264d3571ca72b978ca92&scene=21#wechat_redirect)  

◆🔥[字节跳动 AdaCoT：基于强化学习的自适应推理触发方法（万字）](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247488612&idx=1&sn=c102490ebe796f54090d0c81a69c6f5b&scene=21#wechat_redirect)

◆[Thinkless框架：让LLM学会“聪明偷懒”的智慧](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247488593&idx=2&sn=69ec53036fc47042a3ffe4f2da38e838&scene=21#wechat_redirect)  

◆🔥[Anthropic 发布 Claude 4：为开发者带来的全新编程体验与机遇](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247488558&idx=1&sn=5838aa9eb8764a5aeac9a9f468896d75&scene=21#wechat_redirect)

◆[AdaptThink：推理模型的自适应思考范式](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247488558&idx=2&sn=958dc427cb402b5c25eb9b1cbec9f208&scene=21#wechat_redirect)

◆🔥[LLM 协作革命：Group Think 如何重塑推理边界 (万字)](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247488519&idx=1&sn=a7c3ed22422aecb4e961efd11353b16a&scene=21#wechat_redirect)

◆🔥[系统提示(Prompt)优化：基于元学习的双层优化框架（万字）](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247488496&idx=1&sn=81712590f42840f79dfc0efa4f565b8b&scene=21#wechat_redirect)

◆🔥[并行扩展（Parallel Scaling）：一种新型语言模型扩展范式（万字）](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247488474&idx=1&sn=f355ea54965069fbae0f1ee6fab5964f&scene=21#wechat_redirect)  

◆[Qwen3：开源 LLM 的革新者与多语言先锋（万字）](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247488474&idx=2&sn=e6b7b40ab1e93fadf2af8a67f25ee848&scene=21#wechat_redirect)

◆[Windsurf 发 SWE-1：以数据+智能飞轮驱动软件工程 AI 进化](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247488399&idx=1&sn=16b127e218c3998cbedaf66c61daed2d&scene=21#wechat_redirect)  

◆🔥[多模态推理模型（LMRM）：从感知到推理的演变（2万字综述）](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247488205&idx=1&sn=c8002575e339255212e9fa23ad2b0358&scene=21#wechat_redirect)

◆[智能体式推理与工具集成：ARTIST 基于强化学习的新思路（万字）](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247488180&idx=1&sn=db22ccfaa3913e46db262d6d306946bc&scene=21#wechat_redirect)  

◆[Self-Play Critic：以“对抗博弈”创新大语言模型推理评估（万字）](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247488160&idx=1&sn=d956623957bd2240d6547a514f725a2f&scene=21#wechat_redirect)

◆[解锁大模型推理新潜能：重复采样的魔力](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247488146&idx=1&sn=b250abb5f54947b645742cdc2f8b3816&scene=21#wechat_redirect)

◆[交互式生成视频（IGV）：重塑游戏、智能与驾驶的交互革命（二万字长文）](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247488071&idx=1&sn=a4eaa40c5e4169fdb0b2ae4a2765fb2d&scene=21#wechat_redirect)

◆[DeepSeek-R1 百天：推理语言模型（RLM）的复现与创新（万字长文）](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247488026&idx=2&sn=76a8b98809af8c2743d294e67f7df244&scene=21#wechat_redirect)

◆[LLM 推理新境界：多语言思考的力量](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247488002&idx=1&sn=60900dba14b8a595c4f3e6de7c8eb0ee&scene=21#wechat_redirect)

◆[AI 社会中的共识：语言理解能力如何塑造 AI 的群体决策？](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247487944&idx=1&sn=588eefed55006ae3d932c1617a17c2b8&scene=21#wechat_redirect)

◆[深度解析与精准评估：OmniDocBench的创新之旅](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247487771&idx=2&sn=9ee4263cd8b6b6f9a655580b9c56c783&scene=21#wechat_redirect)

◆🔥[LLM 微调的学习动力学：幻觉、挤压与优化的艺术（万字长文，实战解读）](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247487639&idx=2&sn=0605706ec6852249529f9d4dd6d801f7&scene=21#wechat_redirect)

◆[RLHF - 基于人类反馈的强化学习：语言模型的进化引擎](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247487593&idx=2&sn=680a5d3c0d47f3304622ae44c56569b1&scene=21#wechat_redirect)

◆🔥[深入探索 GPT-4o：图像生成的多面手（3万字综述）](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247487533&idx=1&sn=3b18e1de1e86805376d0dfcb98d84fab&scene=21#wechat_redirect)

◆🔥[OpenAI发布：企业AI落地指南——应用场景识别与规模化应用策略](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247487453&idx=1&sn=bc89b07d8d8efaa74c75a432e7a6fb1d&scene=21#wechat_redirect)

◆[OpenAI 发布：构建 AI Agent 实用指南](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247487453&idx=2&sn=a5949727f355651f6ec849e23cae436b&scene=21#wechat_redirect)

◆🔥[OpenAI 发布企业 AI 集成技术手册：从评估到自动化](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247487420&idx=1&sn=0c0d91562520ec392d38d47b2616196a&scene=21#wechat_redirect)

◆🔥[AI 的下半场：从解决问题到定义问题](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247487340&idx=1&sn=8e217f47094ce8dbca2c21e312d59932&scene=21#wechat_redirect)

◆[微软 BitNet b1.58 2B4T：低比特效率革命，让模型在边缘设备 “飞” 起来](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247487318&idx=1&sn=520a21b573c9217af1d1fde9f7d054d0&scene=21#wechat_redirect)

◆[SQL-R1-7B：用强化学习优化复杂SQL查询，性能比肩32B模型](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247487318&idx=2&sn=8840a0ebbedc0eb61828adab63917ae1&scene=21#wechat_redirect)

◆🔥[DeepSeek-R1：如何让AI像人类一样“深度思考”？(综述)](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247487304&idx=1&sn=e5a7eca4d50f25a8cc8e8de315252259&scene=21#wechat_redirect)

◆🔥[AI 有病！技术的缺陷？还是人性的弱点？](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247487304&idx=2&sn=6bc111da3b1ab4ae661255966e9554bd&scene=21#wechat_redirect)

◆🔥[Reason Model 的“瘦身计划”：量化技术的得与失](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247487258&idx=1&sn=e854d78014236cabcf4f43194fa401bd&scene=21#wechat_redirect)

◆🔥[GLM-4 开源32B推理模型，OpenAI 发布 GPT-4.1](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247487233&idx=1&sn=24ce0784e0e5b7348623995a3aab5e4f&scene=21#wechat_redirect)

◆[AI 的经济性格：litmus 测试揭示 AI 的选择倾向](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247487178&idx=2&sn=84ec64ab499224cada44a96ea7c841b8&scene=21#wechat_redirect)

◆🔥[多模态 InternVL3 发布：从1B到78B多尺寸SOTA](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247487086&idx=1&sn=64dc1112246e230152382511e0c3018d&scene=21#wechat_redirect) 

◆[AI如何读懂角色的内心？《冰与火之歌》揭示新路径](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247487060&idx=1&sn=3ec9c7d069f04ebfad83e0a16e0023a2&scene=21#wechat_redirect)

◆[AI的“读心术”：动态用户画像如何改变人机交互？](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247487060&idx=2&sn=540774cb103031da6f8cee4adfddd15e&scene=21#wechat_redirect)

◆🔥[反认知！Scaling Law被质疑，图搜索熵揭示LLM推理能力并非参数越大越好](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247486962&idx=1&sn=00b306924a30bc5e3be53d8394abb4d4&scene=21#wechat_redirect)

◆[RARE：让 AI 模型从死记硬背进化到聪明推理](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247486962&idx=2&sn=75166c883221db9595ccca9d54be92b2&scene=21#wechat_redirect)

◆[AI的“内心独白”为何不可靠？Anthropic论文揭示CoT监控的局限性](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247486818&idx=1&sn=a56e55727d1c975091ad08c9763a81a3&scene=21#wechat_redirect)

◆[Thinking Intervention：掌控 AI 思考推理的新范式](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247486765&idx=2&sn=b342a49f448f667998481235c2fbd5ee&scene=21#wechat_redirect)

◆[ReSearch 框架：让 AI 像人类一样边思考边搜索](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247486765&idx=1&sn=ec4803dfe5951b0196d9f861ed852667&scene=21#wechat_redirect)

◆🔥[Llama 4 发布：10M 长上下文,MOE,多模态,2 万亿总参数 SOTA 是亮点](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247486717&idx=1&sn=310848d443d4e5ea9f31b19ec864d6f7&scene=21#wechat_redirect)

◆[SICOG：让多模态模型学会 “观察” 和 “思考”](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247486632&idx=2&sn=bed931fa29520fc888e7285e42f57caa&scene=21#wechat_redirect)

◆[Claude 3.7 Sonnet：AI 如何重塑劳动市场与经济格局](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247486589&idx=1&sn=10d0980d53379273ac488fe9079d7ec0&scene=21#wechat_redirect)

◆[全模态的突破：Qwen2.5-Omni-7B技术报告](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247486555&idx=2&sn=94c4b913d1a49940bace9c46ac0066db&scene=21#wechat_redirect)

◆[生成式检索的幻觉难题，看看支付宝的方案](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247486504&idx=1&sn=a99e6b553b76b4c402b589249886965c&scene=21#wechat_redirect)

◆[Claude：AI 如何用“通用语言”思考、规划和计算？](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247486492&idx=1&sn=93eb424e2c112941980d8eb09250b3c7&scene=21#wechat_redirect)

◆[🚀重磅！千问体验站即将接入 MCP！Anthropic 疑将发 500K 上下文 Claude Sonnet3.7 ？](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247486458&idx=1&sn=e172f248a846280d4842891cedba25f4&scene=21#wechat_redirect)

◆🔥[DeepSeek“鲶鱼”：混元-T1正式亮相, Qwen3近在咫尺, GPT-5将免费](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247486329&idx=1&sn=143e8169b46b3fdbabe5a34b0fd92df3&scene=21#wechat_redirect)

◆[OpenAI 发布新一代音频模型](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247486257&idx=1&sn=88be69984c8c43c27b6a3869fce33acd&scene=21#wechat_redirect) 

◆[STEVE：让 AI 更智能地操控图形界面](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247486248&idx=2&sn=3b0f5ee27f5a5d12c86881e998f2a61d&scene=21#wechat_redirect)

◆[MCoT：让机器像人类一样思考 (综述)](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247486232&idx=1&sn=d473e7066c47ed7a4041c704b7bc9656&scene=21#wechat_redirect)

◆[CompassJudger-1：AI模型Judger的全栈解决方案（万字长文）](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247486106&idx=2&sn=7f39e86181a639d278c5c37fbe181303&scene=21#wechat_redirect)

◆[SEAP剪枝：让大型语言模型在效率与性能间找到完美平衡](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247486062&idx=2&sn=8bb05a6342489adca5fb19087f1de367&scene=21#wechat_redirect)

◆🩺[AI在医疗领域的深度探索：Baichuan-M1的实践与展望](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247485945&idx=2&sn=1773efdcec5e742a521f2f2d7fa46b8f&scene=21#wechat_redirect)

◆🤖[AgiBot World：智元通用具身基座模型，为机器人通用智能按下“快进键”](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247485889&idx=1&sn=7f48de41780c9bca249fa07a2afec91a&scene=21#wechat_redirect)

◆🇺🇳[多语言模型的“语言孤岛”：跨语言知识转移的真相](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247485872&idx=2&sn=4d7f97ff94f8d338ac54a0dfc83aae65&scene=21#wechat_redirect)

◆🔥[QwQ-32B比肩671B的DeepSeek-R1，全球首发通用 AI Agent](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247485777&idx=1&sn=4ae7e0b6e9daf40be4ba717d03163ca0&scene=21#wechat_redirect)

◆❄️[QASnowball：用“迭代雪球”打破问答数据困境-问答数据合成](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247485777&idx=2&sn=1bff3ab1bf7ce888e337ef4df156fcae&scene=21#wechat_redirect)

◆[长文本 Prompt 中的语言模型：真的能有效利用所有信息吗？](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247485758&idx=1&sn=80a625cc0b411da868fa27350ec5c0c9&scene=21#wechat_redirect)

◆[AI提示词工程：如何让机器更懂你？预警1.3万字长文](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247485747&idx=2&sn=9de8b9917d9d01b979e8f3b45da4c0f0&scene=21#wechat_redirect)

◆🏃[LoRA 微调：如何在不损害 LLM 的情况下添加新知识](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247485560&idx=1&sn=408c04f4d06342a25e7f5a1e82a7b328&scene=21#wechat_redirect)

◆[LLaDA：打破自回归模型垄断的全新语言模型](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247485533&idx=2&sn=ab002e7ba0075a0eb7dd59176a232e0e&scene=21#wechat_redirect)

◆🔥[Inception Labs 推出 Mercury：语言模型的新突破——Diffusion LLM](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247485513&idx=1&sn=96e0ddd3075223625c224c7295c140a0&scene=21#wechat_redirect)

◆[1B LLM 超越 405B LLM？这项研究揭示了什么](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247485484&idx=2&sn=45c4ca3d2e217335d217baef12640622&scene=21#wechat_redirect)

◆🚀[标点符号的隐藏力量：揭秘 AI 模型中的上下文记忆](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247485426&idx=1&sn=d7f273b901c40090f4a52a75fbf5fd4e&scene=21#wechat_redirect)

◆🔥[EasyR1：多模态强化学习训练的高效框架](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247485388&idx=1&sn=823019a3c72f6035e6820e5137624f0c&scene=21#wechat_redirect)

◆[Themis：如何用 AI 评估 AI ？](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247485329&idx=2&sn=6216ffc837caae2ee9bd078bacac0587&scene=21#wechat_redirect)

◆🔥[R1-V ：用低成本强化学习，让视觉语言模型实现超强泛化](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247485244&idx=2&sn=81588a32bf1c2ea78ec6d0d8dbc717b2&scene=21#wechat_redirect)

◆🔥[强化学习 (RL) 与监督微调 (SFT)：谁更能提升模型泛化能力？](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247484783&idx=2&sn=625b41fe06e2d9a0b037497ebcae3b0d&scene=21#wechat_redirect)

◆ [DeepSeek 等模型训练所依赖的合成数据，BARE 提出了新思路](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247485128&idx=1&sn=ff746c724f716ac70b7387668e7f1d0f&scene=21#wechat_redirect)

◆🔥[Open-R1：深度揭秘 DeepSeek-R1 开源复现进展](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247485093&idx=1&sn=16420c134da394c0ade4fd69aea75d57&scene=21#wechat_redirect)

◆[Satori带来COAT：解锁LLM自省推理潜能，告别Deepseek教师模型](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247485000&idx=2&sn=6a590456a02806ce1fa38f7c024f7624&scene=21#wechat_redirect)

◆🔥[AI学会自我反思？Agent-R 使用蒙特卡洛树搜索(MCTS)自我训练自动纠错，让AI更聪明](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247484867&idx=1&sn=54dfc08be9c192f2f6d80811680f46ae&scene=21#wechat_redirect)

◆[CoRAG：RAG 模型的新思路，多跳问答性能显著提升](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247485049&idx=2&sn=b8c5b3a30ed5adc07a3cc2ae67e14029&scene=21#wechat_redirect) 

◆[Satori 带来 COAT：解锁LLM自省推理潜能，告别Deepseek教师模型](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247485000&idx=2&sn=6a590456a02806ce1fa38f7c024f7624&scene=21#wechat_redirect)

◆🔧[十大LLM基准测评：助力AI团队选型与研发](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247484145&idx=1&sn=433a9fa46d63a60e6b5d0c85deb0d418&scene=21#wechat_redirect)

◆[Meta 隐秘的 AI 训练数据获取：81.7TB 盗版书籍背后的真相](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247484818&idx=1&sn=0617133fac5c823158f3b86cda3f4801&scene=21#wechat_redirect)

◆🔥[AI 训练新风向： FP4 量化赋能大型语言模型训练，打破算力瓶颈](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247484786&idx=1&sn=51fa582b543e67abb5ac6f17e34daa2a&scene=21#wechat_redirect)

◆[微调重排序（reranker）模型：让 AI 更懂你的需求](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247484727&idx=3&sn=6b7ba737b52f32151401f8a698074f3e&scene=21#wechat_redirect)

◆[不要过多思考 2+3=？关于o1类LLMs的过度思考【论文】](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247483851&idx=2&sn=16cc36b6c3802b5ec7a37d7c874db0fc&scene=21#wechat_redirect)

◆🔥[AI的“人味儿”从何而来？DPO和LoRA打造更拟人化的AI](https://mp.weixin.qq.com/s?__biz=Mzk2NDA0MzcxNw==&mid=2247484243&idx=1&sn=a97bf7a8f0b249b7ec487063a326e793&scene=21#wechat_redirect)

  

  

注：本文素材由AI辅助翻译，内容由人工整理/审核发出

  

*欢迎*点** **![Image](https://mp.weixin.qq.com/s/www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg%20stroke='none'%20stroke-width='1'%20fill='none'%20fill-rule='evenodd'%20fill-opacity='0'%3E%3Cg%20transform='translate(-249.000000,%20-126.000000)'%20fill='%23FFFFFF'%3E%3Crect%20x='249'%20y='126'%20width='1'%20height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)****、**加** **![Image](https://mp.weixin.qq.com/s/www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg%20stroke='none'%20stroke-width='1'%20fill='none'%20fill-rule='evenodd'%20fill-opacity='0'%3E%3Cg%20transform='translate(-249.000000,%20-126.000000)'%20fill='%23FFFFFF'%3E%3Crect%20x='249'%20y='126'%20width='1'%20height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)****、**关注**。公号加⭐️精彩不错过**

---

我是肆〇柒🐝，一名热爱AI的互联网人。在这里，我分享自己的观察与思考，希望我的探索能激发同样热爱科技与生活的你，为你带来灵感与思考。  

  

期待我们的不期而遇。点击👇🏻关注

---

*🙋‍♂️*入群交流

1\. 公众号菜单点击“社群”，扫码入群。

2\. 回复“入群”“加群”等，添加作者微信进群。