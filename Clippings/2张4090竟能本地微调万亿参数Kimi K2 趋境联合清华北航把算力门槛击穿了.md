---
title: "2张4090竟能本地微调万亿参数Kimi K2！趋境联合清华北航把算力门槛击穿了"
source: "https://mp.weixin.qq.com/s/cNk68HrnBaVDABbsCGl4NQ"
author:
  - "[[关注前沿科技]]"
published:
created: 2025-11-05
description: "明星国产项目联动"
tags:
  - "低成本微调"
  - "超大模型"
  - "异构计算"
  - "个人工作站"
abstract: "趋境科技与清华KVCache.AI联合推出的KTransformers项目，结合LLaMA-Factory框架，仅需2-4张消费级4090显卡即可在本地微调万亿参数大模型，大幅降低算力门槛。"
---
![cover_image](https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtDVCwqrUg0SbK1bpyQ0tpP99QqBY2DQq8bl6agaNqxj8LkhA3icl0Xqx65eOXzFGqNdN6zO0ibI6ibCw/0?wx_fmt=jpeg)

Original 关注前沿科技 [量子位](https://mp.weixin.qq.com/s/) *2025年11月5日 15:53*

##### 金磊 发自 凹非寺量子位 | 公众号 QbitAI

**微调超大参数模型** ，现在的“打开方式”已经大变样了：

仅需 **2-4 张** 消费级显卡（4090），就能在本地对 **DeepSeek 671B** 乃至 **Kimi K2 1TB** 这样的超大模型进行微调了。

![Image](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtDVCwqrUg0SbK1bpyQ0tpP9z7KyR9uCvkjBkib5orttv7dFKC4sTsFyHLFkxqGYVldCeuowK8Us3Ew/640?wx_fmt=png&from=appmsg&watermark=1&tp=webp&wxfrom=5&wx_lazy=1#imgIndex=0)

你没有看错。

这要放以前啊，各路“炼丹师”是想都不敢这么想的。因为按照传统的方法，类似Kimi K2 1TB参数的模型，用LoRA微调方案理论上需要高达2000GB的显存，而即便是参数量稍小的 DeepSeek-671B的模型微调也需要1400G的显存。

什么概念？

一张H100（80GB）得十几张起步，说是吞矿也是不足为过了。

而现在微调千亿/万亿参数模型的成本能打如此骨折，背后的关键源自 **两个国产明星项目** 的联动。

首先就是 **KTransformers** ，是由趋境科技和清华KVCache.AI共同开源的项目，GitHub已经斩获15.3K星️。

![Image](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtDVCwqrUg0SbK1bpyQ0tpP9REyVZib7iabmwu687tc27FFHELeZXQicF0A4RbEtWDBpfTvpAagGaJgeA/640?wx_fmt=png&from=appmsg&watermark=1&tp=webp&wxfrom=5&wx_lazy=1#imgIndex=2)

KTransformer此前在大模型推理领域就已声名鹊起，凭借GPU+CPU的异构推理的创新路径成为主流推理框架之一，通过KTransformers利用单张4090可以推理Kimi K2 1TB级别大模型。

而这一次，KTransformers已经支持LoRA微调，同样是Kimi K2 1TB这样参数的模型，仅90G左右的显存即可；微调参数量稍小的 DeepSeek 671B也仅需70G左右的显存。真·把成本给打下去了。

另一个国产明星项目，则是 **LLaMA-Factory** ，在GitHub的星标数超6万。它是一个简单易用且高效的大语言模型训练与微调平台，让用户无需编写代码，即可在本地完成上百种预训练模型的微调。

![Image](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtDVCwqrUg0SbK1bpyQ0tpP9RQhBp3OMUuxv0IyGNuB0UfAJtZs3fCdzRAqGXT193SATt8ZHSTe8Mw/640?wx_fmt=png&from=appmsg&watermark=1&tp=webp&wxfrom=5&wx_lazy=1#imgIndex=3)

它俩的联动模式是这样的：

- LLaMA-Factory是整个微调流程的统一调度与配置框架，负责数据处理、训练调度、LoRA（Low-Rank Adaptation）插入与推理接口管理。
- KTransformers则作为其可插拔的高性能后端，在相同的训练配置下接管Attention / MoE等核心算子，实现异构设备的高效协同。

这时候或许有小伙伴要问了，把KTransformers换成其它类似的推理框架不行吗？

答案是，真不行。

例如我们把KTransformers、HuggingFace和Unsloth三种后端的LoRA微调方案放一起比较下效果。

![Image](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtDVCwqrUg0SbK1bpyQ0tpP9hMeMP5emPttXc384icV3OHL43P1joeB9ChjpCZhTkicbNfqPPOtecB4g/640?wx_fmt=png&from=appmsg&watermark=1&tp=webp&wxfrom=5&wx_lazy=1#imgIndex=4)

结果显示，KTransformers为超大规模的MoE模型（Kimi K2 1TB等）提供了4090级别的唯一可行方案，并在较小规模的MoE模型（DeepSeek-14B）上面也展现了更高的吞吐和更低的显存占用。

![Image](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtDVCwqrUg0SbK1bpyQ0tpP9lkQjMBAvKwzwkXALgPWRTZMT5wpGib2nY9sezIky2qQAU4cVEu4ZKfQ/640?wx_fmt=png&from=appmsg&watermark=1&tp=webp&wxfrom=5&wx_lazy=1#imgIndex=5)

嗯，KTransformers可以说是硬生生把微调超大模型的门槛，从数据中心级拉到了个人工作站级了，而且速度极快。

虽然成本是打下来了，但下一个问题是——效果会不会也打折？

## 用骨折的开销自定义千亿/万亿大模型

大模型用在专业领域的时候，往往令人头疼的一个点就是 **“懂得多≠懂得精”** ，这就是微调要解决的问题。  

而正所谓实践是检验真理的唯一标准，效果打不打折，实测说了算。 微调Kimi K2 1TB模型需要90G左右显存+2T左右的内存，微调 DeepSeek 671B模型需要70G左右显存+1.5T左右的内存。以下用 DeepSeek 671B模型为测试样例来看微调效果：

第一个测试的例子，是让DeepSeek在微调之后，生成的文字可以有 **喵娘** （CatGirl）效果的语气。  

数据集采用的是NekoQA-10K，是一种面向猫娘语言建模的对话数据集，来看下效果：

![Image](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtDVCwqrUg0SbK1bpyQ0tpP9TyPmaiaLL8PMk5NDxBf4jrWAUruq5OAms20QXzak5342o19xD1KtdrA/640?wx_fmt=png&from=appmsg&watermark=1&tp=webp&wxfrom=5&wx_lazy=1#imgIndex=6)

微调前，若是提问一个问题：

> 我舌头坏了怎么办，吃什么柠檬都是酸的。

模型的回答是冷冰冰的AI味：1、保持口腔卫生……2、避免刺激性食物……3、避免酸性食物……

Emmm……着实是莫得感情。

而微调后，模型的答案变成了：

> 主人舌头不舒服吗？宝宝好担心喵！（耳朵耸拉下来）柠檬酸是因为柠檬里面有柠檬酸啦，这是正常的喵~”。  
> 
> 微调前后，简直判若两AI，是我们想要的喵味十足的那种。

当然，不止是变喵娘这种整活儿，在严肃的专业领域，KTransformers的微调能力同样能打。

接下来的测试，采用了非洲医疗数据集（AfriMed-QA），包含医疗选择题和简答题，是垂直领域微调的试金石。

![Image](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtDVCwqrUg0SbK1bpyQ0tpP9urK3Ax8avr7gO5q3YYFAWwUrmJZ66FWEjG5icH2DdBPYU7iaZ5rz3t1w/640?wx_fmt=png&from=appmsg&watermark=1&tp=webp&wxfrom=5&wx_lazy=1#imgIndex=7)

在这些具有明确风格和知识需求的任务上，经过KTransformers后端LoRA微调的模型，各项评测指标（如BLEU、ROUGE、Accuracy）均获得了大幅提升。

这些个例子还都是开胃菜，微调背后真正有意思的，是开启了AI的个性化定制时代。

以前我们用大模型，基本上就是“模型有啥，你用啥”；但现在，成本打下来了之后，玩法就多了去了：

- 定制你的专属风格：不只是喵娘，你可以用自己的聊天记录、邮件、文档去微调，打造一个“你风格”的写作助手，让它帮你回邮件、写周报，口吻跟你一模一样。
- 打造私有知识库助手：把公司内部的SOP、技术文档、法律条文喂给它，微调出一个只为你公司服务的AI专家，问啥都懂，而且数据不出本地，绝对安全。
- 创造灵魂伴侣： 把你喜欢的某个角色、某位历史人物的语料丢进去，微调一个能随时随地和你角色扮演的聊天机器人。
- 深入垂直领域： 就像原稿里提到的，用专业数据集（比如医疗、法律）微调，模型在特定领域的表现会暴涨。这在严肃的专业领域同样有用。

这意味着，大模型不再是少数机构的专属技术，而成为高校、团队乃至个人都能驾驭的创意工具。算力门槛的消失，让更多垂直需求与独特想法得以实现，从而催生前所未有的应用创新。

对企业而言，KTransformers带来的低成本微调能力，也为落地大模型提供了新选项：

企业不再需要赌一个大而全的通用模型，而是可以快速在多个业务方向上进行测试，用私有数据喂出一个个懂自家业务的AI专家——无论是客服、营销还是内部知识管理，迭代效率和投资回报率都远超以往。

**这，才是低成本微调的真正魅力** ：它让大模型从一个高高在上的全知全能工具，变成了每个人、每个企业都能随心定制的专属生产力。

## 用起来也是超方便的

而且啊，KTransformers和LLaMA-Factory联动之下，操作方式也变得非常简单。

趋境科技为KTransformers本次封装了wheel包，避免本地编译，安装极简。

你只需同时安装KTransformers和LLaMA-Factory环境，把use\_kt设置为true，并指定相应的kt\_optimize\_rule YAML 文件，然后像往常一样启动LLaMA-Factory的训练命令。

LLaMA-Factory会自动负责所有的数据处理、训练调度、LoRA插入。而KTransformers则会作为即插即拔的高性能后端，在底层默默接管所有Attention和MoE的核心算子，实现GPU+CPU的高效协同。

若是用一个公式来总结二者的联动，或许可以是：

**底层极致性能（KTransformers）+ 上层易用性（LLaMA-Factory）= 微调界的平民法拉利**

至于背后的原理，我们可以简单总结为一套组合拳：

- 第一拳：把最重的包袱甩给CPU。 MoE模型最吃显存的专家层，KTransformers直接让CPU内存来扛。GPU解放出来专心算它擅长的。结果：671B的模型，显存占用从1400GB+理论值，硬是被压到了70GB！
- 第二拳：LoRA和高性能算子无缝合体。 简单说，它搞了个新设计，让你在享受KTransformers极致速度的同时，还能无缝插入LoRA微调，两边的好处都占了。
- 第三拳：榨干CPU。 甩给CPU的任务，也不是让它摸鱼。KTransformers集成了Intel AMX指令集，让CPU处理AI运算也猛得一批。

KTransformers背后的团队——趋境科技与清华KVCache.AI，值得再次被提及。

趋境科技在异构推理这件事上早就声名在外。他们最擅长的，就是“榨干”硬件的每一分性能，让GPU、CPU、内存协同作战，在推理上做到了极致的低成本和高性能，让许多跑不起昂贵GPU的团队也能用上大模型。

如今，趋境科技将这一优势延伸至微调领域，并与社区人气极高的LLaMA-Factory框架无缝集成，无疑是一次强强联合。

从推理到微调这一路径的发展，非常明显的就是剑指加速AI大模型落地，而且是更好更便宜的那种。

这对于资源有限的学术界、渴望快速迭代的创业公司，乃至充满热情的个人开发者来说，无异于一场及时雨。

而且此举还意味着，创新的边界被再次拓宽。你可以不再受限于模型的大小，而是专注于你的创意和数据——无论是打造一个独一无二的虚拟角色，还是构建一个解决特定行业痛点的专业模型。

最后，我们找到了微调的 [详细技术文档和用户操作指南](https://mp.weixin.qq.com/s?__biz=MzkyNDc1Mzg0MA==&mid=2247484885&idx=1&sn=8095c08624a38d70742cc31d74d0c209&scene=21#wechat_redirect) ，如果你手上现在就有几块消费级显卡，不妨可以尝试一下这个性价比极高的微调大法哦~

KTransformers项目地址：  
https://github.com/kvcache-ai/ktransformers

LLaMA-Factory项目地址：  
https://github.com/hiyouga/LLaMA-Factory

**一键三连** **「点赞」「转发」「小心心」**

**欢迎在评论区留下你的想法！**

— **完** —

**🌟 点亮星标 🌟**

**科技前沿进展每日见**

  

[Read more](https://mp.weixin.qq.com/s/)

继续滑动看下一个

量子位

向上滑动看下一个