---
title: "OpenAI最新研究：幻觉根本治不好，除非先改掉\"考试制度\""
source: "https://mp.weixin.qq.com/s/acMT8NjrrLXnK19ptHT6dA"
author:
  - "[[爱学习的Dr杨]]"
published:
created: 2025-09-23
description: "大模型为什么总是“一本正经地胡说八道”？最新研究指出，问题不在模型，而在我们的评价体系。"
tags:
  - "模型幻觉"
  - "评估体系"
  - "置信阈值"
  - "不确定性表达"
abstract: "OpenAI最新研究指出，大模型产生幻觉的根本原因在于当前鼓励猜测的评估体系，只有允许模型表达不确定性才能有效解决幻觉问题。"
---
![cover_image](https://mmbiz.qpic.cn/sz_mmbiz_jpg/nj8NL8ron3J244xAJHx65FibtibHE5ZmrHIHStKc9IKrafUBnIerrffNANG0SDuj76tGnsVlSGv2FArw7AsqOCpg/0?wx_fmt=jpeg)

Original 爱学习的Dr杨 [Grapher图说智能](https://mp.weixin.qq.com/s/) *2025年09月12日 15:38*

大模型为什么总是“一本正经地胡说八道”？最新研究指出，问题不在模型，而在我们的评价体系。

你有没有遇到过这样的情况：问一个语言模型某人的生日，它自信满满地给出一个完全错误的日期？或者让它数一个单词中有几个“D”，它却答非所问？这种现象被称为 **“幻觉”** ，是当前大语言模型最令人头疼的问题之一。

OpenAI 团队在2025年9月发表的一篇论文《Why Language Models Hallucinate》中，首次从统计学习和评估体系的角度系统分析了幻觉的根源。他们指出， **幻觉并非模型缺陷，而是当前训练与评估体系的必然结果** 。更惊人的是，研究者们直言： **除非彻底改变现有的“考试式”评估体系，否则幻觉问题根本无法根治** 。

## 一、什么是幻觉？为什么它如此棘手？

幻觉指的是模型在不确定时不是承认“我不知道”，而是 **生成一个看似合理但实际错误的回答** 。例如：

- 问：“Adam Tauman Kalai 的生日是哪天？”
- 模型答：“9月30日”（实际在秋季，但并非这一天）

甚至有三个主流模型在被问及“Adam Kalai 的博士论文题目”时，分别给出了三个不同的错误答案，而没有一个答对。

幻觉之所以严重，是因为它 **摧毁了用户对模型的信任** 。尤其是在医疗、法律、金融等高风险领域，一个看似自信的错误答案可能导致严重后果。

## 二、幻觉的根源：从预训练开始就注定了

### 1\. 预训练阶段：模型天生就会“猜”

论文通过一个巧妙的 **“Is-It-Valid”（IIV）** 二分类问题，将生成错误与分类错误联系起来。结论是：

**生成错误率 ≳ 2 × 二分类错误率**

这意味着， **只要模型在判断“某个回答是否合理”时会出错，它就一定会在生成内容时出现幻觉** 。

举个例子：模型在训练数据中只见过一次某人的生日（比如在某篇报道中提及），那么它 **有相当高的概率在测试时“编造”一个生日** 。这是因为模型无法从稀疏数据中学习到真实规律，只能依靠统计猜测。

> “就像你只见过某人一次，却要猜他的生日，你很可能猜错——语言模型也是如此。”

### 2\. 模型越“校准”，越容易幻觉

一个反直觉的结论是： **模型越是被训练得输出“校准”过的概率（即置信度与准确率匹配），它就越容易产生幻觉** 。因为校准要求模型在不确定时也要输出一个概率分布，而不是拒绝回答。

## 三、后训练阶段：为什么RLHF也救不了幻觉？

很多人认为，通过 **人类反馈强化学习（RLHF）** 或其它后训练方法可以减轻幻觉。但论文指出： **当前的评估体系一直在暗中鼓励模型“猜答案”** 。

### 现在的评估就像“考试评分”：

- 答对：得1分
- 答错：得0分
- 不答/说“我不知道”：也是0分

在这种情况下，模型发现： **猜一把还有可能得分，不说就肯定没分** 。所以它宁愿“瞎猜”也不愿表达不确定性。

论文中称之为 **“惩罚不确定性的流行病”** （epidemic of penalizing uncertain responses）。

## 四、解决方案：改掉“考试制度”，允许模型说“我不知道”

OpenAI 团队提出， **唯一有效的方法是修改主流评估基准的评分方式** ，而不是增加更多的“幻觉专项评测”。

### 具体建议：引入“明确置信度阈值”

在每个问题中明确告诉模型：

> “如果你置信度超过90%才回答，答错扣9分；答对得1分；说‘我不知道’得0分。”

这样模型才会在不确定时选择“不答”，而不是盲目猜测。

目前， **绝大多数主流评测（如MMLU、GPQA、SWE-bench等）都是“二元评分”** ，即只判断对错，不给“我不确定”任何分数。这正是幻觉屡禁不止的根本原因。

## 五、总结：幻觉不是技术问题，是系统问题

OpenAI 的这篇论文打破了我们对幻觉的传统认知：

- 🧠 **幻觉不是BUG，而是当前统计学习框架下的必然产物** ；
- 📉 **预训练阶段就注定了模型会幻觉** ；
- 🎯 **后训练和RLHF之所以效果有限，是因为评估体系在鼓励模型“猜”** ；
- ✅ **唯一的治本之道是修改评测标准，允许模型表达不确定性** 。

如果我们继续用“考试”的方式评测模型，那它就会继续做一个“只会猜答案的考生”。只有当我们允许它说“我不知道”，它才能真正成为一个值得信任的助手。

## 六、对研究者、模型工程师、产品经理的实操建议（落地清单）

- 研究者：设计新的 benchmark 时 **显式写明置信门槛** 与放弃的计分规则，报告放弃率（abstention rate）和不同阈值下的准确率。
- 模型工程师：在训练/微调时考虑 行为层面的校准目标 （例如训练模型学习在低置信情况下输出 I don't know），并在 RLHF 策略里把“合理放弃”纳入偏好。
- 产品/产品经理：上线前明确哪些场景需要强制“拒答”策略（例如医疗、法律类高风险场景），并在界面上清晰显示置信度或拒答提示，避免把“流畅”当成“可靠”。
- 社区/标准组织：推动主流排行榜（leaderboards）和竞赛采纳置信目标，才能改变行业激励。

  

  

继续滑动看下一个

Grapher图说智能

向上滑动看下一个