---
title: "Agentic CPT 揭示智能体缩放新定律，模型越大≠Agent越强！"
source: "https://mp.weixin.qq.com/s/OR1u06hxWL5Dj-cBAbjbgw"
author:
  - "[[Tensorlong 看天下]]"
published:
created: 2025-09-19
description: "❝一句话概括： 作者认为，让大模型边学Agent技能边对齐任务是在“左右互搏”，于是提出一个名为 Agenti"
tags:
  - "智能体预训练"
  - "数据合成"
  - "缩放定律"
  - "解耦训练"
abstract: "阿里巴巴提出Agentic CPT方法，通过解耦训练先培养大模型的基础智能体能力再微调任务，在多个基准测试中达到SOTA性能。"
---
Original Tensorlong 看天下 *2025年09月18日 14:20*

![Image](https://mmbiz.qpic.cn/sz_mmbiz_png/Z24DyenWDNjlz58PN7UG2DictRYyquYWFClx239SUXYKKf8ia99zEZgK0SnMSrwNVFv8rJWWhibeZIeIhb75PD0BQ/640?wx_fmt=png&from=appmsg&watermark=1&tp=webp&wxfrom=5&wx_lazy=1#imgIndex=0)

> ❝
> 
> 一句话概括： 作者认为，让大模型边学Agent技能边对齐任务是在“左右互搏”，于是提出一个名为 Agentic CPT 的“解耦”方案，先把Agent的基础能力（如规划、决策）通过持续预训练单独刻进模型里，效果拔群。（原论文题目见文末，点击阅读原文可直接跳转至原文链接， Published on arxiv on 16 Sep 2025, by Alibaba Group）

### 第一阶段：识别核心概念

#### 论文的motivation分析

这篇论文的出发点非常明确，它指出了当前开源AI Agent（智能体）领域的一个普遍痛点：尽管大型语言模型（LLM）本身越来越强大，但基于这些通用模型通过后期微调（Post-training）构建的Agent，在处理复杂的、需要多步推理和工具使用的任务时，其性能总是与顶尖的闭源模型（如OpenAI的Deep Research Agent）存在巨大差距。

作者认为，问题的根源在于一个"根本性的优化矛盾"。传统的做法是拿一个通用的、什么都懂但什么都不精的"通才"模型（general-purpose foundation models），然后通过微调，强行让它同时学习两件复杂的事情：

1. \*\*学习"如何成为一个Agent"\*\*：掌握使用工具、规划步骤、应对环境变化等一系列复杂的"智能体行为"（agentic behaviors）。
2. \*\*学习"如何完成特定任务"\*\*：根据专家示范的数据，对齐到具体的任务目标上。

这就好比让一个刚毕业的医学生，一边从零开始学习如何使用手术刀、缝合针等各种复杂器械（学习Agent能力），一边又要他照着顶尖专家的手术录像，完美复现一台高难度的脑部手术（对齐任务）。这种"一心二用"的训练方式，效率低下且效果不佳，模型很难真正内化底层的Agent能力。

因此，作者提出，我们不应该跳过基础训练，直接进行专科手术实操。我们应该在通用模型和任务微调之间，增加一个专门的"Agent预科班"阶段，先让模型成为一个合格的"准Agent"，再让它去学习具体的任务。这个"预科班"，就是论文的核心思想——\*\*Agentic Continual Pre-training (Agentic CPT)\*\*。

#### 论文主要贡献点分析

- **论文声称的主要创新点**
- **提出新的训练范式** ——首次将"Agentic Continual Pre-training"（Agentic CPT）作为一个独立的、关键的阶段，正式引入到AI Agent的训练流程中，旨在构建强大的"智能体基础模型"（Agentic Foundation Models）
	- **提出可扩展的数据合成方法** ——为了支撑Agentic CPT所需的海量数据，论文设计了两套创新的、无需真实调用昂贵API（如搜索引擎）的数据合成方法：一阶行为合成（First-order Action Synthesis, FAS）和高阶行为合成（Higher-order Action Synthesis, HAS）
	- **发布了一个SOTA模型** ——基于上述方法，团队开发并开源了一个名为AgentFounder的30B模型。该模型在10个主流Agent基准测试中取得了当前最先进（State-of-the-Art, SOTA）的性能，证明了该方法的有效性
- **支撑这些创新的关键技术或方法**
- **Agentic CPT Pipeline** ——整个训练流程被重新设计为"通用预训练 -> Agentic CPT -> 任务微调"三段式。其中的Agentic CPT又被细分为两个阶段：第一阶段用海量数据和标准上下文长度（32K）学习基础Agent技能；第二阶段用高质量长文本数据和扩展上下文长度（128K）学习复杂的长程规划能力
	- \*\*First-order Action Synthesis (FAS)\*\*——这是一种"无中生有"的数据制造技术。它首先将海量文本知识构建成一个围绕"实体"的知识库，然后基于这些知识自动生成复杂的问题，并进一步合成解决这些问题所需的"规划"和"推理"步骤
	- \*\*Higher-order Action Synthesis (HAS)\*\*——这是一种"变废为宝"的数据增强技术。它利用在其他训练（如强化学习）中被丢弃的"不完美"的Agent执行轨迹，在轨迹的每一步，都让模型思考"除了这么做，我还能怎么做？"，从而将一条单调的执行路径，扩展成一个包含多种决策可能性的"决策树"式训练数据
- **论文的显著性结果**
- **性能的显著超越** ——AgentFounder-30B在多个高难度基准测试上大幅领先于所有已有的开源Agent模型，例如在HLE测试上达到了31.5%，成为首个突破30分大关的开源模型
	- **验证了Agentic CPT的普适性** ——实验证明，无论后续采用何种微调策略，使用经过Agentic CPT的AgentFounder-Base模型作为起点，都比直接使用通用的Qwen3-Base模型效果更好。这表明Agentic CPT提供了一个更优越的"Agent能力基座"
	- **揭示了Agent能力的Scaling Law** ——论文通过实验证明，Agent的能力也遵循类似LLM的"缩放定律"。随着模型尺寸的增大和Agentic CPT数据量的增加，Agent的性能会稳步提升。这为未来如何"大力出奇迹"地培养更强Agent指明了方向

#### 理解难点识别

- **理解论文的关键概念/方法**
- \*\*Agentic Continual Pre-training (Agentic CPT)\*\*——这是全文的顶层设计，理解它与传统微调（SFT/RL）和通用预训练的区别至关重要
	- \*\*First-order Action Synthesis (FAS)\*\*——这是实现Agentic CPT的数据来源之一，其核心在于如何低成本、大规模地创造出模拟Agent思考过程的"规划"和"推理"数据
	- \*\*Higher-order Action Synthesis (HAS)\*\*——这是另一个关键数据来源，其核心在于如何从已有的、不完美的执行轨迹中，榨取出关于"决策"和"选择"的宝贵学习信号
- **最具挑战性的部分**
- 最具挑战性的部分是理解 **FAS 和 HAS 的内在机制和设计哲学** 。特别是，它们是如何在 **不实际执行工具** （Offline）的情况下，生成高质量的、包含"思考链"和"工具调用意图"的训练数据的。这两种方法的区别与互补关系也是一个关键点。FAS像是从零编写教材，而HAS像是对过往案例进行复盘，两者共同构建了Agentic CPT的"课程体系"
- **需要重点解释的核心概念**
- 我们将重点解释 **FAS（一阶行为合成）和 HAS（高阶行为合成）** 。因为它们是实现Agentic CPT这个宏大构想的具体技术抓手，是整个工作的基石。理解了它们，就理解了论文的创新之源

#### 概念依赖关系

1. **切入点** ：论文的核心论点是 **Agentic CPT** 能有效提升Agent能力。
2. **依赖一** ：要实施Agentic CPT，就需要海量的、专门用于培养Agent能力的训练数据。
3. **依赖二** ：获取这类数据成本高昂（如API费用和人工标注）。
4. **解决方案** ：因此，论文提出了低成本的离线数据合成方法 **FAS** 和 **HAS** 。
5. **关系梳理** ：FAS和HAS是实现Agentic CPT的技术基础。FAS负责从静态知识中创造出基础的"思考-行动"模式数据，而HAS负责从动态的执行轨迹中提炼出高级的"多路径决策"数据。两者共同为Agentic CPT提供了丰富、多样且可扩展的"教材"。

---

### 第二阶段：深入解释核心概念

#### 设计生活化比喻

- **培训一位实习侦探**

想象一下，我们要办一个"福尔摩斯侦探学院"，目标是将一批聪明但毫无经验的"实习侦探"（通用的LLM）培养成能够独立破案的"精英侦探"（如AgentFounder）。传统的训练方法是直接把他们扔到真实的、复杂的案发现场去学习，结果自然是一团糟。

我们的"福尔摩斯学院"则采用了创新的两阶段教学法，这正对应着论文中的FAS和HAS。

- **第一阶段课程：案件沙盘推演（对应FAS）**
- **建立案件档案库** ——我们不让实习生去真实案发现场。相反，我们收集了海量的卷宗、报纸、法医报告等静态资料（ **静态知识源** ），建立了一个庞大的"案件档案库"
	- **设计虚构案件** ——然后，课程教官（一个辅助模型）会从档案库中抽取几份看似无关的资料，比如"一份关于A富商的财务报告"、"一张B城市码头的地图"和"一篇C地区化学品泄漏的新闻"，然后精心设计一个 **全新的、虚构的案件** （ **多风格问题合成** ）。例如："在B城码头附近失踪的A富商，其失踪是否与C地区的化学品泄漏有关？"
	- **沙盘推演训练** ——接下来，实习侦探的任务不是去破案，而是在纸上进行 **沙盘推演** 。他们需要写下自己的 **初步分析** （ **推理行为合成** ）和 **下一步的调查计划** （ **规划行为合成** ）。比如："我的思路是：首先，调查A富商的财务报告，看有无异常资金流动，这可能揭示作案动机。我计划下一步调用'财务背景调查'工具来核实这一点。"
	- **成本优势** ——这个过程完全在教室里完成，不需要派人去实地调查（ **无需API调用** ），成本极低，但能大规模地训练实习侦探的 **逻辑推理** 和 **行动规划** 能力。这就是 **FAS** 的精髓
- **第二阶段课程：真实案例复盘（对应HAS）**
- **真实案例录像** ——当实习侦探有了一些基础后，我们拿出一些 **过往的真实案例录像** （ **已有的Agent执行轨迹** ），其中甚至包括一些探员搞砸了的失败案例
	- **关键节点讨论** ——课程会播放录像，并在每个关键节点暂停。比如，录像里的老侦探在发现了A线索后，决定去审问嫌疑人B。这时，教官会问所有实习生："各位，如果你是当时的老侦探，除了去审问B，你们还有哪些选择？（ **步级扩展** ）比如，是不是可以先去化验A线索？或者去调查嫌疑人C的不在场证明？"
	- **多路径学习** ——然后，教官会让实习生们把所有可能的选项（审问B、化验A、调查C）都列出来，并最终告诉他们，当时老侦探选择了"审问B"，结果是"嫌疑人B情绪激动，透露了D信息"，而整个案件的最终结果是"成功破案"
	- **决策能力培养** ——通过这种方式，实习侦探学习的不再是一条固定的破案路径，而是在每个决策点上， **权衡不同选择的优劣** 。他们学习的是一种 **更高阶的决策能力** 。这就是 **HAS** 的精髓，它把单线程的执行记录，变成了一个富含决策智慧的多路径学习材料

#### 建立比喻与实际技术的对应关系

| **比喻中的元素** | **对应的实际技术概念** | **合理性解释** |
| --- | --- | --- |
| **实习侦探** | 通用的大型语言模型 (如 Qwen3-Base) | 聪明、有潜力，但缺乏特定领域的专业技能。 |
| **福尔摩斯侦探学院** | **Agentic Continual Pre-training (Agentic CPT)** | 一个专门的、介于基础教育和实战之间的强化训练阶段。 |
| **案件档案库 (卷宗、报告)** | 静态知识源 (网页、维基百科、代码库等) | 大量的、未经组织的原始信息。 |
| **设计虚构案件** | First-order Action Synthesis (FAS) | 从静态知识中创造出需要Agent去解决的动态问题场景。 |
| **在纸上写分析和调查计划** | 生成"思考链"和"工具调用意图" | 模拟Agent的内在思维过程，但不实际执行，从而降低成本。 |
| **真实案例录像** | 已有的Agent执行轨迹 (Trajectory) | 记录了Agent在解决真实问题时的每一步操作和反馈。 |
| **在关键节点暂停，讨论所有选项** | Higher-order Action Synthesis (HAS) | 将单路径的轨迹数据，扩展为包含多个备选行为的决策学习数据。 |
| **精英侦探** | 训练完成的Agent模型 (AgentFounder) | 掌握了底层推理、规划和决策能力，准备好应对真实、复杂的任务。 |

#### 深入技术细节

现在，我们从比喻回到技术。整个Agentic CPT的核心是让模型学习预测"下一步该说什么/做什么"，这在技术上依然遵循着语言模型最基本的学习范式： **Next-Token Prediction** 。

- **原始数学形式**
	论文中给出的基本损失函数是标准的交叉熵损失：
- **符号替换版本**
	这个公式可以这样解读：
- `L`: 模型的总学习误差（Total Error），或者叫"总困惑度"。
	- `Σ`: "把每个位置的误差加起来" 的意思。
	- `-log P(...)`: 这是一个衡量"意外程度"的数学技巧。如果模型预测正确下一个词的概率 `P` 很高（比如99%），那么 `-log(P)` 就接近于0，误差很小；如果概率 `P` 很低（比如1%）， `-log(P)` 就会变得非常大，误差很大。
	- `$x_{t+1}$`: 在训练文本中，位置 `t` 后面那个"正确的"词。
	- `| $x_1, ..., x_t$`: 表示"基于前面从第1个到第t个词的全部内容"这个条件。
- **技术实现关键步骤**
- **FAS 数据生成流程** ——知识抽取与组织：系统扫描网页等数据源，围绕特定实体（如"巴黎"）构建知识陈述句；问题生成：模型读取关于"巴黎"的多条知识，生成一个需要结合这些知识才能回答的复杂问题；行为合成：给定这个问题和相关知识，模型被要求生成一段文本。这段文本的格式是：" `<think>` 我应该这样分析...第一步是...第二步是... `</think><tool_call>` 我决定调用搜索工具，搜索关键词是'...' `</tool_call>` "。这整段文本就成了训练数据中的一个片段
	- **HAS 数据生成流程** ——轨迹选择：从之前的任务中获取一条完整的Agent执行轨迹，形如 `(思考1, 行动1, 结果1), (思考2, 行动2, 结果2), ...`；步级扩展 (Step-level Scaling)：在第 `k` 步，模型会看到 `思考1...结果k-1` 的所有历史信息，然后被要求生成 `N` 个除了原始的 `思考k` 之外的、同样合理的其他思考和行动方案；决策重构 (Contrastive Decision-Action Synthesis)：将原始轨迹重构成一段新的训练文本，格式如下："问题是... 在第一步，有以下几个选项：\[选项A, 原始选项B, 选项C\]。我决定选择原始选项B。 `<tool_call>`...`<tool_response>`... 最终，整个任务成功了。"

#### 将技术细节与比喻相互映射

- **公式与比喻的映射**
- 在我们的侦探学院比喻中，那个核心的数学公式 `$L = ...$` 就好比是 **教官的评分标准**
	- `$x_1, ..., x_t$` 是实习侦探已经写下的分析报告的前半部分
	- `$x_{t+1}$` 是标准答案（比如，由资深侦探写的报告）中的下一个词
	- `$P(x_{t+1} | ...)$` 是实习侦探写出和标准答案一模一样的下一个词的"可能性"
	- `L` (总误差) 就是教官在整篇报告上画的红叉的总数。训练的目标，就是让实习侦探（模型）的思维方式和表达方式无限接近于资深侦探（高质量训练数据），从而让红叉最少
- **技术步骤在比喻中的体现**
- **FAS的数据生成** ，就是教官们在办公室里，根据档案库编写"沙盘推演"教材的过程
	- **HAS的数据生成** ，就是教官们带着实习生，一遍遍地观看和复盘"真实案例录像"的过程
- **比喻的局限性**
- 这个比喻很好地解释了FAS和HAS的"离线"和"思考过程模拟"的特性。但它没有完全体现出训练过程的"自回归"（auto-regressive）性质，即模型是一个词一个词地生成文本的。不过，对于理解FAS和HAS的核心思想来说，这个局限性影响不大

#### 总结

通过"培训实习侦探"的比喻，我们可以清晰地理解：

- **FAS** 就像是 **编写理论课教材和沙盘案例** ，它让模型从静态知识中学会基本的 **分析和规划能力** 。
- **HAS** 就像是 **进行实战案例复盘** ，它让模型从过去的经验中学会 **权衡利弊和动态决策能力** 。

这两种方法共同为Agentic CPT提供了丰富、廉价且高效的训练数据。而这一切训练的底层数学原理，都统一在那个简单的 **Next-Token Prediction公式** 之下：通过海量的"模仿"和"预测"练习，让模型的大脑中形成深刻的、属于精英侦探（Agent）的思维模式。

---

### 第三阶段：详细说明流程步骤

#### Agent生产线

**输入** ：一个复杂、开放式的用户问题。例如论文中那个关于"在巴黎举办的航空展上，哪家公司下了一笔订单和期权数量完全相等的订单？"的难题。

**生产线起点：原材料**

- **模型** ：一个预训练好的、通用的基础语言模型，比如 `Qwen3-30B-A3B-Base` 。这相当于一块潜力巨大但未经雕琢的"璞玉"。

**第一道工序：Agentic CPT - 智能体基础能力刻画**

这是整个流程的核心创新环节，它本身又分为两个子阶段，旨在为模型注入通用的Agent能力。

- **子阶段一：基础技能强化 (Stage 1)**
- **数据准备** ——首先，系统会使用我们之前深入讨论的 **FAS（一阶行为合成）** 和 **HAS（高阶行为合成）** 方法，生成规模极其庞大的训练数据（论文中提到约200B tokens）。这些数据主要由FAS生成，包含了大量的"问题-思考-规划"片段，以及部分较短的、由HAS生成的决策复盘数据
	- **训练配置** ——将 `Qwen3-Base` 模型放入训练环境中。上下文长度设置为一个标准值，比如32K。这意味着模型在学习时，一次最多能看到32K个词的内容
	- **训练过程** ——模型开始读取这些合成数据，并根据标准的"Next-Token Prediction"任务（即预测下一个词是什么）进行学习。通过阅读海量的"沙盘推演"和"案例复盘"材料，模型开始学习：如何将一个复杂问题分解成多个子问题；识别解决问题需要哪些信息；规划出获取信息的第一步应该是什么（比如，应该用搜索引擎，还是代码解释器）；进行初步的逻辑推理和信息整合
	- **产出** ——经过这个阶段，模型已经初步具备了Agent的思维雏形，但对于需要长篇大论、跨越很长上下文的复杂规划还比较欠缺
- **子阶段二：长程规划与决策精炼 (Stage 2)**
- **数据准备** ——系统筛选出质量更高的数据，特别是那些由 **HAS** 生成的、包含复杂多步决策的轨迹，以及由 **FAS** 生成的、需要长篇推理的逻辑题。数据总量比第一阶段少（约100B tokens），但"精"得多
	- **训练配置** ——上下文长度被大幅扩展到128K。这使得模型能够一次性读完一个非常长的、完整的复杂案例复盘记录
	- **训练过程** ——模型继续进行"Next-Token Prediction"训练。得益于超长的上下文窗口，模型现在可以学习：如何在前几十步的行动和结果的基础上，规划出后面几十步的策略；理解早期决策对最终结果的深远影响；在长篇报告中保持逻辑的一致性和连贯性
	- **产出** —— **AgentFounder-30B-Base** 模型。这块"璞玉"已经被精心雕刻，具备了坚实的、通用的Agent基础能力，可以被称为"智能体基础模型"。它已经是一个合格的"侦探学院毕业生"了

**第二道工序：Post-training SFT - 领域专家技能微调**

虽然模型已经很强了，但它还需要针对特定的工作场景进行最后的"岗前培训"。

- **数据准备** ——收集或构造一批高质量的、针对"深度研究"这一特定任务的完整Agent执行轨迹。这些数据是"专家级"的示范，展示了从接收问题到给出最终研究报告的全过程
- **训练过程** ——使用这些专家数据，对 `AgentFounder-30B-Base` 模型进行监督微调（Supervised Fine-Tuning, SFT）。这个过程的训练量远小于CPT阶段。其目标是：让模型的行为和输出风格与最终产品要求对齐；进一步强化在"深度研究"任务中最高频、最关键的技能组合
- **产出** ——最终的成品—— **AgentFounder-30B** 模型

**生产线终点：成品交付**

- **推理执行** ——当最终的 `AgentFounder-30B` 模型接收到最开始那个复杂的用户问题时，它会启动它的"Agent模式"：
- **思考（Think）** ——模型会首先生成一段 `<think>`... `</think>` 的内心独白，分析问题、拆解任务、制定计划
	- **行动（Act）** ——根据计划，模型生成一个 `<tool_call>`... `</tool_call>` 指令，调用外部工具（如搜索引擎、网页浏览器、代码解释器）
	- **观察（Observe）** ——模型接收到工具返回的结果 `<tool_response>`... `</tool_response>`
	- **循环** ——模型将新的观察结果纳入自己的思考中，再次生成 `<think>`... `</think>` ，并决定下一步行动，如此循环往复
	- **最终回答** ——当模型判断已收集到足够信息并完成推理后，它会生成最终的、结构化的研究报告，回答用户的问题

通过这个详尽的流程，读者应该可以清晰地看到，一个通用的LLM是如何通过Agentic CPT（基础能力刻画）和Post-training SFT（专家技能微调）这两大核心工序，被一步步打造成一个能够解决复杂问题的强大Agent的。整个过程环环相扣，前一阶段的输出，精确地成为了后一阶段的输入。

---

### 第四阶段：实验设计与验证分析

#### 主实验设计解读：核心论点的验证

- **核心主张** ：引入 Agentic CPT 这一中间阶段，可以构建出性能远超传统微调方法的"智能体基础模型"，从而显著提升AI Agent在复杂任务上的表现。
- **实验设计分析** ：
- **数据集选择** ——作者选择了10个在AI Agent领域广为认可的基准测试（Benchmarks），这些测试可以分为两大类：通用网页搜索类，如 `BrowseComp-en/zh`, `GAIA`, `Xbench-DeepSearch` 。这类任务模拟了用户最常见的深度信息检索需求，是评估Agent核心能力的"必考题"；特定场景类，如 `HLE` （高难度专家级问答）、 `Frames` （多视角信息整合）、 `SEAL-0` （对抗性/误导性信息鲁棒性测试）、 `Academic Browse` （学术研究能力）。这些数据集覆盖了Agent能力的多个维度，从知识深度到逻辑鲁棒性，考察非常全面。这个选择是 **非常合理** 的，因为它既包含了通用能力的考察，也深入到了特定高级能力的测试，能够全面、立体地评估一个Agent的综合实力
	- **评价指标** ——主要采用 `Pass@1` （一次通过率）和准确率（Accuracy）。对于Agent任务， `Pass@1` 是最直接、最苛刻的指标，它衡量了Agent在没有人为干预的情况下，一次性成功解决问题的能力，这非常符合现实世界对自动化智能体的期望
	- **基线方法（Baselines）** ——作者选择了三类极具代表性的对手进行比较：通用LLMs+Tools，如 `Qwen3`, `DeepSeek-R1`, `Claude-4-Sonnet` 。这代表了"聪明大脑+临时工具"的模式，用于证明仅有强大的通用智能是不够的；商业闭源Deep Research Agents，如 `Kimi-Researcher`, `OpenAI Deep Research`, `Gemini Deep Research` 。这是业界的"黄金标准"，是作者想要追赶和超越的目标；开源Deep Research Agents，如 `DeepSeek-V3.1`, `GLM-4.5`, `WebSailor-72B` 。这是最直接的竞争对手，用于证明 `AgentFounder` 在开源社区中的领先地位。这样的基线选择 **极具说服力** ，因为它构建了一个从"基础水平"到"业界顶尖"的全方位坐标系，能清晰地标定出 `AgentFounder` 的性能位置
- **主实验结论** ：
- 在 **表1和表2** 中， `AgentFounder-30B` 的性能数据非常亮眼。它不仅在几乎所有指标上都 **碾压** 了其他所有开源Agent，而且在多个高难度测试（如 `HLE`, `Frames`, `Academic Browse` ）上 **超越** 了包括 `Gemini` 和 `OpenAI` 在内的商业闭源Agent。这强有力地证明了论文的核心论点： **通过Agentic CPT训练出的Agent，确实达到了SOTA（State-of-the-Art）水平**

#### 消融实验分析：内部组件的贡献

消融实验的目的是回答"我的方法之所以好，是因为我提出的A、B、C模块都起了作用，缺一不可"。

- **关键模块1：Agentic CPT 本身 (表3)**
- **实验设计** ——作者做了两组对比实验。一组直接用通用的 `Qwen3-30B-A3B-Base` 进行三种不同的SFT微调；另一组则先对 `Qwen3-Base` 进行Agentic CPT得到 `AgentFounder-30B-Base` ，再用同样的三种SFT进行微调
	- **被消融的部分** ——整个"Agentic CPT"阶段
	- **结论** ——在所有三种SFT策略下，从 `AgentFounder-Base` 出发的模型性能都 **显著高于** 从 `Qwen3-Base` 出发的模型（例如，在SFT-B策略下，BrowseComp-en得分从28.6%暴涨到39.9%）。这无可辩驳地证明了 Agentic CPT 作为一个预备阶段，为后续的微调提供了一个 **高得多的起点** ，其贡献是巨大且普适的
- **关键模块2：两阶段训练策略 (表4)**
- **实验设计** ——比较只进行Stage 1 CPT的模型和完成了Stage 1+2完整CPT的模型
	- **被消融的部分** ——CPT的第二阶段（长上下文、高质量数据精炼）
	- **结论** ——完成了两个阶段的模型在所有测试上性能都更好（例如，BrowseComp-zh的Pass@3从50.5%提升到58.5%）。这证明了 **专门针对长程规划的第二阶段训练是必要的** ，它能带来单纯增加第一阶段数据量所无法达到的效果
- **关键模块3：FAS和HAS数据 (表5)**
- **实验设计** ——比较了三种情况：不用CPT、只用FAS数据进行CPT、用FAS+HAS混合数据进行CPT
	- **被消融的部分** ——HAS数据
	- **结论** ——仅使用FAS数据就已经带来了巨大的性能提升（例如，BrowseComp-zh的Pass@1从29.8%提升到37.0%）。在FAS的基础上再加入HAS数据，性能还能进一步提升（40.1%）。这定量地证明了： **FAS是构建Agent基础能力的核心，而HAS则提供了宝贵的、互补的决策能力，两者结合效果最佳**

#### 深度/创新性实验剖析：洞察方法的内在特性

除了证明"我能行"，这些实验还试图回答"我为什么行"以及"我的潜力有多大"。

- **创新性实验1：Scaling Law探索 (图6)**
- **实验目的** ——不仅仅是展示模型性能，而是要验证 Agentic CPT 这个 **方法论** 本身是否具有良好的 **可扩展性** 。这是判断一个方法是否有前途的关键
	- **实验设计** ——作者绘制了两条曲线：(a) 模型性能随模型尺寸（1B, 4B, 30B）的变化；(b) 模型性能随CPT训练数据量（0B到315B tokens）的变化
	- **实验结论** ——两条曲线都呈现出清晰的 **对数增长** 趋势。这意味着，只要持续投入更多的计算资源（更大的模型）和数据（更多的CPT数据），Agent的性能就会持续、可预测地增长。这揭示了方法的 **巨大潜力** ，并给出了一个明确的"变强公式"
- **创新性实验2：训练过程可视化 (图7)**
- **实验目的** ——深入模型训练的"黑箱"，从 **训练动态** （Training Dynamics）的角度证明Agentic CPT的优势
	- **实验设计** ——作者没有比较最终的准确率，而是画出了在进行SFT微调时，不同起点模型（有无经过CPT）的 **损失（Loss）下降曲线**
	- **实验结论** ——经过Agentic CPT的 `AgentFounder` 模型，在SFT一开始就具有 **更低的初始Loss** ，并且最终收敛到了一个 **更低的Loss值** 。这个结果非常巧妙和深刻，它直观地表明：Agentic CPT 相当于给模型进行了"预对齐"，使得后续的微调任务变得 **更简单、更高效** ，模型能学得更"透彻"
- **创新性实验3：Agent行为分析 (图8)**
- **实验目的** ——研究训练出的Agent是否真的"智能"，即它能否根据任务的难易程度 **自适应地调整其行为策略**
	- **实验设计** ——作者统计了Agent在解决不同类型任务时，调用工具的次数分布。横轴是工具调用次数，纵轴是样本比例
	- **实验结论** ——在面对 `BrowseComp` 和 `HLE` 这种开放式研究任务时，工具调用次数呈现"长尾分布"，说明Agent会进行大量、深入的探索。而在面对 `GAIA` 和 `WebWalker` 这种结构化任务时，工具调用次数则集中在较少的几次，说明Agent会采取更直接、高效的策略。这证明了 `AgentFounder` \*\*学会了"见机行事"\*\*，而不是一个只会死板执行流程的"脚本小子"

---

本文题目：Scaling Agents via Continual Pre-training

**欢迎Deep Learning同好与我交流、讨论、合作！**

**现已提供论文解读和idea讨论定制服务，可私信后台联系**

**公众号广告位招租，欢迎咨询👏**

  

个人观点，仅供参考

[Read more](https://mp.weixin.qq.com/s/)

继续滑动看下一个

沈公子今天读什么

向上滑动看下一个