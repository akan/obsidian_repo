---
title: "大模型RL篇：VSRM 终结大模型“过度思考”的AI效率革命"
source: "https://mp.weixin.qq.com/s/8n2QOMVDw3dJQLoLqpcwOg"
author:
  - "[[TommyYang]]"
published:
created: 2025-10-14
description:
tags:
  - "过度思考"
  - "逐步奖励"
  - "推理效率"
  - "强化学习"
  - "模型优化"
abstract: "VSRM通过可验证的逐步奖励机制解决大模型过度思考问题，在保持准确率的同时显著压缩输出长度。"
---
![cover_image](https://mmbiz.qpic.cn/sz_mmbiz_jpg/B2ib2Zr2e3bjQjxLqWgalwWzvMYpiahK60NHjvliaJcvibLYwAp6v6mtPNvqQofBfvLE1vk3hX7d3aqYG1lb5qBP6w/0?wx_fmt=jpeg)

Original TommyYang [Tommy学习录](https://mp.weixin.qq.com/s/) *2025年10月13日 08:58*

关注公众号，获得更及时的大模型前沿知识：

## 论文介绍

> 论文名称：Promoting Efficient Reasoning with Verifiable Stepwise Reward
> 
> 论文地址：https://arxiv.org/pdf/2508.10293v2

![Image](https://mmbiz.qpic.cn/sz_mmbiz_png/B2ib2Zr2e3bjQjxLqWgalwWzvMYpiahK60jCD71OUjbqKH7Ke3az91KJ5704OibA3B64rbh9iaIBNbfYASibmAtoqHA/640?wx_fmt=png&from=appmsg&watermark=1&tp=webp&wxfrom=5&wx_lazy=1#imgIndex=0)

## 摘要

本文针对当前 大型推理模型（LRMs）普遍存在的“过度思考”（Overthinking）问题，即在解决简单问题时消耗过多计算资源、生成冗长且低效的推理过程，提出了一种名为“可验证的逐步奖励机制”（Verifiable Stepwise Reward Mechanism, VSRM）的创新解决方案。 传统强化学习方法通常只在推理的最后一步根据最终结果的正确与否给予一次性奖励，这使得模型倾向于“宁滥勿缺”，通过生成冗长的内容来穷尽可能性，以期获得最终的正确答案。VSRM的核心思想是 **将最终奖励拆解到推理过程的每一步** ，通过一种巧妙的、无需额外训练奖励模型的方式，实时评估每一步推理的有效性。如果一个推理步骤能提升得出正确答案的概率，就给予正向奖励；反之，则给予惩罚。这种细粒度的奖惩机制能够引导模型学习更简洁、高效的推理路径，从根本上抑制无效和冗长的思考步骤。实验结果表明，该方法在不牺牲甚至有时能提升模型推理性能的前提下，显著压缩了输出内容的长度，实现了效率与准确性的最佳平衡，为构建更实用、更高效的大型推理模型提供了重要的思路和技术路径。

## 论文解决了什么问题？——大模型“想太多”的通病

想象一下，你请一位数学博士计算一道小学奥数题。他没有直接给出答案，而是洋洋洒洒写了十几页的论文，从数理逻辑的根基开始推演，虽然最终答案正确，但整个过程耗时耗力，显得“小题大做”。这就是当前很多大型推理模型正在做的事情—— **过度思考** 。

这篇论文首先精准地剖析了这个问题。大型推理模型，尤其是那些在复杂数学、编程等任务上表现出色的模型（如OpenAI的O1，DeepSeek-AI的DeepSeek-R1），其强大的推理能力很大程度上来源于一种叫做“基于可验证奖励的强化学习”（Reinforcement Learning with Verifiable Rewards, RLVR）的训练范式。这种范式的逻辑很简单：模型针对一个问题生成一长串的思考过程（Chain-of-Thought, CoT），然后系统只检查最终答案是否正确。答对了，给个大大的奖励（比如+1）；答错了，就是惩罚（比如-1或0）。

这种“成败论英雄”的奖励方式带来了一个严重的副作用。模型为了确保自己能拿到最终的奖励，会变得极度“谨慎”和“啰嗦”。它会不断地进行自我怀疑、重复验证、探索各种可能的分支，哪怕其中很多步骤对于解决当前问题是完全多余的。正如论文中图2所示的例子，在回答“在\[-500, 500\]区间内，有多少个整数k能让某个方程只有一个实数解”这个问题时，一个模型竟然生成了8000多个token的超长回答。它在“小于0的整数有多少个”这种简单问题上反复纠结，生成了大量“Wait, wait...”（等等，让我想想）之类的无效步骤，最终还给出了错误的答案。

这种“过度思考”的现象，其根源在于 **奖励信号的延迟和稀疏** 。模型在漫长的推理过程中，无法实时知道自己哪一步走对了、哪一步走错了。它就像一个在黑暗中走迷宫的人，只有走到终点才知道此路是否通，因此它只能把所有能走的路都摸索一遍，效率极其低下。

现有的解决方案主要有两种，但都有缺陷：

1. **强行限制输出长度** ：比如给模型一个“token预算”，强制它在一定字数内完成回答。这种方法简单粗暴，但缺乏灵活性。面对真正复杂的问题时，可能会因为“预算”不足而无法完成推理。
2. **难度自适应推理** ：根据问题的预估难度，为模型选择不同的推理模式（比如简单问题少思考，复杂问题多思考）。这种方法看似智能，但本质上还是依赖于对问题难度的准确预判，而这种预判本身就是个难题，且灵活性依然不足。

因此，本文要解决的，正是这个由训练机制本身引发的、导致模型推理效率低下的根本性问题。它不满足于给模型“戴上紧箍咒”，而是希望从根本上教会模型“如何聪明地思考”。

## 论文核心方法与原理是什么？——可验证的逐步奖励机制 (VSRM)

为了根治“过度思考”，作者团队提出了VSRM，其核心理念非常直观： **不再等到秋后算账，而是在推理的每一步都进行“绩效考核”** 。一个推理步骤是有效的，就立刻表扬（奖励）；是无效的，就立刻批评（惩罚）。通过这种即时反馈，引导模型走向最高效的推理路径。

![Image](https://mp.weixin.qq.com/s/www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg%20stroke='none'%20stroke-width='1'%20fill='none'%20fill-rule='evenodd'%20fill-opacity='0'%3E%3Cg%20transform='translate(-249.000000,%20-126.000000)'%20fill='%23FFFFFF'%3E%3Crect%20x='249'%20y='126'%20width='1'%20height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

这个机制的实现可以分解为三个关键步骤：

### 第一步：如何准确地“断句”？——基于规则的步骤拆分 (Step Separation)

要奖励“每一步”，首先得能识别出“一步”的边界在哪里。作者发现，模型在推理时，其生成的文本中天然存在一些标志着思考转折或递进的“特殊词元”（special tokens），例如 “因此 (therefore)”, “所以 (so)”, “但是 (but)”, “等等 (wait)” 等。这些词元就像是人类思考过程中的标点符号，自然地将长篇的思考分割成了一个个独立的逻辑步骤。

VSRM利用这个发现，设计了一套基于规则的自动化步骤拆分算法。具体来说：

1. 它会先在模型生成的一大段思考文本中，通过正则表达式找到这些“特殊词元”。
2. 为了保证拆分出的每一步都是语义完整的，算法还加入了两个约束：一是相邻的切分点之间必须有足够的距离，避免一句话里出现多个特殊词元时被切得过碎；二是切分点必须对齐到一个完整句子的开头。
3. 通过这种方式，一个完整的、冗长的推理轨迹（main rollout）就被清晰地切割成了若干个包含完整语义的子推理轨迹（sub-rollouts）。这为后续的“打分”奠定了基础。

  

![Image](https://mp.weixin.qq.com/s/www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg%20stroke='none'%20stroke-width='1'%20fill='none'%20fill-rule='evenodd'%20fill-opacity='0'%3E%3Cg%20transform='translate(-249.000000,%20-126.000000)'%20fill='%23FFFFFF'%3E%3Crect%20x='249'%20y='126'%20width='1'%20height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

  

### 第二步：如何公正地“打分”？——验证中间步骤的有效性 (Assigning Reward)

这是整个方法最核心、最巧妙的部分。如何评价一个中间步骤的好坏？传统方法可能会去训练一个“奖励模型”（Reward Model, RM）来打分，但训练RM本身成本高且稳定性差。

VSRM另辟蹊径，提出了一种“可验证”的打分方式，完全不需要额外的模型。它的做法是：

1. 对于每一个通过上一步拆分出来的子推理轨迹（比如，从开头到第 `i` 个切分点的所有内容），VSRM会把它和原始问题拼在一起，形成一个新的、更具体的Prompt。
2. 然后，它让模型基于这个新的Prompt，直接生成最终答案。为了结果的稳定性，这个过程会重复多次（例如生成N个候选答案）。
3. 通过计算这N个候选答案的平均准确率，就能得到当前这个子推理轨迹的“状态得分” `A_i` 。

有了每个中间状态的分数，奖励的计算就变得非常简单了。 **第 `i` 步的奖励，就等于第 `i` 个状态的得分 `A_i` 减去第 `i-1` 个状态的得分 `A_{i-1}`** 。

- 如果 `A_i > A_{i-1}` ，说明第 `i` 步的推理让模型离正确答案更近了，奖励为正。
- 如果 `A_i < A_{i-1}` ，说明这一步是“臭棋”，让情况变得更糟，奖励为负（即惩罚）。
- 如果 `A_i = A_{i-1}` ，说明这一步是无效的“原地踏步”，奖励为零。

通过这种方式，VSRM将对最终结果的验证，巧妙地转化为了对每一个中间步骤有效性的验证，从而为整个推理过程提供了密集、实时、且高度可解释的奖励信号。

### 第三步：如何应对“奖励稀疏”？——巧妙的奖励传播机制 (Reward Propagation)

在实际推理中，有时好几步连续的思考都是在为最后那“灵光一现”做铺垫，这些铺垫步骤本身可能并不会立刻提升准确率，导致它们的奖励都是0。如果这种情况太多，奖励信号就会变得“稀疏”，模型的学习效率会降低。

为了解决这个问题，VSRM引入了一种“奖励传播”机制。当某一步的奖励计算出来接近于0时，系统会“向前看”几步（比如最多看4步）。

- 如果在不远处发现了一个奖励显著为正的步骤（一个大的突破），系统就会认为当前的“原地踏步”是为这个突破做准备的必要铺垫。
- 于是，它会将未来那个正向奖励，按照一定的折扣（距离越远，折扣越大），“分发”给当前这一步。
- 这样一来，即便是那些没有立竿见影效果的必要步骤，也能得到正向的激励，鼓励模型进行长远规划和深度思考。
![Image](https://mp.weixin.qq.com/s/www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg%20stroke='none'%20stroke-width='1'%20fill='none'%20fill-rule='evenodd'%20fill-opacity='0'%3E%3Cg%20transform='translate(-249.000000,%20-126.000000)'%20fill='%23FFFFFF'%3E%3Crect%20x='249'%20y='126'%20width='1'%20height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

通过以上三步，VSRM构建了一套完整的、能够实时指导模型进行高效推理的强化学习框架。

## 论文创新价值是什么？

VSRM的创新价值主要体现在以下几个方面：

1. **直击问题本质，理念先进** ：它没有停留在“限制输出长度”这种表层症状的缓解上，而是深入到强化学习的奖励机制层面，通过将“终点奖励”分解为“过程奖励”，从根本上改变了模型的行为动机。它教会模型“为效率而思考”，而不仅仅是“为正确而思考”。
2. **方法巧妙，可解释性与实用性强** ：它最大的亮点在于“可验证”的奖励计算方式。通过让模型自己验证中间步骤的有效性，VSRM完全绕开了训练昂贵且不稳定的奖励模型这一难题，使得整个方案更加轻量、可靠和易于部署。同时，基于规则的步骤拆分和基于准确率变化的奖励计算，都使得整个过程的逻辑非常清晰，具有很强的可解释性。
3. **实现了效率和性能的卓越平衡** ：从论文的实验结果（Table 2）可以看出，无论是用1.5B还是7B的模型，在应用了VSRM之后，模型在AIME、MATH-500等多个主流数学推理基准测试上的输出长度都得到了大幅度的压缩（普遍减少了40%-60%）。更难得的是，在长度显著降低的同时，模型的准确率（pass@1）并没有下降，在很多任务上甚至还有小幅提升。这证明了VSRM真正实现了“去芜存菁”，在剔除无效思考的同时，保留甚至强化了有效的推理能力。
![Image](https://mp.weixin.qq.com/s/www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg%20stroke='none'%20stroke-width='1'%20fill='none'%20fill-rule='evenodd'%20fill-opacity='0'%3E%3Cg%20transform='translate(-249.000000,%20-126.000000)'%20fill='%23FFFFFF'%3E%3Crect%20x='249'%20y='126'%20width='1'%20height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

**4.保护了模型的探索能力** ：一个潜在的担忧是，过度强调简洁会不会扼杀模型的创造力，让它不敢探索更多可能性？论文通过pass@k指标（即生成k个答案中至少有一个正确的概率）的分析（Figure 4）回应了这一点。结果表明，经过VSRM训练的模型，其pass@k的增长曲线与原始模型基本一致甚至更优，这说明VSRM在抑制无效步骤的同时，并没有损害模型探索有价值的、多样化解题路径的能力。

## 总结

《Promoting Efficient Reasoning with Verifiable Stepwise Reward》这篇论文完成了一项非常漂亮的工作。 它精准地识别并剖析了当前大型推理模型中普遍存在的“过度思考”问题，并指出其根源在于传统强化学习范式中延迟、单一的奖励机制。

为此，论文提出了一种创新、优雅且实用的解决方案——VSRM。该方法通过 **基于规则的步骤拆分** 、 **无需额外模型的自我验证式奖励计算** 以及 **巧妙的奖励传播机制** ，成功地为模型的整个推理过程提供了实时、密集的指导信号。 它鼓励模型走出每一个“有效步”，惩罚每一个“无效步”，从而从根本上训练模型养成简洁、高效的推理习惯。

最终，VSRM在实验中取得了令人信服的成果：它在不牺牲甚至有时能提升模型推理准确性的前提下，将模型的输出长度压缩了近一半，实现了效率与性能的完美平衡。这项工作不仅为解决大模型推理效率问题提供了一个即插即用的强大工具，更为未来如何设计更智能、更高效的AI训练机制带来了深刻的启示，是推动大型推理模型从“实验室玩具”走向“现实世界生产力工具”的重要一步。

---

关注公众号，获得更及时的大模型前沿知识：

![](https://mmbiz.qlogo.cn/mmbiz_jpg/icJiceiabX3zCRGzLiallBmqvablOtxGewQZdpbUDicCMvOokFZWQEWCYaYCB51TbYDL2IzlibVUqicTicxtNI6OgBzqgA/0?wx_fmt=jpeg)

您的鼓励是我坚持的动力

继续滑动看下一个

Tommy学习录

向上滑动看下一个