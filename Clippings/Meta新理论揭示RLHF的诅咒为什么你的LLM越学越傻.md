---
title: "Meta新理论揭示RLHF的诅咒：为什么你的LLM越学越傻？"
source: "https://mp.weixin.qq.com/s/uDPHFaJRf35G4Capf_nWow"
author:
  - "[[Tensorlong 看天下]]"
published:
created: 2025-09-16
description: "❝一句话概括，本文戳破了“强化学习提升LLM推理能力”的美好泡沫，指出其背后隐藏着多样性灾难，并开创性地提出一"
tags:
  - "强化学习"
  - "多样性崩塌"
  - "基于结果的探索"
abstract: "Meta提出新理论揭示RLHF训练会导致LLM多样性崩塌，并开创性地提出基于结果的探索策略来解决这一问题。"
---
Original Tensorlong 看天下 *2025年09月16日 14:20*

![Image](https://mmbiz.qpic.cn/sz_mmbiz_png/Z24DyenWDNiaJZ6Qp7TcrSkOuia5zia6vHfYLWXVicSU44vuWibkJ0DuDloRYDFe6RDmCx2R9bQy98ylic28OzAQwQvg/640?wx_fmt=png&from=appmsg&watermark=1&tp=webp&wxfrom=5&wx_lazy=1#imgIndex=0)

> ❝
> 
> 一句话概括，本文戳破了“强化学习提升LLM推理能力”的美好泡沫，指出其背后隐藏着多样性灾难，并开创性地提出一种“基于结果的探索”策略，通过奖励“冷门答案”来让模型实现真正的智能涌现。（原论文题目见文末，点击阅读原文可直接跳转至原文链接， Published on arxiv on 08 Sep 2025, by FAIR at Meta）

### 第一阶段：识别核心概念

#### 论文的 motivation 分析

这篇论文的出发点是当前 LLM 领域一个非常普遍但棘手的问题。研究者们发现，使用强化学习（RL）来训练 LLM 进行数学推理等任务，虽然能显著提高模型的“单次作答”准确率，但却带来了一个严重的副作用： **多样性崩塌（Diversity Collapse）** 。

想象一下，一个学生为了考试，只反复练习一道题的标准解法，最终他能完美地背诵出这个解法，但对于这道题的其他可能解法，或者稍微变化一下的题目，他就束手无-策了。LLM 也是如此，RL 训练让它过度“自信”地集中于少数几个它认为正确的推理路径上，扼杀了探索其他潜在正确路径的可能性。

这种多样性的缺失在实际应用中是致命的。例如，在“多数投票”或“多路径推理”这类需要模型提供多个不同答案以供筛选的场景（学术上称为 `pass@k` ，即生成 k 个答案中只要有1个正确就算对），多样性差的模型表现会急剧下降。因此，本文的动机就是： **如何在通过强化学习提升 LLM 推理准确性的同时，避免多样性的损失，甚至提升多样性？**

#### 论文主要贡献点分析

- **列出论文声称的主要创新点** ：
1. **深入诊断多样性崩塌** ：论文首次将 RL 训练过程视为一种采样过程，并揭示了一个关键现象—— **多样性退化的转移（Transfer of Diversity Degradation）** 。即模型在已解决问题上丧失的多样性，会“传染”到尚未解决的问题上，导致整体探索能力下降。
	2. **提出“基于结果的探索”（Outcome-based Exploration）** ：鉴于推理过程（即思维链）的可能性是无限的，而最终答案（结果）的可能性是有限且可处理的，论文提出将探索的重点从复杂的“过程”转移到简单的“结果”上。
	3. **设计两种互补的探索算法** ： **历史探索（Historical Exploration）** ——借鉴了经典的 UCB（Upper Confidence Bound）思想，对历史上很少出现的答案给予探索奖励，鼓励模型"温故而知新"，发掘被遗忘的角落； **批次探索（Batch Exploration）** ——为了提升测试时的即时多样性，该算法在同一个批次（batch）内惩罚重复的答案，鼓励模型在一次生成中就给出多种不同的候选答案。
	4. **提供理论支撑** ：通过一个新的"基于结果的赌博机"（Outcome-based Bandits）模型，从理论上证明了"基于结果的探索"策略的有效性。
- **找出支撑这些创新的关键技术或方法** ：
- 关键技术是对经典强化学习探索算法 **UCB (Upper Confidence Bound)** 的巧妙改造和应用。论文不仅仅是简单地应用 UCB，而是设计了带有基线的变体（如 `UCB-Con` ），使得探索信号有正有负，从而更精细地平衡“探索”与“利用”。
	- 核心方法是将探索空间从 **高维、连续的推理文本空间** 映射到了 **低维、离散的最终答案空间** ，极大地降低了探索的难度。
- **论文的显著性结果** ：
- 最显著的结果是，论文提出的方法成功地在提升模型推理准确率（ `pass@1` ）的同时，也显著提升了多样性（ `pass@k` for k > 1），实现了“鱼与熊掌兼得”的效果。
	- 实验还揭示了传统 RL 方法可能存在的 **“过度优化”** 问题，即训练后期性能反而下降，而加入探索机制后，模型的性能更加稳定。
	- 论文通过实验清晰地证明了“多样性退化转移”现象的存在，为理解 RL 训练 LLM 的内在机制提供了新的视角。

#### 理解难点识别

- **分析哪些概念/方法是理解论文的关键** ：
1. **多样性崩塌** ：需要理解它为什么会发生，以及为什么它是一个严重的问题。
	2. **结果空间的可处理性（Tractability of the Outcome Space）** ：这是论文整个方法论的基石，必须理解为什么作者强调这一点。
	3. **历史探索 vs. 批次探索** ：这两种探索方式的目标和机制有细微但重要的差别，理解它们的区别是理解论文算法设计的关键。
	4. **带基线的 UCB 探索（UCB-Con）** ：为什么简单的 UCB 不够好？引入基线（baseline）的作用是什么？这是理解算法细节的核心。
- **找出这些概念中最具挑战性的部分** ：
- 最具挑战性的部分在于理解 **从传统的、基于状态-动作的探索，到“基于结果的探索”这一思维范式的转变** 。传统的 RL 探索是在每一步决策时进行的，而这里是将整个复杂的生成过程视为一个“黑箱”，只根据最终的输出结果来进行探索奖励。
	- 同时，理解 **UCB-Con** 中引入“负探索奖励”背后的直觉也颇具挑战性。它意味着模型不仅要奖励新奇的尝试，还要主动惩罚那些“虽然安全但不够好”的重复性尝试。
- **确定需要重点解释的核心概念** ：
- 基于以上分析，最需要深入解释的核心概念是 **“基于结果的探索”及其核心算法实现——带基线的历史探索（UCB-Con）** 。这个概念是全文的枢纽，连接了问题（多样性崩塌）和解决方案（具体的探索算法）。

#### 概念依赖关系

1. **切入点** ：从 **多样性崩塌** 问题开始，这是所有工作的起点。
2. **基石** ：解释为什么能解决这个问题，引出 **结果空间的可处理性** 这一关键洞察。
3. **核心思想** ：基于上述洞察，正式提出 **“基于结果的探索”** 这一核心思想。
4. **具体实现** ：详细拆解其主要实现方式 **历史探索（UCB-Con）** ，并与 **批次探索** 进行对比，阐明它们各自的侧重点。

### 第二阶段：深入解释核心概念

#### 设计生活化比喻：寻宝探险队

想象一下，一位经验丰富的探险队长，带领一支队伍去一个神秘岛屿寻宝。

- **岛屿（问题）** ：就是一道复杂的数学题。
- **宝藏（正确答案）** ：这道题的唯一正确答案。
- **探险队员（LLM）** ：语言模型。
- **探险路线（推理过程）** ：队员们从登陆点出发，穿越丛林、沼泽、山谷，最终到达某个地点的完整路线。这对应 LLM 生成的“思维链”。
- **最终挖到的东西（最终结果/Outcome）** ：队员们在路线终点挖出来的东西。可能是 **真正的宝藏** （奖励=1），也可能只是 **一块普通的石头** （奖励=0），甚至是 **一个空箱子** （另一个奖励=0的错误答案）。

岛上的 **探险路线** 有成千上万条，但 **最终能挖到的东西种类** 却非常有限，无非就是“宝藏”、“石头”、“空箱子”这几种。这就是 **“结果空间的可处理性”** 。

**传统 RL 的困境（多样性崩塌）** ： 第一天，一个队员碰巧走出了一条路线，挖到了宝藏。作为队长，非常高兴，于是下令：“所有人，明天都必须严格按照这条路线走！” 结果，第二天所有队员都轻松地挖到了宝藏。但第三天、第四天……所有人都只会走这一条路。当天气变化（题目微调）导致这条路被淹没时，队伍就再也找不到宝藏了，因为他们已经忘记了如何探索新的路线。队伍的整体探险能力（多样性）严重退化了。

**本文的“基于结果的探索”策略** ： 一位更聪明的队长，不再只奖励找到宝藏的行为，而是引入了一套 **“新发现奖励”** 机制。

- **核心规则** ：拿出地图，在每个队员挖到东西的地方做一个标记。奖励那些在 **很少有人去过的地方** 挖到东西的队员，哪怕他们只挖到了一块奇特的石头。
- **这套机制就是“历史探索（UCB）”** ：一个地点被探索的次数越少，它的“新发现”潜力就越大，探索它就能获得越高的奖励。

#### 建立比喻与实际技术的对应关系

| 比喻中的元素 | 实际技术概念 | 解释 |
| --- | --- | --- |
| 探险队长 | 强化学习训练算法 | 负责制定奖励规则，更新队员（模型）的行为策略。 |
| 探险队员 | LLM Policy () | 负责根据当前策略生成探险路线。 |
| 探险路线 | 推理过程/思维链 () | 模型为了得到答案而生成的完整文本。 |
| 最终挖到的东西 | 最终答案/结果 () | 模型从推理过程中提取出的最终答案。 |
| 东西的种类 (宝藏/石头) | 结果空间 (Outcome Space) | 所有可能的不同答案的集合，数量有限。 |
| 挖到宝藏 | 正确答案 () | 获得环境给予的“正确性奖励”。 |
| 在地图上做标记 | 记录答案的历史访问次数 | 统计每个答案 `a` 对问题 `x` 已经被生成过多少次。 |
| “新发现奖励” | 探索奖励 (Exploration Bonus) | 除了正确性奖励外，额外给予的用于鼓励多样性的奖励。 |

#### 深入技术细节

从比喻回到论文的技术实现。论文提出的核心算法 `UCB-Con` 的奖励机制，可以用一个公式来精确描述。在传统的 RL 目标之上，它增加了一项探索奖励。

**（1）朴素 UCB 探索**

首先是一个基础版的 UCB 探索奖励。它的目标函数（简化后）大致是这样：

**原始数学形式**: $ \\text{Total Reward} = A(x, {y\_i, a\_i}) *i + c \\cdot b* {\\text{ucb}}(x, a\_i) b\_{\\text{ucb}}(x, a) = \\min{1, \\frac{1}{\\sqrt{N(x, a)}}} $

**符号替换版本**:

```
总奖励 = (这次尝试的优势值) + (好奇心系数) * (本次尝试的“新颖度”奖励)
```

其中：

```
新颖度奖励 = 最小(1, 1 / 开根号(对于当前问题x, 答案a在历史上出现的总次数))
```
- **解释** ： `A(...)` 代表答案本身的质量（是否正确）。 `N(x, a)` 是“地图上的标记”，记录答案 `a` 出现过多少次。这个公式完美体现了“越少见，越奖励”的原则：
- 如果一个答案 `a` 从未出现过（N 接近 0）， 会非常大，探索奖励就很高。
	- 如果一个答案 `a` 已经出现过成百上千次（N 很大）， 就趋近于 0，几乎没有探索奖励。

**（2）带基线的 UCB 探索 (UCB-Con)**

朴素 UCB 存在一个问题：它只给予“正奖励”。这可能导致模型为了拿到探索奖励，去探索一些明显没有价值的错误答案。聪明的探险队长（UCB-Con 算法）更进一步，他设定了一个 **“探索期待值”** 。

**原始数学形式**: 探索奖励项被修改为： $ B(x, {y\_i, a\_i}) *i = b* {\\text{ucb}}(x, a\_i) - b\_0 $

**符号替换版本**:

```
净探索得分 = (本次尝试的“新颖度”奖励) - (探索的“及格线”)
```
- **解释** ： 是队长设定的一个“及格线”（例如， ）。
- **获得正奖励** ：如果一个答案 `a` 的历史出现次数 少于 4 次，那么它的“新颖度奖励” 就会大于 0.5，最终的“净探索得分”为正。队长会说：“干得不错，这个地方很有潜力，继续挖！”
	- **获得负奖励（惩罚）** ：如果一个答案 `a` 已经出现超过 4 次，它的“新颖度奖励”就会小于 0.5，“净探索得分”为负。队长会说：“这个地方挖过太多次了，别再浪费时间了，去别的地方看看！”

这种有正有负的精细化奖励机制，使得模型在探索时更有“方向感”，它会优先探索那些 **真正新颖** 的答案，同时主动 **避免** 在那些 **已经被充分探索过但没有结果** 的错误答案上兜圈子。

#### 将技术细节与比喻相互映射

- **公式** 在比喻中就是队长评估一个地点“新颖度”的计算方法。地图上标记越少（N 越小），新颖度越高。
- **常数** 在比喻中是队长内心的“探索及格线”。他认为一个地点被挖了 4 次以上就失去了探索价值。
- **正的 `净探索得分`** 对应队长对队员的口头表扬和额外物资奖励，鼓励他们继续探索这个方向。
- **负的 `净探索得分`** 对应队长的批评和资源削减，阻止他们继续在没有希望的地方浪费精力。

**比喻的局限性** ：这个比喻很好地解释了历史探索。对于 **批次探索（Batch Exploration）** ，可以想象成队长在每天任务结束时开总结会，如果发现今天好几个小队都报告了同一个新地点，他会说：“虽然这个地点是新的，但今天去的人太多了，明天别扎堆去！” 它更关注 **即时的、小范围内的多样性** 。

#### 总结

通过“寻宝探险队”的比喻，可以清晰地理解“基于结果的探索”的核心思想。它不再纠结于队员们错综复杂的“探险路线”（推理过程），而是聚焦于他们最终挖到的“东西”（答案结果）。通过一个类似地图标记（ `N(x,a)` ）和设立探索及格线（ `b_0` ）的精细化奖惩系统（UCB-Con 公式），算法能像一位智慧的探险队长一样，高效地引导模型在广阔的未知世界中进行探索，最终在找到宝藏（准确率）的同时，也绘制出一幅内容丰富的探险地图（多样性）。

### 第三阶段：详细说明流程步骤

#### 完整流程步骤

**输入** ：

- 一批训练问题 `X_train` 。
- 当前的 LLM 策略 。
- 一个用于记录历史答案计数的哈希表（或字典） `N` ，初始为空。
- 强化学习算法超参数，包括好奇心系数 `c` 和探索及格线 。
1. **批次数据采样** ： 从训练集 `X_train` 中随机抽取一个问题 `x` 。
2. **并行生成（Rollout Phase）** ： 让当前的 LLM 针对问题 `x` ，并行生成 `n` 个独立的“推理-答案”对。这一步就像是派出了 `n` 个探险队员，让他们同时从起点出发，各自寻找路线。
- **输出** ：一个包含 `n` 个样本的批次数据： `Batch = {(y_1, a_1), (y_2, a_2), ..., (y_n, a_n)}` 。其中 是第 `i` 个队员的完整探险路线（思维链）， 是他最终挖到的东西（最终答案）。
4. **奖励与信息计算** ： 对于批次中的 **每一个** 样本 `i` （从 1 到 `n` ），需要计算它的综合得分。
- **(3a) 计算基础奖励** ：将答案 与标准答案进行比对，得到基础的正确性奖励 （正确为 1，错误为 0）。
	- **(3b) 获取历史信息** ：查询历史计数表 `N` ，找到答案 针对问题 `x` 已经出现过的次数 。如果 是第一次出现，则 为 0（在计算时通常会加一个平滑项，比如当成 1）。
	- **(3c) 计算探索奖励（UCB-Con）** ：这一步包含三个关键子步骤—— **新颖度奖励计算** ：根据历史次数计算"新颖度奖励"： ； **净探索得分计算** ：计算"净探索得分"： ； **特殊处理机制** ：论文中提到一个重要细节，如果一个批次中所有的基础奖励 都是 1（即所有队员都找到了宝藏），那么所有探索奖励 都会被置为 0，这是为了防止在已经完美解决问题的情况下，因为探索奖励为负而"惩罚"正确的答案。
6. **构建优化目标** ：
- **(4a) 计算优势函数（Advantage）** ：对于每个样本 `i` ，使用类似 GRPO 或 PPO 的算法，计算其优势值 。优势值衡量的是，当前样本获得的基础奖励 比该策略在该问题下的平均表现要好多少。这是一个标准的强化学习步骤。
	- **(4b) 组合成最终信号** ：将优势值和探索奖励结合起来，形成最终用于梯度更新的信号 。 这里的 `c` 是“好奇心系数”，用来调节探索奖励在总信号中的权重。
8. **模型参数更新** ：
- 使用 `TotalSignal` 作为奖励信号，通过 PPO 等强化学习算法计算策略损失（Policy Loss），并进行反向传播，更新 LLM 的网络参数。
	- 这个更新过程会使得模型倾向于生成那些能够获得更高 `TotalSignal` 的“推理-答案”对。也就是说，模型不仅会学习如何得到正确答案（ 高），还会学习去尝试那些新颖的、未被充分探索的答案（ 高）。
10. **更新历史记录** ：
- 训练步骤完成后，必须更新“地图”。对于本批次中生成的所有 `n` 个答案 ，更新它们在历史计数表 `N` 中的计数值。 (对于 `i` 从 1 到 `n`)。
12. **循环** ： 回到步骤 1，抽取下一个问题，重复整个流程，直到训练结束。

通过这个流程，模型在每一次迭代中都会接收到来自“正确性”和“新颖性”两方面的反馈，从而在不断提升解题能力的同时，也保持和发展了其生成答案的多样性。

### 第四阶段：实验设计与验证分析

#### 1\. 主实验设计解读：核心论点的验证

- **核心主张** ： 论文的核心主张是，其提出的探索方法（ `UCB-Con` 和 `Batch` ）能够在不牺牲甚至提升单次回答准确率（ `pass@1` ）的前提下，显著缓解强化学习（RL）训练导致的多样性崩塌问题，从而提高多样本下的综合性能（ `pass@k` for ）。
- **实验设计分析** ：
- **数据集选择** ：包含训练集和测试集两个维度—— **训练集** 采用了 `MATH` (easy) 和 `DAPO` (medium) 数据集， `MATH` 是数学推理领域的经典基准，而 `DAPO` 是一个更大规模、难度更高的数据集，这个选择覆盖了不同难度，能够检验方法的泛化性和有效性； **测试集** 使用了 `MATH-500` 、 `AIME2024/2025` 和 `AMC23` 等竞赛级别的数学题，这些都是公认的高难度、标准化的测试基准，用它们来评估性能极具说服力。
	- **评价指标设计** ：\*\* `pass@k` \*\* 是本文最重要的评价指标。 `pass@1` 直接衡量模型的 **准确性** ，而 `pass@k` （特别是当 `k` 较大，如 16, 32）则同时衡量了 **准确性与多样性** 。如果模型只会生成一种正确的答案，那么它的 `pass@1` 和 `pass@32` 将会非常接近；而如果模型能生成多种不同的正确答案， `pass@32` 将会远高于 `pass@1` 。这个指标完美地契合了论文要解决的核心问题。
	- **基线方法选择** ： **Vanilla RL (GRPO)** 是论文最核心的比较对象。GRPO 是一种不带任何显式探索机制的、基于结果的 RL 微调方法。通过与它对比，可以直接证明"探索"机制带来的增益。选择这个基线是极其合理的，因为它隔离了变量，使得任何性能提升都可以归因于新提出的探索策略。
- **实验结果与结论** ：
- **Figure 1** 是主实验结果的浓缩展示。从图中可以清晰地看到：
	- **结论** ：主实验有力地证明了，基于结果的探索不仅解决了多样性崩塌的问题，还带来了实质性的性能提升，并提高了训练的稳定性。
1. **全面超越** ：在 Llama 和 Qwen 两个模型以及不同难度的数据集上， `UCB-Con` 和 `Batch` 方法的 `pass@k` 曲线几乎在所有 `k` 值上都高于 `Vanilla RL` 。这直接证明了新方法的有效性。
	2. **缓解过拟合** ：特别是在左图 Llama 的实验中， `Vanilla RL` 的后期检查点（last checkpoint，虚线）性能甚至比早期检查点（early checkpoint，实线）还要差，这表明了严重的 **过度优化/过拟合** 问题。而 `UCB-Con` 和 `Batch` 的后期性能依然稳健，说明探索机制有助于防止模型陷入局部最优，增强了泛化能力。

#### 2\. 消融实验分析：内部组件的贡献

论文虽然没有设置标题为“消融研究”的独立章节，但 **Figure 4** 的内容实质上扮演了这一角色，它深入比较了不同 UCB 变体的效果。

- **被“消融”/比较的关键模块** ： 这里比较的是探索奖励的设计方式，从简单到复杂，层层递进：
1. **Vanilla RL** ：无探索奖励。
	2. \*\*UCB (Naive UCB)\*\*：只提供正向的、基于历史计数的探索奖励。
	3. **UCB-Mean** ：引入批次内均值作为基线。
	4. **UCB-Con** ：引入一个可调的常数基线 ，从而实现有正有负的探索信号。
- **实验结果与证明** ：
- 从 **Figure 4** 中可以看出一个清晰的性能梯度： `UCB-Con` \> `UCB-Mean` \> `UCB` ≈ `Vanilla RL` 。
	- 这一结果定量地证明了： **简单 UCB 还不够** ——仅仅添加一个正向的探索奖励 (`UCB`) 并不足以在测试集上带来稳定的性能提升，这说明简单的"鼓励新奇"可能会让模型陷入对无意义答案的探索； **基线是关键** ——引入基线（无论是 `UCB-Mean` 还是 `UCB-Con` ）后，性能有了显著提升； **负反馈的重要性** —— `UCB-Con` 作为表现最好的方法，其核心优势在于能够产生负探索信号，即主动"惩罚"对那些已被探索过但价值不高的答案的重复生成，这证明了"有奖有罚"的探索机制对于泛化性能至关重要。

#### 3\. 深度/创新性实验剖析：洞察方法的内在特性

作者设计了几个非常巧妙的实验，来提供超越性能数字的深刻洞见。

- **巧妙实验一：训练动态可视化 (Figure 2)**
- **实验目的** ：直观地验证论文提出的核心洞察—— **“多样性退化的转移”** 现象。
	- **实验设计** ：这个实验的巧妙之处在于，作者没有看整体的答案多样性，而是将问题分为“已解决”和“未解决”两类，并分别追踪在训练过程中，模型在 **“未解决”问题** 上生成的不同答案的数量（ `diff@k` ）。他们将 RL 训练过程（x轴为训练步数）与简单地从基础模型采样（x轴为等效的采样数量 `k` ）进行对比。
	- **实验结论** ： **Figure 2** 底部图中的虚线是这个实验的“点睛之笔”。结果显示，随着 RL 训练的进行，模型在那些它 **根本还没能正确解答的问题上** ，其生成答案的多样性（橙色虚线）也显著低于基础模型（蓝色虚线）。这强有力地证明了，模型在解决了某些问题后形成的“思维定势”会“传染”到其他问题上，从而抑制了对这些难题的探索。这个实验为论文的动机提供了坚实的、可视化的证据。
- **巧妙实验二：历史探索 vs. 批次探索的机理分析 (Section 4.1, Tables 1 & 2)**
- **实验目的** ：深入剖析 `UCB-Con` (历史探索) 和 `Batch` (批次探索) 这两种方法在行为上的根本区别。
	- **实验设计** ：作者在训练的某个中间节点，从两个维度对不同方法生成的文本进行了"快照"分析：\*\*生成熵 (Table 1) **——测量生成文本的token级别熵，熵越高，代表文本越不可预测、越随机；** 批次内多样性 (Table 2)\*\*——直接统计在一个批次内生成了多少个不同的答案。
	- **实验结论** ：结果显示了三个关键发现—— **生成随机性差异** ： `Batch` 方法生成的文本熵显著更高，尤其是在错误答案上（ `0.153` vs `0.103` ），说明它在生成层面就更"天马行空"； **批次内多样性提升** ： `Batch` 方法在批次内能生成更多不同的答案（ `3.230` vs `2.926` ），尤其是在未解决问题上，探索性更强； **方法哲学区别** ：这个实验揭示了两种方法的不同哲学—— `UCB-Con` 像一个历史学家，着眼于长期的、全局的探索覆盖；而 `Batch` 更像一个头脑风暴的主持人，追求每一次讨论（批次）都能激发出最多的新点子。这清晰地解释了为什么 `Batch` 方法在需要即时多样性的 `pass@k` (大 `k`) 指标上表现优异。

---

本文题目：Outcome-based Exploration for LLM Reasoning

**欢迎Deep Learning同好与我交流、讨论、合作！**

**现已提供论文解读和idea讨论定制服务，可私信后台联系**

**公众号广告位招租，欢迎咨询👏**

  

个人观点，仅供参考

[Read more](https://mp.weixin.qq.com/s/)

继续滑动看下一个

沈公子今天读什么

向上滑动看下一个