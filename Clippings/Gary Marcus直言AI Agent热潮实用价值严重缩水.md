---
title: "Gary Marcus直言：AI Agent热潮，实用价值严重缩水"
source: "https://mp.weixin.qq.com/s/C3O73OPimidvOSsc4PyanA"
author:
  - "[[Gary Marcus]]"
published:
created: 2025-09-23
description: "拥抱确定性。"
tags:
  - "AI代理"
  - "炒作泡沫"
  - "可靠性不足"
  - "技术缺陷"
abstract: "Gary Marcus认为当前AI代理技术存在严重可靠性问题，过度炒作但实用价值有限，需要神经符号AI等更稳健的方法才能实现真正突破。"
---
![cover_image](https://mmbiz.qpic.cn/mmbiz_jpg/vECHJ8kDD4jthVicEAI9ibqS0cvqqChfickYhvGpYtho9SEOMKTX7dBlS3wUezMicicZfMCDfibFrOSs0vXJwt0yM7oA/0?wx_fmt=jpeg)

Gary Marcus [简写2019](https://mp.weixin.qq.com/s/) *2025年09月22日 18:12*

### 作者 Gary Marcus，翻译自 AI Agents have, so far, mostly been a dud。

Gary Marcus曾是纽约大学心理学系的教授，主要研究人类语言、认知发展和人工智能。他创办过多家 AI 初创公司，比如 Geometric Intelligence后来被 Uber 收购。

  

Marcus 在 AI 圈子里以“批判派”著称，长期强调深度学习的局限性，主张结合符号推理与神经网络的“混合方法”。

  

  

  

  

### 到目前为止，AI代理基本上是个失败品

去年，科技巨头们喋喋不休地谈论着AI“代理”将成为2025年的下一件大事。但事实并非如此。

  

如果说聊天机器人是回答你的查询，那么AI代理则应该代表你做事，主动打理你的生活和工作。它们可能会为你购物、预订旅行、安排日程、以自定义方式为你总结新闻、跟踪你的财务状况、维护数据库乃至整个软件系统，等等。最终，Agent代理应该能够完成任何你可能会请人类为你做的认知性工作。

  

到2024年底，谷歌、OpenAI、Anthropic等许多公司都宣布他们的首批代理即将问世。今年一月，Anthropic的达里奥·阿莫代（Dario Amodei）告诉《金融时报》：“我认为2025年将是AI能够完成博士生或某一领域初级专业人士所能完成工作的年份。”

  

几乎同时，OpenAI的首席执行官山姆·奥尔特曼（Sam Altman）写道：“我们相信，在2025年，我们可能会看到首批AI代理‘加入劳动力大军’，并实质性地改变公司的产出。”谷歌则更为低调地宣布了Project Astra项目，称其为“构建通用AI助手之路上的一个项目”。

  

《纽约时报》的科技专栏作家凯文·罗斯（Kevin Roose）总是热衷于为这个行业摇旗呐喊，他当时写道：

![Image](https://mmbiz.qpic.cn/mmbiz_jpg/vECHJ8kDD4jthVicEAI9ibqS0cvqqChfickvy7tcydN56n2yFTVFUcaah1kRicwskibGo6tXPrgRZHIfbO68CcpMAKw/640?wx_fmt=jpeg&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1#imgIndex=0)

截图翻译：AI 代理正在革新各行各业，自动化任务，并提供曾经难以想象的洞察。它们不仅仅是未来；它们已经是当下，正在重塑我们的工作和生活方式。从通过聊天机器人提升客户服务，到简化数据分析，AI 代理正成为企业不可或缺的工具。

在医疗领域，AI 代理帮助诊断疾病并个性化治疗方案。在金融领域，它们被用于欺诈检测和算法交易。甚至在创意领域，AI 也在生成艺术和音乐，推动着可能性的边界。

  

好吧，也许吧。但它们真的管用吗？

我本人在2025年1月1日做出的更悲观的预测是：“‘AI代理’将在整个2025年被无休止地炒作，但除了在非常狭窄的用例中，它们远非可靠。”

如今还剩五个月，情况如何呢？

  

  

  

  

  

事实上，所有大公司都推出了Agent。这一点没错。

但是，据我所知，除了可能在非常狭窄的用途中，没有一个代理是可靠的。Astra仍处于测试版（或可能是Alpha版），仅通过“可信测试者的等候名单”提供。OpenAI实际上发布了他们的ChatGPT代理\[颇为不寻常的是，其名称中的“agent”一词并未大写\]。

ChatGPT代理听起来几乎完全符合人们对代理的期望——但它附带了许多警告。

> ChatGPT现在可以思考和行动，主动从一套代理技能工具箱中进行选择，使用它自己的计算机为你完成任务……
> 
>   
> 
> 例如，它可以通过API收集你的日历信息，使用基于文本的浏览器高效地处理大量文本，同时还能与主要为人类设计的网站进行视觉交互。
> 
>   
> 
> 所有这些都是通过它自己的虚拟计算机完成的，即使使用了多种工具，也能保留任务所需的上下文——模型可以选择使用文本浏览器或视觉浏览器打开页面，从网络下载文件，通过在终端中运行命令来操作文件，然后在视觉浏览器中查看输出。该模型会调整其方法，以实现速度、准确性和效率。
> 
>   

但同时：

> ChatGPT代理仍处于早期阶段。它能够承担一系列复杂的任务，但仍会犯错……这带来了新的风险，特别是因为ChatGPT代理可以直接处理你的数据，无论是通过连接器访问的信息，还是你通过“接管模式”登录的网站信息。

  

在实践中，这些错误和失误极大地限制了ChatGPT代理等代理的实用性。罗斯向我们承诺的“突破可能性的边界”？基本上不存在。

  

  

甚至到了三月底，人们就开始意识到现实与炒作并不相符，正如《财富》杂志一篇关于当时风靡一时（如今已被遗忘）的Manus系统的报道所言，一些AI代理客户表示“现实与炒作不符”。在过去的几个月里，这种情绪只增不减。

最近几周，出现了多篇类似的报道：

![Image](https://mmbiz.qpic.cn/mmbiz_jpg/vECHJ8kDD4jthVicEAI9ibqS0cvqqChfickKKOIA2anQ8BMbNiaWRnWdukiaB6rUckfVtUmpRXX372DjnXDdv1ZMhcA/640?wx_fmt=jpeg&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1#imgIndex=1)

截图翻译：AI删除了数据库

  

AI编程代理似乎也在累积大量的技术债务，编写出大量难以调试的“复制粘贴”式代码。正如麻省理工学院教授阿曼多·索拉-莱萨马（Armando Solar-Lezama）告诉《华尔街日报》的那样，AI就像一张“全新的信用卡，它将使我们能够以前所未有的方式累积技术债务。”或者套用一句可以追溯到1969年的老话：“人非圣贤，孰能无过；要搞砸大事，还得靠AI代理。”

  

  

Penrose.com创建了一个用于基本账户余额跟踪的基准测试，使用了来自Stripe等地的一整年实际数据——并发现了一个我怀疑会很典型的结果：AI错误往往会随着时间的推移而累积。（公平地说，Penrose进行测试时ChatGPT代理尚未推出，但我很惊讶他们的结果会有天壤之别。）

![Image](https://mp.weixin.qq.com/s/www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg%20stroke='none'%20stroke-width='1'%20fill='none'%20fill-rule='evenodd'%20fill-opacity='0'%3E%3Cg%20transform='translate(-249.000000,%20-126.000000)'%20fill='%23FFFFFF'%3E%3Crect%20x='249'%20y='126'%20width='1'%20height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

  

  

  

  

  

挥之不去的“幻觉”问题频频抬头：

![Image](https://mp.weixin.qq.com/s/www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg%20stroke='none'%20stroke-width='1'%20fill='none'%20fill-rule='evenodd'%20fill-opacity='0'%3E%3Cg%20transform='translate(-249.000000,%20-126.000000)'%20fill='%23FFFFFF'%3E%3Crect%20x='249'%20y='126'%20width='1'%20height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

  

  

  

  

一位在AI代理行业工作的人在Reddit上发布了以下评论；他们提到的“演示与现实脱节”问题，正是我们未来几年可能会持续看到的情况：

![Image](https://mp.weixin.qq.com/s/www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg%20stroke='none'%20stroke-width='1'%20fill='none'%20fill-rule='evenodd'%20fill-opacity='0'%3E%3Cg%20transform='translate(-249.000000,%20-126.000000)'%20fill='%23FFFFFF'%3E%3Crect%20x='249'%20y='126'%20width='1'%20height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

截图翻译提炼：

**“Agent”标签已经失去意义**

- 市场上大多数所谓的 AI Agent 其实只是工作流（workflow），只是套了点 GPT 调用，看似“智能”。
- 这并不是自主决策，过度营销导致“Agent”一词变得空洞。

**演示与现实差距巨大**

- 在大会和 Twitter 上的炫酷 demo 并不代表真实世界表现。
- 系统非常脆弱，一个小小的用户输入或一次幻觉就可能让客户完全失去信任。
- 现实中仍然存在大量可靠性和稳定性问题，却鲜少有人愿意公开讨论。

**行业叙事随场合而变**

- 对投资人说 AI Agent 将取代知识工作者，迎来生产力新时代；
- 对监管者说 AI Agent 只是帮助处理表格的小工具。
- 这种话术反复横跳，既让客户困惑，也阻碍了外界对系统真正能力的认知。

**内部从业者行为与宣传不符**

- 许多顶尖 AI 研究人员并没有全力推进所谓“改变世界”的使命，而是频繁跳槽，只为更高薪水或更好期权。
- 如果真的相信 18 个月后能颠覆世界，就不会为了 20% 的加薪而换工作。

**解决的是不存在的问题**

- 大量 VC 投资流入一些所谓“革命性”的自动化场景，但这些场景并非企业真正的痛点。
- 真正成功的 Agent 项目往往只是一些细碎、琐碎的自动化任务，而不是宏大叙事里的“未来工作革命”。

  

  

  

  

  

  

今年七月，《未来主义》（Futurism）进行了一项分析，报道称“AI代理目前任务失败的百分比可能给该行业带来麻烦”，并补充道“失败率令人痛苦”，该分析引用了卡内基梅隆大学的AgentCompany基准测试，显示某些任务的失败率高达70%。

  

  

  

  

  

  

X平台上最热情、通常也最乐观的网红之一几天前无奈地发帖问道：“严肃提问：你们中有谁在使用ChatGPT代理？如果有的话，是用来做什么的？我就是找不到一个与其（有限的）能力相匹配的用例。”而本周二，《财富》杂志的杰里米·卡恩（Jeremy Kahn）——他通常对AI持乐观态度——也写道：

![Image](https://mp.weixin.qq.com/s/www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg%20stroke='none'%20stroke-width='1'%20fill='none'%20fill-rule='evenodd'%20fill-opacity='0'%3E%3Cg%20transform='translate(-249.000000,%20-126.000000)'%20fill='%23FFFFFF'%3E%3Crect%20x='249'%20y='126'%20width='1'%20height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

截图翻译：

  

**人人都想要 AI Agent，但真正使用的人很少。**

去年大家都在期待 AI Agent；如今 OpenAI、Google、Anthropic 等公司都推出了相关产品。然而实际采用率依然远远落后于满天飞的炒作。为什么？

  

从本质上讲，Agent 的风险比大多数预测型 AI 或仅仅生成内容的生成式 AI 要高得多。而现在，AI Agent 往往并不那么可靠。提升其可靠性的方式——比如使用多个 Agent，每个被分配到工作流中的特定任务，再让一些 Agent 去检查其他 Agent 的工作——也同样代价高昂。

  

由于对所处理任务的理解流于表面，当前的代理极其容易受到网络攻击。

正如卡内基梅隆大学博士生安迪·邹（Andy Zou）最近在一项大型多团队合作研究中报告的那样：

即使是最安全的系统，也有1.45%的时间被攻破，这意味着超过一千五百次攻击是成功的。（即使一次成功的攻击也可能造成毁灭性后果。一个重要的、公开可见的系统，如果其0.001%的攻击都能成功，那它就是个灾难。）

  

  

  

  

  

这些缺陷本不应让任何人感到惊讶。驱动LLM（大型语言模型）以及当前这批代理的核心是模仿，而不是我和欧内斯特·戴维斯（Ernest Davis）在2019年提出的 *深度理解* 。

  

当前的系统可以 *模仿* 人们在完成任务时使用的语言，而且常常是在语境相关的情况下，但这并不真正意味着它们 *理解* 自己正在做的事情；它们对“删除数据库”或“编造一个虚构的日历条目”毫无概念。它们是否会犯下此类错误，主要取决于它们碰巧模仿了什么。有时模仿有效，有时则无效。正如我经常强调的，这时我们就会看到“幻觉”和愚蠢的推理错误。

  

重要的是，代理任务通常涉及多个步骤。在像LLM这样根本不可靠的系统中，这意味着有多个出错的机会。迟早，这些错误会累积起来，有时甚至会导致灾难。

  

  

我并不认为代理会消失；最终，AI代理将成为人类历史上最大的省时工具之一。研究它们是有道理的，最终也能赚取数万亿美元。

  

但我严重怀疑LLM能否成为我们所需的基石。

  

本周最具说服力的报道之一来自 *The Information* ，该报道援引了OpenAI内部的消息，基本上证实了我一直所说的观点（尽管没有引用我之前的分析）：纯粹的规模扩展无法让我们实现AGI（通用人工智能）；回报正在递减。他们报道称，GPT-5相对于GPT-4的飞跃，将不会像GPT-4超越GPT-3那样巨大。

本周另一篇新论文认为，LLM正面临一堵墙（呼应了我本人著名的说法）：

> 如果没有与整个系统更深度融合的神经符号AI，以及以丰富的世界模型为核心组件——大致遵循我五年前提出的方案——我看不出代理如何能成功。可靠的代理或许不需要“人工通用智能”，但它们肯定需要我在2020年提出的“稳健”、可信赖的AI。LLM无法带我们抵达那里。

  

> 我预测，有朝一日，当前Agent未能创造出多少持久价值的事实，将被视为一个令人心酸的反思，从智力和经济角度来说，这是一个巨大的错误：错误地几乎只投资于LLM，将其视为一种充满希望的捷径。这个想法是，我们可以通过收集大量数据并投入海量算力，来避免面对经典AI曾苦苦挣扎的挑战。这使少数人（最著名的是英伟达CEO黄仁勋）变得极其富有，但经过数年、投资总额可能接近万亿美元后，这种方法未能产生出能够可靠处理你的日程或银行账户的系统，更不用说成为我们被承诺的博士级别的Agent了。

  

然而，更多的钱正被源源不断地投入这个无底洞。数额大得惊人。根据Renaissance Macro Research的数据，“AI资本支出”（他们将其定义为信息处理设备加软件）对GDP增长的贡献已超过消费者支出。在一个理性的世界里，生成式AI因未能实现围绕代理的预期而应受到惩罚；但在现实世界中，投资者和大型科技公司仍在不断加大投入。

  

与此同时，像神经符号AI这样的替代方法继续被严重低估，我猜它们获得的总投资可能只有1%，甚至更少。想要获得短期回报的投资者似乎不愿意涉足此地，也似乎抗拒探索真正的新途径。风险投资家自诩为勇敢的创新资助者，但在当前的气候下，他们恰恰相反。

  

也许，在AI代理经历了足够多的失败之后——它们连你的日历都不该被信任，更别说你的信用卡了——现实最终可能会开始被人们所接受。

内容含AI生成图片，注意甄别

继续滑动看下一个

简写2019

向上滑动看下一个