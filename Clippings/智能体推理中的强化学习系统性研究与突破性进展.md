---
title: "智能体推理中的强化学习：系统性研究与突破性进展"
source: "https://mp.weixin.qq.com/s/Joc88kTKuknR8UPD8q7KUw"
author:
  - "[[葱葱]]"
published:
created: 2025-10-14
description: "摘要随着大型语言模型（LLM）在各个领域的广泛应用，如何让这些模型能够有效地使用外部工具进行推理成为了一个关键"
tags:
  - "智能体推理"
  - "强化学习"
  - "工具调用"
  - "数据多样性"
  - "深思模式"
abstract: "该研究系统性地探讨了智能体推理中强化学习的关键设计原则，发现真实轨迹数据、多样化数据集和深思熟虑的工具调用策略能显著提升模型性能。"
---
Original 葱葱 *2025年10月14日 15:47*

## 摘要

随着大型语言模型（LLM）在各个领域的广泛应用，如何让这些模型能够有效地使用外部工具进行推理成为了一个关键挑战。本文深入分析了来自新加坡国立大学、普林斯顿大学和伊利诺伊大学香槟分校联合发表的重要研究成果《揭秘智能体推理中的强化学习》，该研究从数据、算法和推理模式三个维度系统性地探讨了智能体强化学习的关键设计原则和最佳实践。

研究团队通过大量实验发现，真实的端到端工具使用轨迹比合成数据能提供更强的监督微调初始化；高多样性、模型感知的数据集能够维持探索并显著提高强化学习性能；探索友好的技术对于智能体强化学习至关重要。更重要的是，他们发现较少但更深思熟虑的工具调用策略优于频繁的工具调用或冗长的自我推理，这为智能体推理模式提供了重要指导。

![研究概览](https://mmbiz.qpic.cn/sz_mmbiz_png/JeVU4AmEeMiclX0x7MCYo3FrwNQjicT3BhfD9Yxhj7AWTcc03czeMsNTFI76hccrDCJaSaTETR4aeaLBhwGiaZEiaA/640?wx_fmt=png&from=appmsg&watermark=1&tp=webp&wxfrom=5&wx_lazy=1#imgIndex=0)

## 引言与背景

在人工智能快速发展的今天，大型语言模型已经展现出了令人瞩目的推理能力。然而，仅仅依靠模型内部的知识和推理能力往往无法解决复杂的现实问题。智能体推理（Agentic Reasoning）范式的出现为这一挑战提供了解决方案，它使大型语言模型能够在推理过程中主动调用外部工具，如代码解释器、搜索引擎等，从而大大扩展了模型的能力边界。

传统的监督微调方法虽然能够教会模型使用工具，但这种方法存在固有的局限性。模型只能按照训练数据的分布来使用工具，无法发展出自适应的工具使用策略。更关键的是，传统方法无法让模型学会何时调用工具、调用频率如何控制，以及如何平衡工具使用与内部推理的关系。

强化学习为解决这些问题提供了新的思路。通过将工具调用建模为动作空间的一部分，并通过结果驱动的奖励来优化自适应策略，智能体强化学习能够让模型超越静态监督，发展出更加灵活和有效的推理行为。然而，直接将策略优化方法应用于智能体推理往往会导致次优的训练和推理行为，包括低效的策略推出采样、奖励和熵崩溃，以及不稳定的训练动态。

## 核心研究方法与技术框架

### 问题形式化

研究团队将智能体强化学习的训练目标形式化为：

max ⁡ π θ E x ∼ D,y ∼ π θ (⋅ ∣ x;T) \[r ϕ (x,y)\] − β D K L (π θ (y ∣ x;T) ∥ π r e f (y ∣ x;T))

![Image](https://mmbiz.qpic.cn/sz_mmbiz_png/JeVU4AmEeMiclX0x7MCYo3FrwNQjicT3BhWwpuOk07bhHLoADyqDnnkI7cmK6ZZtO1jrPDcYMLeHrn9TzGFsy34w/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1#imgIndex=1)

其中 $\\mathcal{T}$ 表示可用工具集合，$\\pi\_\\theta$ 代表策略语言模型，$\\pi\_{ref}$ 是参考模型，$r\_\\phi$ 和 $\\mathbb{D}\_{KL}$ 分别表示奖励函数和KL散度。

与传统的仅依赖语言模型推出的强化学习不同，智能体强化学习在推理过程中集成了工具调用反馈。推出分布可以分解为：

P θ (R,y ∣ x;T) \= ∏ t \= 1 t R P θ (R t ∣ R < t,x;T) ⏟ 智能体推理 ⋅ ∏ t \= 1 t y P θ (y t ∣ y < t,R,x;T) ⏟ 答案生成

![Image](https://mmbiz.qpic.cn/sz_mmbiz_png/JeVU4AmEeMiclX0x7MCYo3FrwNQjicT3Bhm73Z4yaK1LCrknr3UKpQOxEkvS37ObzanmGA9LnlvLOkNQfFcXWkYA/640?wx_fmt=png&from=appmsg&watermark=1&tp=webp&wxfrom=5&wx_lazy=1#imgIndex=2)

### GRPO算法改进

研究采用了GRPO（Group Relative Policy Optimization）作为基础算法，并提出了三种改进技术：损失聚合粒度、奖励塑形和裁剪策略。他们构建了三种不同的配方：

**GRPO-TCR** ：结合了Token级损失、更高裁剪和过长奖励塑形技术。这种配方在实验中表现出了最佳的性能，能够在保持探索能力的同时避免过度探索导致的不稳定性。

**GRPO-SCR** ：采用序列级损失、更高裁剪和过长奖励塑形技术。虽然在某些情况下表现良好，但总体上不如Token级损失的效果。

**GRPO-T** ：作为基线方法，遵循原始GRPO实现，将样本级损失改为Token级损失。

## 数据维度的深入分析

### 真实轨迹vs合成轨迹

研究团队发现，当前的智能体训练流水线往往依赖于LLM编辑或基于模板的合成轨迹，这些方法将选定的推理步骤替换为工具调用。虽然这种方法具有可扩展性，但这种拼接式数据不可避免地错过了关键的决策线索：不仅是如何调用工具，还包括何时、为什么以及接下来该做什么。

![数据多样性对比](https://mmbiz.qpic.cn/sz_mmbiz_png/JeVU4AmEeMiclX0x7MCYo3FrwNQjicT3BhniaGNmjfpMoBTEYWkXjGFNWIEEaI0kbUuGMN5hGkUzFoFd7ia6jxDaPA/640?wx_fmt=png&from=appmsg&watermark=1&tp=webp&wxfrom=5&wx_lazy=1#imgIndex=3)

实验结果显示，使用真实端到端轨迹训练的Qwen3-4B-Instruct-2507在AIME2025上的average@32性能达到29.97%，pass@32达到72.88%，maj@32达到45.22%。相比之下，合成基线在average@32上的表现低于10%，性能不稳定且能力上界显著较低。

真实轨迹的优势在于能够捕获完整的智能体推理行为，这是合成拼接无法复制的。具体来说，真实数据集保留了：调用前分析、受保护的执行、错误恢复和策略修订，以及自我反思和校准等关键环节。

### 多样性数据维持高熵训练

研究发现，大多数现有工作专注于纯数学数据集进行强化学习训练，虽然直观，但这种狭窄的范围忽略了一个关键因素：数据集多样性。通过构建包含17k DAPO-Math样本、4902个数学和3586个代码样本以及3k科学问题的多样化30k样本强化学习数据集，研究团队发现多样化数据集在训练早期导致显著更高的熵增益，并在整个收敛过程中维持更高的熵水平。

这种高熵状态直接驱动了更丰富的探索行为。更重要的是，使用多样化数据集的智能体在仅150步内就能在AIME2025上达到50%以上的average@32准确率，而DAPO-Math基线需要220步才能达到相同水平。

### 模型感知数据集

研究团队观察到不同容量模型之间的明显分歧：Qwen3-4B-Instruct-2507表现出一致且持续的策略改进，而Qwen2.5-7B-Instruct在相同算法和数据集下无法改进，迅速遇到瓶颈。这说明了能力-难度不匹配的问题：当基础策略相对于数据集过于薄弱时，无法提取有意义的梯度进行策略更新。

为解决这个问题，研究团队构建了模型感知的强化学习数据集，将任务分布适应到模型的能力。通过使用SFT模型对30k强化学习数据集中的每个问题执行8次推出，以正确解决方案的比例作为相对于给定模型的问题难度代理。实验结果表明，使用精心策划的模型感知数据集进行训练比使用未过滤数据集产生了显著更有效的改进。

## 算法设计与训练动态分析

### 强化学习技术的影响

![算法性能对比](https://mmbiz.qpic.cn/sz_mmbiz_png/JeVU4AmEeMiclX0x7MCYo3FrwNQjicT3BhMjJWyXxiamPmnl7mXfpiakby4vwOvSkcvqyF9jNpydECzELWDQddCHSQ/640?wx_fmt=png&from=appmsg&watermark=1&tp=webp&wxfrom=5&wx_lazy=1#imgIndex=4)

研究团队通过比较GRPO-TCR和GRPO-T来调查更高裁剪和过长奖励塑形对智能体强化学习的影响。对于Qwen3-4B-RA-SFT，GRPO-TCR相比GRPO-T在AIME2024/AIME2025上取得了显著改进，在仅450步内就达到了70.93%/68.13%的性能，初始准确率分别为29.79%和33.23%。相比之下，GRPO-T在AIME2024/AIME2025上的最佳average@32性能仅为54.7%/40.93%，而GRPO-TCR在仅100个训练步骤内就能达到这一水平，仅使用了GRPO-T 25%的训练计算量。

通过比较GRPO-TCR和GRPO-SCR来调查损失聚合粒度对智能体强化学习的影响，研究发现对于初始性能和探索能力较弱的Qwen2.5-7B-RA-SFT，Token级损失和序列级损失在AIME2024/2025上达到了可比较的average@32性能。然而，对于具有更强初始性能和探索能力的Qwen3-4B-RA-SFT，Token级损失在收敛速度和峰值准确率方面始终优于序列级损失。

### 探索-利用动态

研究发现，在智能体强化学习中，GRPO-TCR和GRPO-SCR都实现了pass@k和average@k的显著且同时改进（在AIME2024/AIME2025上超过10%的增益）。然而，这种改进并非无条件成立：使用基线GRPO-T，仍然观察到传统的权衡，即在训练过程中探索被抑制。

研究将此归因于GRPO-T过于保守的设计：限制性裁剪上界和强KL正则化的结合对分布偏移造成了严重约束，迫使模型维持自包含的生成模式，阻止其充分利用工具交互。

与外部工具的多轮交互在训练过程中也引入了外部信息。来自外部工具的信息使模型能够"更聪明地思考"而不是纯粹"更长时间地思考"，通过发展更高级的认知能力来自主利用工具进行更高效的推理，并从反馈信号中学习。

### 高熵驱动更好效率

![熵分析](https://mmbiz.qpic.cn/sz_mmbiz_png/JeVU4AmEeMiclX0x7MCYo3FrwNQjicT3BhxVmdmUcDkjOfKS1q31WeyPnuTgXodWPN3jGqKPZtbcZicZGxlcbvACA/640?wx_fmt=png&from=appmsg&watermark=1&tp=webp&wxfrom=5&wx_lazy=1#imgIndex=5)

研究团队通过可视化熵轨迹并将其与训练效率和推理性能相关联来进行调查。GRPO-T表现出早期熵崩溃，而性能更好的模型（使用GRPO-TCR和GRPO-SCR训练）的熵上升更快并稳定在更高水平。这表明更大的策略熵与更有效的智能体强化学习训练和更强的智能体推理相关联。

通过使用不同的裁剪上界（包括0.28、0.315、0.35）进行实验，研究发现裁剪上界和训练效率之间存在非单调关系。适度增加裁剪上界能够加速性能改进，但过度推进会产生递减回报。更高的裁剪上界扩展了探索预算并改善了短期进展，但过于激进的裁剪最终会减慢收敛速度，引入过度熵，导致次优的智能体推理性能。

## 推理模式的深入研究

### 工具调用策略分析

![工具使用分析](https://mmbiz.qpic.cn/sz_mmbiz_png/JeVU4AmEeMiclX0x7MCYo3FrwNQjicT3BhtBib2Wd0ichzRHibImDCChqVfibZaEssRMXUickOG999GulSdUt2n7AOaCw/640?wx_fmt=png&from=appmsg&watermark=1&tp=webp&wxfrom=5&wx_lazy=1#imgIndex=6)

研究团队识别出智能体推理中的两种不同模式：反应模式（短思考+频繁工具调用）和深思模式（深思熟虑+较少工具调用）。将这些模式与整体性能相关联，发现最强的模型始终采用深思模式，而较弱的模型主要陷入反应模式。

工具调用效率进一步解释了这种性能差距。深思模式智能体在工具使用中达到70%以上的成功率，表明在行动前仔细推理能够实现高度准确和有效的调用。相比之下，反应模式智能体表现出显著较低的成功率，因为它们快速、频繁的调用往往产生无效或错误的结果。

这些发现突出了一个明确的"质量胜过数量"原则：在深思推理中投入更多推理令牌的智能体最终会进行更少但更成功的工具调用，导致更高的工具使用效率和卓越的任务性能。

### 长CoT模型的局限性

![长CoT模型训练动态](https://mmbiz.qpic.cn/sz_mmbiz_png/JeVU4AmEeMiclX0x7MCYo3FrwNQjicT3Bh8N1DooqxOFmCAwVVXQ6vT1EyRrgQP3feicq8mvGCX2jGfRVO2Io7oIw/640?wx_fmt=png&from=appmsg&watermark=1&tp=webp&wxfrom=5&wx_lazy=1#imgIndex=7)

研究团队发现，当前的长CoT模型在面对推理密集型任务时倾向于避免调用工具，完全依赖内部推理。这种行为因任务类型而异。对于推理密集型任务，长CoT模型倾向于利用其内部推理能力来解决这些任务，因此专注于问题本身而不是分析用户指令或考虑调用可用工具。

为了解决长CoT模型在推理密集型任务中经常避免工具调用的限制，研究团队通过监督微调将它们与智能体推理明确对齐。通过利用SFT数据集来初始化长CoT模型，从而指导它们平衡深思的内部推理与适当的工具使用。

![指令模型vs长CoT模型对比](https://mmbiz.qpic.cn/sz_mmbiz_png/JeVU4AmEeMiclX0x7MCYo3FrwNQjicT3BhSUGtGzopuk6fQFz9ibdkkibOLr5XEvEd1ZAtJ9eVrf0jZd235gmf2IpA/640?wx_fmt=png&from=appmsg&watermark=1&tp=webp&wxfrom=5&wx_lazy=1#imgIndex=8)

实验结果显示，SFT初始化的长CoT模型积极利用工具，同时保持强大的内部推理，与非初始化版本相比显示出显著改进的智能体强化学习性能。然而，尽管有这种初始优势，长CoT模型最终只能达到与基于指令的模型相当的性能，而不是超越它们。

分析表明，基于指令的模型专注于从头开发智能体推理能力，没有专门的内部推理偏见，通过专注的工具使用学习实现持续增长。然而，长CoT模型面临冲突的目标：它们根深蒂固的内部推理模式与智能体推理范式相矛盾，迫使进行缩放和修剪过程，其中智能体推理的增益被抑制过度思考行为的需要所抵消。

## 实验结果与性能评估

研究团队在四个具有挑战性的基准测试上进行了全面评估，包括AIME2024、AIME2025、GPQA-Diamond和LiveCodeBench-v6。评估分为两种范式：自包含推理（模型仅依赖内部推理能力）和智能体推理（模型利用外部工具如代码解释器和搜索引擎）。

尽管只有4B参数，DemyAgent-4B在具有挑战性的基准测试中匹配甚至超越了更大的模型（14B/32B）。值得注意的是，DemyAgent-4B实现了最先进的智能体推理性能，超越了ReTool-32B和rStar2-Agent-14B，甚至在AIME2025上超越了DeepSeek-R1-Zero等长CoT模型。

这些结果进一步证明了简单而有效的训练配方能够在紧凑模型中释放强大的智能体能力。研究表明，通过适当的数据策划、算法设计和推理模式优化，小型模型可以在智能体推理任务上达到甚至超越大型模型的性能。

## 技术创新与贡献

### 数据集贡献

研究团队贡献了高质量的3K端到端智能体SFT数据集和30K多样化有效的强化学习数据集。这些数据集的特点包括：

**真实性** ：SFT数据集包含真实的端到端多轮交互轨迹，而不是合成拼接的数据。这些轨迹保留了完整的智能体推理行为，包括调用前分析、受保护执行、错误恢复和自我反思等关键环节。

**多样性** ：强化学习数据集涵盖数学、科学和代码三个领域，确保了训练过程中的高熵维持和有效探索。

**模型感知性** ：根据不同模型的能力水平调整任务难度分布，确保每个模型都能获得有效的学习信号。

### 算法改进

研究提出的GRPO-TCR算法在原有GRPO基础上进行了三个关键改进：

**Token级损失聚合** ：相比序列级损失，Token级损失确保每个令牌对优化信号的贡献相等，更有效地利用模型的探索能力。

**更高裁剪策略** ：通过设置更高的裁剪上界（εhigh=0.315），平衡了探索和约束满足，避免了过于保守的策略更新。

**过长奖励塑形** ：通过对过长回复进行惩罚，鼓励模型生成更加简洁有效的推理过程。

### 推理模式优化

研究确立了"质量胜过数量"的智能体推理原则，发现深思模式（较少但更深思熟虑的工具调用）优于反应模式（频繁但浅层的工具调用）。这一发现对智能体系统的设计具有重要指导意义。

## 未来发展方向与展望

### 数据燃料稀缺性挑战

当前智能体强化学习面临的一个主要挑战是高质量训练数据的稀缺性。虽然研究表明真实的端到端轨迹对于有效训练至关重要，但收集这类数据的计算成本仍然很高。未来的研究方向可能包括：

**数据蒸馏技术** ：借鉴s1和limo等工作的经验，开发专门针对智能体推理的小规模高质量数据集策划方法。这不仅能够缓解数据稀缺问题，还能通过策划过程的洞察加深对智能体行为的理解。

**自动化数据生成** ：开发能够自动生成高质量智能体推理轨迹的系统，结合大型语言模型的生成能力和专门设计的验证机制，确保生成数据的质量和多样性。

**增量学习策略** ：设计能够从少量高质量数据开始，逐步扩展到更大规模数据集的训练策略，最大化数据利用效率。

### 智能体推理的有效扩展

研究表明，深思推理模式在智能体问题解决中表现优异，但如何有效扩展这种推理行为仍然具有挑战性。未来的发展方向包括：

**智能体特定推理框架** ：开发专门针对智能体的推理框架，优先考虑高级战略规划和高效的工具编排，而不是过度依赖模型的内部推理能力。这种框架应该强调将问题分解为工具可执行的子任务、战略性工具选择以及工具输出的综合。

**分层推理架构** ：设计多层次的推理架构，在不同抽象层次上进行决策。高层负责战略规划和工具选择，低层负责具体的工具调用和结果处理。

**自适应推理深度** ：开发能够根据问题复杂度自适应调整推理深度的机制，在简单问题上快速决策，在复杂问题上进行深度思考。

### 多工具环境的扩展应用

当前研究主要关注代码解释器这一单一工具，但现实应用中往往需要协调多个工具。未来的扩展方向包括：

**多工具协调策略** ：在多工具环境中，正确的解决方案可能需要不同工具的组合，这需要更多的探索来找到最优策略和选择最有效工具的能力。

**工具间依赖关系建模** ：开发能够理解和利用不同工具间依赖关系的模型，实现更高效的工具链调用。

**动态工具发现** ：设计能够在运行时发现和集成新工具的智能体系统，提高系统的适应性和扩展性。

### 大规模模型的适配

虽然当前研究主要在小规模模型（4B/7B）上进行，但将这些发现扩展到更大规模的模型仍然是一个重要方向：

**超参数敏感性研究** ：大规模模型可能对奖励信号表现出不同的敏感性，需要不同的探索策略，或展现出与强化学习训练动态不同交互的更鲁棒推理模式。

**计算效率优化** ：开发针对大规模模型的高效训练策略，包括梯度累积、模型并行和数据并行的优化组合。

**知识蒸馏应用** ：探索将大规模模型的智能体推理能力蒸馏到小规模模型的方法，实现性能和效率的平衡。

### 评估体系的完善

当前的评估主要集中在数学、科学和编程领域，未来需要建立更全面的评估体系：

**多模态智能体评估** ：扩展到包含视觉、音频等多模态信息的智能体推理任务。

**现实世界应用评估** ：开发更贴近实际应用场景的评估基准，包括长期任务规划、多步骤问题解决等。

**人机协作评估** ：建立评估智能体与人类协作效果的指标体系，考虑可解释性、可控性等因素。

## 相关资源与开源贡献

研究团队已经全面开源了他们的工作成果，为学术界和工业界提供了宝贵的资源：

论文： https://arxiv.org/abs/2510.11701

**代码仓库** ： https://github.com/Gen-Verse/Open-AgentRL 提供了完整的训练和评估代码，包括SFT和强化学习阶段的实现。

**数据集** ：

- 3K智能体SFT数据集： https://huggingface.co/datasets/Gen-Verse/Open-AgentRL-SFT-3K
- 30K智能体强化学习数据集： https://huggingface.co/datasets/Gen-Verse/Open-AgentRL-30K

**模型检查点** ：

- Qwen2.5-7B-RA-SFT： https://huggingface.co/Gen-Verse/Qwen2.5-7B-RA-SFT
- Qwen3-4B-RA-SFT： https://huggingface.co/Gen-Verse/Qwen3-4B-RA-SFT
- DemyAgent-4B： https://huggingface.co/Gen-Verse/DemyAgent-4B

这些开源资源为研究社区提供了重要的基础设施，有助于推动智能体推理领域的进一步发展。

## 结论

这项研究为智能体推理中的强化学习提供了系统性的分析和实用的指导原则。通过从数据、算法和推理模式三个维度的深入研究，揭示了智能体强化学习的关键成功因素。

研究的主要贡献不仅在于理论层面的深入分析，更在于提供了可操作的实践指导。通过简单而有效的设计选择，如更高裁剪、奖励塑形和Token级损失，显著提高了训练效果。"质量胜过数量"的推理原则为智能体系统设计提供了重要指导。

更重要的是，研究证明了通过适当的方法，小规模模型可以在智能体推理任务上达到甚至超越大规模模型的性能。这一发现对于资源受限的应用场景具有重要意义，为智能体技术的普及和应用提供了新的可能性。

随着人工智能技术的不断发展，智能体推理将在更多领域发挥重要作用。这项研究为该领域的未来发展奠定了坚实的基础，其开源的数据集、模型和代码将为后续研究提供宝贵的资源和参考。

  

继续滑动看下一个

顿数AI

向上滑动看下一个