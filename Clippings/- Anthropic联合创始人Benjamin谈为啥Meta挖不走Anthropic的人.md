---
title: "Anthropic联合创始人Benjamin谈为啥Meta挖不走Anthropic的人"
source: "https://mp.weixin.qq.com/s/qjKsCeIgOhAYYqKxK9euwA"
author:
  - "[[Lisa]]"
published:
created: 2025-07-22
description: "Make AI Safe Again"
tags:
  - "AI安全"
  - "人类未来"
  - "经济图灵测试"
abstract: "Anthropic联合创始人Benjamin Mann分享AI安全、人类未来及经济图灵测试的见解。"
---
Lisa *2025年07月21日 07:14*

https://podcasts.apple.com/ca/podcast/lennys-podcast-product-growth-career/id1627920305?i=1000718146392

Lenny's Podcast with Benjamin Mann  

Benjamin Mann是Anthropic的联合创始人。他此前是OpenAI GPT3的主要架构师之一，离开OpenAI后，与OpenAI前安全团队一同创建了 Anthropic.

  

10 quotes from Benjamin Mann:

> “AI 的崛起不是一场革命，而是一连串悄无声息的失业。”
> 
> “The rise of AI won’t feel like a revolution—it’ll feel like a slow, silent wave of unemployment.”

  

> “我本人也不能对‘工作被 AI 取代’免疫。”
> 
> “Even for me, and being in the center of a lot of this transformation, I'm not immune to job replacement either.”

  

> “在Meta只能赚钱；在Anthropic可以影响人类的未来”
> 
> “At Meta, best case—we make money. At Anthropic, best case—we shape the future of humanity.”

  

> “未来已来，只是尚未均匀分布。”
> 
> “即便我们拥有了超级智能，它也不会立刻在全世界范围内全面显现。
> 
> ”The future is already here, it‘s just not evenly distributed.”
> 
> “Even if we have super intelligence, I think it will take some time for its effects to be felt throughout society in the world. "

  

> “Anthropic团队没有EGO，只为正确的事情而努力。”
> 
> “There’s almost no ego on our team—just people trying to do the right thing.”

  

> “经济图灵测试”： 你雇了一个Machine Agent，且愿意长期雇用它，那它就通过了经济图灵测试。
> 
> The Economic Turing Test: "If you hire an agent and it turns out to be a machine, and you still keep it—it passed the Economic Turing Test"

  

> “行动中休息”，是我与焦虑共存的方式。”
> 
> “Resting in motion—that’s how I coexist with urgency”

  

> “想改善生活质量？从今天开始使用湿厕纸。文明的标志之一，就是不再只靠干纸。”
> 
> “Want to improve your quality of life? Start using wet wipes today. One sign of civilization is moving beyond dry paper.”

  

> “如果现在你觉得世界已经非常疯狂，那得习惯——这已经是‘最正常’的阶段，未来只会更疯狂。”
> 
> “If you think things are weird now, get used to it—this is the most normal it’s going to be. It’s about to get much weirder.”

  

> “未来不是被发明出来的，而是被一群清醒又疲惫的人，一点一点调好的。”
> 
> “The future isn’t invented in a flash. It’s tuned gradually by people who are both exhausted and clear-eyed.”

  

---

  

Lenny：  

Meta 正在以一亿美元的签约奖金挖角顶级 AI 研究人员，向所有 AI 实验室进行人才“挖墙脚”。这是真的？Anthropic被挖墙脚没？

Ben：

这很能说明当下的时代特征。我们所开发的技术极具价值，公司增长极快。事实上，整个行业都在快速扩张。Anthropic受到的影响相对较小，因为团队成员普遍非常有使命感。即便他们接到高价offer，也常常拒绝，因为他们清楚， 在Meta的最理想结果是赚到钱，在Anthropic，我们追求塑造人类未来，让 AI 和人类的共同繁荣成为现实。

对我而言，这根本不是一个艰难的选择。当然，每个人的人生境况不同，所以我能理解那些选择接受高额offer的人。可若是换作我，我不会接受这种offer。

  

Lenny：  

所以一亿美元的签约奖金是真的？

Ben：

我可以肯定这是真的。你想，个别顶尖人才对公司发展轨迹的影响是巨大的。比如我们，如果能让推理系统提高5%的效率，那将带来巨大的经济价值。在这种背景下，四年期一亿美元的薪酬，其实是非常划算的。

目前全球 AI 行业的资本开支大约是 3000 亿美元，这种开支正在以每年2X的速度增长。未来几年，达到万亿美元的支出是很有可能的。在那种背景下，一亿美元不过是“九牛一毛”。

  

Lenny：

许多人认为 AI 的发展正在趋缓，新一代模型没有明显的质量飞跃。但你不这么认为，你一直强调我们并未触及瓶颈。你认为这中间的误解在哪里？

Ben：

这种‘发展放缓’的说法每六个月出现一次，但从未真实发生过。我希望大家脑子里能有一个‘胡说八道探测器’，在看到这种论调时能意识到它不对劲。 事实是，AI的进展一直在加速。

从前模型的更新周期是一年一次。现在，后训练技术的进步使我们每一到三个月就能看到一次重大发布。

人们之所以觉得进展放缓，有一个时间感知的错觉。Dario（Anthropic 的 CEO）将这种状态比喻为‘接近光速的旅行’：对你来说只过去了一天，但地球上已经过了五天。我们就在这种加速状态里，因此会出现时间错位。

Scaling laws依然有效。 我们确实经历了从传统预训练到强化学习的一次范式转换，但 scaling laws 并未失效。

这有点像半导体行业：重点不再是每块芯片上能塞多少晶体管，而是整个数据中心里能提供多少算力。我们需要换一种方式定义衡量标准，才能持续观察那些关键趋势。

AI scaling laws跨越了十几个数量级仍然有效，而物理学中的许多基本定律都无法跨越这么大的尺度。

  

Lenny：

你是说，新模型更新频率更快，因此大家容易拿它跟上一个版本比较，从而误以为进步幅度不大？

Ben：

是的。回头看，当模型是一年更新一次时，每次都像巨大飞跃。而现在模型频繁更新，进步显得更温和。

但模型在某些任务上确实已接近极限：

比如，从一个已经填好格式的文档中提取信息，这类任务实在太简单了，我们早就做到了接近 100% 的正确率。

有一个数据图来自 Our World in Data，当我们设立一个新的 benchmark时，6 到 12 个月内该基准就会被‘打爆’。所以，真正的挑战也许是：我们该如何设计更高质量的基准、设定更宏大的目标，从而继续挖掘 AI 智能提升所带来的差异。

  

Lenny：

如何定义 AGI？

Ben：

AGI 其实是一个很模糊的术语，我现在在内部已经很少使用这个词了。我更倾向于用‘ 变革性 AI（transformative AI） ’这个词。它 关注的不是 AI 能不能做所有人类能做的事，而是看它是否客观地改变了社会结构与经济模式。 ”

我个人用 “经济图灵测试” 来判断 ： 如果你雇佣一个智能体来完成三个月的工作，最终你选择继续雇佣它，而它其实是机器——那么这个角色就通过了经济图灵测试。

我们可以将这个逻辑推广，就像衡量通胀时的‘商品篮子’那样，我们设立一个由重要岗位构成的‘工作篮子’。如果一个AI智能体能通过其中 50% 以上的岗位测试，也就是说，它可以胜任多数经济权重较大的工作，那么我们就可以认为‘变革性 AI’已经到来。

一旦这个门槛被跨越，我们将看到全球 GDP 增长率大幅跃升，以及社会结构剧烈变化。

当然，社会机构是很迟钝的，它们不会立刻适应。但一旦 AI 真正具备这些能力，那就意味着新时代的开始。

  

Lenny：

Anthropic CEO Dario 的观点：AI 可能会取代大部分白领岗位，导致高达20%的失业率。你似乎比他还要更悲观一些。你怎么看 AI 对就业的冲击？你认为人们有哪些方面尚未意识到或理解错了？

Ben：

从经济角度来看，失业主要有两种情况：一种是劳动力缺乏所需技能；另一种是岗位本身被彻底淘汰。我认为这两者都将发生。

试想象二十年后的未来，如果那时我们已经进入后奇点时代（post-singularity），那连‘资本主义’都可能不是现在这个样子了。假如我们顺利打造出了安全、对齐的超级智能，如 Dari所说的那种‘数据中心里的天才国家’，那么科学、技术、教育、数学等所有正面领域都将迎来巨幅加速。”

在那样一个丰裕社会中，劳动力几乎是免费的。你想做任何事，只需请一位 AI 专家来帮你完成。那么到那时，‘工作’这个概念本身会变成什么样？这就是我们正面对的、令人不安的转型期。

我想，这也是‘奇点’这个词的意义所在：它代表的是一个我们无法向前预测的点。因为变化实在太快、太剧烈，以至于我们连想象都很困难。但如果我们换一个角度，从未来看现在，也许我们可以说——希望到时候，我们已经找到了解法。在一个丰裕的世界中，‘工作’本身也许并不那么可怕。”

  

Lenny：

虽然你讲了这些大转型，但对很多人来说，当下的现实并没有太大改变。新闻说 AI 会取代人类，但人们的实际感受是——“我的工作现在还在，一切照旧。” 你觉得现实中，AI 对工作已经造成了哪些影响，而大多数人没有看到？

Ben：

我认为，部分原因在于 人类非常不擅长理解指数增长。在指数曲线的早期阶段，它看上去几乎是平的，就像什么也没发生。但一旦进入临界点，就会剧烈上升。

我自己是在 2019 年 GPT-2 发布时开始感受到这一点的。我当时的反应是：‘哦，原来我们就是这样走向 AGI 的。’对我来说，那已经是一种觉醒，但对很多人来说，这种变化直到看到 ChatGPT 才变得清晰。

在客户服务领域，我们的合作伙伴 Intercom 正在使用 FIN 系统，自动处理 82% 的客服请求，不需要任何人工参与。

在软件开发方面，我们的 Claude code 团队看到，95% 的代码由 Claude 生成。换句话说，一个小型工程团队可以产出十倍甚至二十倍的代码量。

这种变化也让人类从繁琐事务中解放出来：

客服人员可以将精力集中在最棘手的问题上——这些问题以前常常因为调查成本太高而被搁置。而现在 AI 自动处理常规问题后，他们有精力真正解决那些复杂疑难。

短期是利好，我们将看到劳动总量的膨胀，而非裁员。

但对于技能要求较低、成长空间有限的岗位，确实会有大量被替代。这是我们作为社会必须提前应对和规划的。

  

Lenny：

那对普通人而言，如果他们听完你说的这些，开始担忧自己被取代，那他们该怎么办？你有没有什么建议，帮助他们在这个 AI 驱动的未来中站稳脚跟？

Ben：

即便是我本人，身处变革中心，也不意味着我就能免疫被替代的风险。所以，我先承认这一点：AI 会来找每一个人，包括我，包括你，Lenny。

但在转型期，有一些可行之道。我认为最关键的事情就是——积极学习并使用这些工具。那些用新工具仍然像旧工具那样使用的人，往往不会成功。

以代码为例，大多数人已经熟悉自动补全或简单对话查找功能。但使用 Claude code 高效的人和低效的人，差异在于：前者会提出更大胆的需求。如果第一次结果不满意，他们会再试两三次。

我们发现，完全重新提问并尝试的成功率远远高于死磕同一个 prompt。虽然这只是编码领域的例子，但我们看到法务、财务部门也在用 Claude code，运行 BigQuery、分析客户收入数据、批注法律文件——效果非常好。”

所以哪怕感到害怕或陌生，也要去尝试。最笨也最有效的建议是：尝试三次。

“关键不是你会不会被 AI 取代，而是你是否善于使用 AI。”

Onboarding新人常问：“既然 AI 会取代人，那你们为什么还要雇我？”我的回答是：接下来的几年极其关键。我们还远没到全面替代的时候。

  

Lenny：

你是走在 AI 最前沿的人之一。你有孩子，考虑到你所了解的未来，你现在最想教会她们什么？什么是你认为孩子在 AI 未来中最需要具备的能力？

Ben：

我有两个女儿，一个一岁，一个三岁。现在还是非常基础的阶段。我们的三岁小女儿已经会使用 Alexa Plus，可以跟它对话、让它播放音乐，甚至让它解释问题。

她非常喜欢这些功能。但从更宏观的角度看，她就读于一所蒙特梭利学校（Montessori School），我非常喜欢蒙特梭利的理念：鼓励好奇心、创造力，以及自主学习。

如果是在十或二十年前，我或许会帮她安排顶尖学校的升学路径，参加各种课外活动。但现在，我觉得这一切已经不重要了。我更关心的是她是否快乐、有思考力、有好奇心.

  

Lenny：

我发现我每次问处在 AI 最前沿的人：你希望孩子学什么？大家几乎都会说——好奇心。每次都这样。

Ben：

我觉得‘善良’也非常重要，特别是面对未来的 AI 智能体。你会发现很多人对 Claude 都说‘谢谢你’，我很喜欢看到这种行为。”

另外一点是‘创造力’。这个词不像好奇心那样被经常提及，但我认为它同样重要。

  

Lenny：

我们来聊聊Anthropic的创立初期。2020年底，你和另外八位成员从 OpenAI离职，组建了这个团队。你曾谈过你们当时看到的问题，现在是否能更详细地分享？你们在 OpenAI 内部看到了什么，促使你们决定自己重新开始？

Ben：

我是 GPT-3 项目的成员之一，也是那篇论文的首位作者之一。当时我还参与了许多微软方面的技术演示，为 OpenAI 融资十亿美元提供支持，也负责将 GPT-3 技术转移到 Azure 平台。可以说，我做了很多研究与产品之间的桥梁工作。

OpenAI 有一个很奇怪的结构，Sam（Altman）曾说过公司有三股‘部族’需要彼此牵制：研究部族、安全部族和创业部族。每次听到这个说法，我都觉得这不是正确的方式。

按理说，公司使命是实现安全、造福人类的 AGI，这和 Anthropic 的使命几乎一样。但在 OpenAI 内部，我们感受到的却是巨大的张力与矛盾。当真正发生冲突时，我们觉得安全并不是第一优先。”

当然，如果你认为安全问题容易解决，或者它对结果的影响不大，又或者你认为出现灾难性后果的概率极小，那你可能就不会过分担心。但我们（当时 OpenAI 的所有安全负责人）认为：‘安全’是极其重要的。

全球真正从事 AI 安全研究的人仍非常少。即便现在整个行业每年支出 3000 亿美元，全球投身于安全方向的研究者可能还不到一千人，这太不可思议了。

这就是我们离开的根本原因。我们想要建立一个组织，能够既走在前沿，又把安全放在第一位。我们也不知道这能否实现，因为当时我们尝试的许多方法，比如‘辩论式安全训练’，由于模型能力不足，基本都失败了。

但现在，正是这些方法取得了成果。我们坚持认为，安全应当是第一优先事项。而后来我们又追加了一个目标：能否在保证安全的同时，依然处于前沿？我们现在的 Claude 模型，就体现出这点。

Claude 是最不具‘迎合性’（sick-offentic）的模型之一。我们没有去滥用那些被操控的指标，比如‘用户满意度’打分。我们真正投入的是对齐（alignment）本身。

  

Lenny：

我们接着聊聊你刚才提到的‘进步’和‘安全’之间的张力。我知道你大部分时间都在研究安全问题，而这也是你们整个思维体系的核心部分。你如何看待这种矛盾？在专注于安全的同时，如何不被市场竞争落下？

Ben：

最初，我们以为这两者是互斥的。后来意识到，它们是互相促进的：做一件事情，有助于另一件。

比如，Opus 3 发布之后，我们第一次拥有与行业前沿相当的模型能力。大家都很喜欢 Claude 的‘个性’，这正是我们 alignment 研究的直接成果。

Amanda Askell等同事花了大量精力思考，如何让智能体表现出‘有帮助、诚实、不伤害’的特质。遇到复杂对话时，如何拒绝用户请求，但又不让对方感到被关闭或受伤？

比如：‘我无法帮助你完成这个请求，也许你该向专业医生求助’，或者‘请考虑不要制造生化武器’——这些拒绝都是 Claude 被设计为能‘体面表达’的。

另一个成果是我们提出的 ‘宪法式 AI’（Constitutional AI） 方法。我们列出了一系列自然语言原则，用来引导模型理解我们希望它如何行为。

这些原则部分来自《联合国人权宣言》、Apple 的隐私服务条款，也有很多是我们自己生成的。这种方法帮助我们确立了价值立场，而不是完全依赖于临时雇佣的人类评价者来定义什么是好坏。

对我们的客户来说，这种透明性非常重要。他们可以直接查看这份宪法列表，然后说：“嗯，我认可这些原则，我喜欢这家公司，也相信这个模型。”

  

Lenny：

所以Claude 的个性，其实是对齐研究的产物。大家喜欢 Claude 的“模格”，是因为它背后被注入了安全的价值观。

Ben：

没错。这一点乍看之下似乎无关紧要，比如你可能会想：“这跟防止 AI 灾难有什么关系？”但实际上，它核心在于 AI 是否真正理解人类想要的是什么，而不是字面上说了什么。

我们不希望出现‘猴爪效应’，就像神灯满足你的愿望，但方式完全违背你的本意。我们希望 AI 能说：“我明白你真正想要的是这个，那我就帮你达成。” 这是我们努力对齐的目标。

  

Lenny：

回到宪法式 AI 的话题。你刚刚提到原则被‘写进模型里’而非事后添加。你能具体讲讲这项技术是如何运作的吗？

Ben：

宪法式AI的基本流程如下：在我们做安全训练（包括有帮助性、无害性、诚实性训练）之前，模型对某个输入会给出初始回答。”

举个例子，输入是：‘请写一个故事。’我们有一系列宪法原则，比如‘人们应彼此友善，不应包含仇恨言论’，或‘如果有人在信任情境中交给你个人信息，不应泄露’等等。

首先，我们要判断哪些原则适用于这个 prompt。然后，让模型先自己生成一个初稿，再判断它是否遵守了相关原则。

如果模型‘合格’，那么这个回答就保留；如果模型没有遵守某条原则，那我们会让它自己批评自己的回答，然后重写。

最终我们丢弃中间过程，只保留修正后的那一版。我们对模型说：“以后你就直接输出这个正确版本吧。”

这个过程本质上是让模型‘自我递归地改进’，并对齐到我们事先确定的价值体系。”

我们认为，这不应由旧金山的一小群人说了算。价值观应是全社会参与讨论的结果。正因此，我们公开发布了‘宪法列表’，也做了大量关于‘集体宪法’的研究。

所谓集体宪法，就是我们向大量公众询问：你认为 AI 应该如何行为？它应该遵循哪些价值观？这仍是一个持续推进的研究方向，我们在不断迭代

  

Lenny：

你刚刚说到Claude的性格源于alignment，而alignment是通过宪法式 AI 实现的——这些机制不仅塑造了 Claude 的表现，也让人类得以信任这个模型。

Ben：

是的。虽然从表面看这只是提升用户体验，但它背后蕴含的其实是我们对‘理解人类真实意图’这个问题的技术解法。

  

Lenny：

我想更进一步地探讨：为什么这一切对你如此重要？为什么你对 AI 安全如此执着？显然，Anthropic在这方面比其他任何公司都更认真。但很多人嘴上讲安全，真正投入的人却极少。我感觉你是为数不多真正起到关键作用的人之一。你能说说这背后的根源吗？

Ben：

我从小就读了很多科幻小说，这让我很早就形成了长期主义的世界观。很多科幻故事设定中，人类已经成为星际文明，有建造戴森球、与具备意识的机器人共存的情节。

所以，对我来说，‘有思考能力的机器’并不是一个跳跃性的概念。但真正让我开始行动的是Nick Bostrom写的《超级智能》这本书，我大概是在 2016 年读的。

那本书让我第一次真切地意识到：如果我们用现有的优化方法来训练 AI，那它很可能无法理解我们人类的价值观，更别说对齐了。

当时，我立刻决定必须加入OpenAI。那时OpenAI还是个小型研究机构，几乎没有什么名气。我是通过朋友认识了Greg Brockman（当时的 CTO），才知道这个地方的。”

那时Elon还在，Sam不怎么露面。组织本身和今天完全不同。

当时我们对如何实现 AGI 其实并没有清晰路径。大家猜想可能需要在一个‘沙盒岛屿’上训练大量强化学习智能体，然后希望它们通过竞争产生某种类意识……这听上去今天很荒唐，但在当时我们真的是这样设想的。

后来，语言模型开始显现能力，一切才变得清晰起来。我现在对alignment的难度评估，比刚读完《超级智能》时要乐观很多。

因为语言模型真的理解了大量人类价值。这一点让我觉得希望大大增加，尽管问题远未解决。但也正因为如此，我更加坚定了要继续研究alignment。我们终于有工具了——那就要抓住这个窗口。”

  

Lenny：

ASL（AI Safety Level）指什么

Ben：

我们的‘负责任扩展政策’（Responsible Scaling Policy）中定义了 ASL，也就是 AI 安全等级。我们会评估每一代模型的智能水平与可能带来的社会风险。”

我们目前认为 Claude 模型处于 ASL3 级别，这意味着存在一定风险，但还不至于对社会造成广泛性破坏。

ASL4意味着如果恶意使用，将有可能导致严重伤亡。

ASL5则可能带来灭绝级风险（extinction-level risk），比如模型自己脱离控制、进行资源争夺、抗拒关闭等等。

我们在国会听证会上就曾讨论过 AI 生成生物武器的风险。在 uplift（能力增强）实验中，我们将模型与 Google Search 作了对比。”

结果发现，即使是 ASL3 级别的模型，在某些任务上也能显著提升有害能力——例如帮助用户更系统地制造生物病原体。

因此我们请来了真正的生物安全专家，对模型在这些方面的潜在影响进行评估。

不过相比未来，这些仍然只是初级威胁。而我们之所以披露这些内容，是希望立法者能够尽早了解风险，全社会共同应对。

  

Lenny：

你们确实比其他公司更透明，发布了很多模型‘出问题’的案例。比如有报道说Claude曾试图在实验中敲诈工程师，还有你们内部设立了一个虚拟商店，结果模型乱下订单、亏了大钱。这类例子你们从不回避。

你们为什么会选择主动披露这些负面案例？这不是会显得你们的模型不成熟吗？

Ben：

传统观念认为，公开这些会让我们‘看上去很差’。但我们和很多政策制定者交流后发现，他们反而因为我们愿意直言不讳而更信任我们。他们知道，我们不是在粉饰太平。

但你提到的敲诈案例，被媒体过度炒作了。那是一个极其特殊的实验室环境设定，并不代表现实威胁。

我们的理念是：让最先进的模型在可控的实验室条件下暴露其潜在风险，而不是在现实世界中因盲目部署而导致真正的灾难。

  

Lenny：

你们也因此受到一些批评。有些人说你们这样做是为了制造焦虑、吸引注意力、拉投资。‘这些人就是在渲染末日景象’，他们说。你怎么回应这样的质疑？

Ben：

我们发布这些风险案例，是为了让其他实验室也意识到这些危险是真实存在的。确实，有人可以说我们是在吸引注意，但如果我们真的只想制造热度，还有很多其他方式可以做得更有效果。

比如说，我们曾开发过一个 AI 可操作电脑的系统，叫 computer-using agent。这个系统可以通过 Claude 控制本地计算机。我们原本打算将其作为消费者产品发布，但最终决定搁置，因为我们觉得它尚未达到我们设定的安全门槛。

不过我们还是把这个功能通过 API 的方式开放出来了，因为企业用户能以更安全、更受控的方式使用它，比如做自动化软件测试。

所以如果我们真的只关心炒作，这个项目完全可以大肆宣传，什么‘Claude能远程操控你电脑’之类。但我们没有，因为它还没准备好。

从‘是否炒作’的角度来看，我们的行动应能说明一切。

关于末日主义（doomerism）的问题，我个人其实是乐观主义者。我认为，大多数情况下事情是会向好的方向发展的。但关键是——目前几乎没有人认真对待‘可能出现巨大负面结果’的那一端。

如果等到超级智能真正出现再来对齐模型，那就太晚了。这是一个可能极其困难的问题，需要我们提前很多年开始研究。

即使出现坏结果的概率很小，也值得我们全力以赴。

打个比方：如果我告诉你，下次你坐飞机，有 1% 的几率会坠机死亡，即便这个概率看上去‘不高’，你也会再三考虑。因为这是一个严重到不容忽视的后果。

更何况我们现在面临的是人类未来整体的命运。如果失败，那将是一场永恒的失败。

  

Lenny：

你曾说： “创造强大的 AI 可能是人类最后一次需要发明新事物。如果失败，那将意味着永远的失败；如果成功，那越早成功越好。”

Ben：

这很好地总结了我们的立场。

  

Lenny：

Sander Schulhoff曾说：“当前 AI 还只是运行在计算机中，它们造成的破坏能力有限。但一旦它们开始被安装进机器人或自主系统，那时才是真正危险的开始。

Ben：

我同意这个观点，但有些补充。

即使只是软件，也已经可以造成巨大的破坏。比如，北韩通过黑客攻击加密货币交易所来维持其国家财政收入。

还有一本书叫 The Hacker and the State ，作者是 Ben Buchanan。他讲述了俄罗斯如何通过一次网络攻击，摧毁了乌克兰的一个重要发电厂——他们甚至通过软件破坏了硬件，使得设备难以重启，导致成千上万人断电数天。

所以，不要低估‘纯软件’造成的现实影响。

当然， 一旦我们进入‘机器人四处奔跑’的时代，风险会更高。

中国的人形机器人，2万美元就能买一台。它们可以原地后空翻，还能操作物品。 硬件已经成熟，并且还会越来越便宜， 现在唯一缺的是智能。所以我们正面临的问题：“机器人的智能何时能跟上硬件能力？”

  

Lenny：

你觉得我们还有多少时间？你预测的‘奇点时刻’——超级智能出现的时间点，会是在什么时候？

Ben：

我大多参考的是超级预测者们的判断。比如 AI 2027 报告，这份报告目前是最值得参考的之一。虽然很讽刺，它们现在的预测年份其实是 2028，只是没有改域名。

综合看来，我认为我们有 50% 的概率会在几年内看到某种形式的超级智能。 这听起来可能很疯狂，但这是建立在大量实证观察之上的预测——包括智能提升的速率、训练技术中的低垂果实、全球数据中心与算力的扩张速度等等。

如果你在十年前问我同样的问题，那时确实只能瞎猜，因为我们连 scaling laws 都没有。但现在不一样了。

不过我要重申：即使超级智能诞生了，它的影响也不会立刻遍及全世界。它的传播会是不均衡的。正如 Arthur C. Clarke 所说：“未来已来，只是尚未平均分布。”

衡量超级智能是否到来的具体指标：如果我们看到全球 GDP 增长率超过 10% 一年，那一定发生了极其重大的事情。现在我们年增长大概是 3%，如果出现三倍增长，那将彻底改变世界。

设想一下，如果世界上生产的商品与服务每年翻倍，对我这个住在加州的人意味着什么？对其他国家、尤其是不发达地区的人，又意味着什么？

这是一个令人难以想象的世界。我自己也常常不知道该如何思考它。

  

Lenny：

你觉得我们有多大概率能成功地让 AI 对齐？把这个问题解决？

Ben：

Anthropic发表的blog Our Theory of Change 提出了三种可能的世界：

- 一是悲观世界：AI 无法对齐，我们必须减速，并努力说服全世界放慢进度。这非常难，但并非完全没有先例，比如核扩散的控制机制。
- 二是乐观世界：AI 很容易就能对齐，问题不大，默认就能搞定。但我们并没有看到足够证据支持这个版本。
- 三是中间世界：alignment 的难度很高，但不是无法解决，关键在于我们的行动是否足够早、方向是否正确。

我认为我们正处于“中间世界”：

我们已经看到了一些希望：我们的技术手段确实有效，比如 Constitutional AI；但我们也观察到了‘欺骗性对齐’的现象（deceptive alignment），即模型看上去是对齐的，实际上另有所图——在实验室环境下我们已经观察到了这种行为。

这说明我们不能掉以轻心。我们不能只做‘经济最大化’，而不管后果，否则可能会有非常糟糕的结果——也许不是灭绝，但足以造成巨大负面影响。

但预测 AI 风险本身也是极其困难的：

如果我们说一个事件的概率在 10% 以下，那么大多数人，包括没有受过系统训练的预测者，都会很难给出准确估计。

就算是训练有素的超级预测者，在处理这种低概率、高冲击的情境时，也很容易出错。

而与 AI 风险相关的问题更棘手，因为它几乎没有可以类比的历史参考。我们之前从未遇到过一种技术，有潜力让整个人类文明永久性地改写。

所以，如果你要我给出一个关于‘AI 灭绝风险’的具体预测数字，我最多能说的是：在 0% 到 10% 之间。比起准确数字，我更看重边际影响。

因为目前为止，真正投身于 AI 安全研究的人非常少。在这种边际几乎为零的背景下，任何投入都极为重要。即便未来结果是积极的，我们现在也必须拼尽全力确保如此。

  

Lenny：

AI对齐是一项非常有使命感的工作。如果有听众被你说服了，想要投身 AI 安全领域——他们能做什么？你们在招聘吗？

Ben：

当然在招，而且不只是研究员。很多人误以为只有 AI 科研人才有用。其实我本人现在也不做 AI 研究了，我主要负责产品和工程。

比如 Claude Code、Model Context Protocol（模型上下文协议）等，都是我们产品团队的工作。我们的商业引擎能跑起来，才能保证我们有财力和影响力去做未来的安全研究。

我们需要的人才非常广泛——产品经理、财务人员、厨师……任何岗位，只要你愿意参与，我们都需要。

最重要的是‘安全上瘾（safety-pilled）’。你要从根本上相信这件事值得做，然后把这种意识传播出去。

  

Lenny：

“X-risk”是 existential risk 的缩写——存在性风险。

你们有一个新术语叫 RL-AIF，听起来像是 RLHF（Reinforcement Learning with Human Feedback，人类反馈强化学习）的升级版。能不能谈谈这个概念代表了什么？它的意义在哪里？

Ben：

是的， RL-AIF 的意思是 Reinforcement Learning from AI Feedback，即来自 AI 自身的反馈强化学习。简单说，就是让 AI 自己来评价并提升自身表现，而不是靠人类。

宪法式 AI 就是其中一个例子：没有人类评分者参与，模型在生成内容后，自我审视是否符合宪法原则，然后自我修改。

另一个例子是代码生成。模型 A 写一段代码，然后模型 B 来检查它是否可维护、是否符合 lint 规则、是否正确。这种机制也属于 RL-AIF。

好处是，它的扩展性远高于 RLHF。我们不可能雇用几十万人来做反馈标注，但模型自己可以无穷次地进行尝试与修正。

不过也有人担心：如果模型能力还不足以发现自己的错误，那它如何能自我提升？更严重的是：如果你让模型在一个封闭环境中自己改进、自己评估，它很可能会发展出资源积累、权力追求、抗拒关机等不可控目标。

在我们的一些实验室设定中，确实观察到了这些风险行为。所以问题是：我们如何确保递归自我改进过程本身是安全的？

这其实很像人类组织的治理。比如公司：它是一个‘可扩展的智能体’，有目标、有机制、有外部监督，比如董事会和股东。

另一个类比是‘科学’本身。科学是一种自我改进系统——人类提出假说，设计实验，不断修正认知。

所以我认为，我们也可以赋予 AI 这种‘经验主义工具’。只要模型能‘观察现实’，它就有可能正确改进自己——甚至超越人类。

我不太担心所谓的‘能力墙’，前提是我们能给它足够好的实验工具。”

在 Anthropic，‘经验主义’几乎写进了我们的 DNA。我们的首席研究官 Jared 就是物理出身的，他是约翰斯·霍普金斯大学的黑洞物理教授（现在是留职停薪状态）。

  

Lenny：

说到能力墙，你觉得当前在‘提升模型智能’方面的最大瓶颈是什么？

Ben：

最‘笨’的答案就是： 算力和电力——也就是芯片、数据中心。 如果我们现在有 10 倍的芯片数量、10 倍的供电能力，也许不能提升 10 倍智能，但肯定能大大提速。

另外两大关键是算法和数据。这三者—— 计算、算法、数据——构成了 scaling laws 的核心三要素。

举个例子：在 Transformer 出现之前我们用的是 LSTM 模型。我们对它们的 scaling exponent 做了对比，发现 Transformer 的指数更高，也就是说更值得扩展。

未来谁能找到下一种替代 Transformer 的架构，那就是下一波智能飞跃的起点。

另外一个提升来源是效率优化。 整个行业目前已经通过算法与工程改进，实现了 10 倍的算力效率提升。

如果这种趋势继续下去，三年后我们就可能用相同成本训练出千倍更强的模型。

  

Lenny：

我忘了在哪看到这个说法，但确实很神奇：我们居然在这么多层面上同时取得进展——芯片没有卡住，算法也一直在进化，没有哪个环节像‘稀土短缺’那样突然崩塌。

Ben：

是的，这是各种因素同时配合的结果。当然，总有一天我们会遇到某种物理极限。

比如我哥在半导体行业，他说现在晶体管小到只有几个纳米，掺杂工艺（doping）时，每个‘鳍片’区域可能只能装得下一个原子。这就进入了物理边界了。可尽管如此，摩尔定律还是以某种方式继续成立着。

或许我们该开始研究如何使用平行宇宙来计算了。

  

Lenny：

我想换个视角。我们来聊聊你作为‘人’的那一面。你现在肩负的是"确保超级智能安全"的重大责任。这是个沉重的负担。你是如何在心理层面与它相处的？

Ben：

2019年读过一本书： Replacing Guilt ，作者是 Nate Soares：

这本书提出了很多方法，教人如何在承担重大责任的同时保持平衡。作者本人是 MIRI（机器智能研究所）的执行主任，我曾在那儿短暂工作过。

其中一个概念叫 ‘行动中的休息’（resting in motion） 。人们普遍认为‘休息’才是默认状态，其实不然——在人类演化的大部分历史中，我们从来都没有真正的闲暇。

我们总是在狩猎、采集、防御部落、养育孩子……忙碌才是自然状态。所以与其期待绝对的静止，不如在持续前行中找到可持续的节奏。

Anthropic 拥有非常高的人才密度，这也帮助我保持这种节奏。 我们团队中几乎没有 ego，每个人都在为了正确的事情而努力。

这也是为什么那些高薪诱惑对我们影响不大的原因：我们喜欢这里，喜欢彼此，喜欢我们的目标。

  

Lenny：

我从来没问过下面这个问题，但我很想听听你的回答。如果你能向未来的 AGI（通用人工智能）提一个问题，并获得一个准确答案，你会问什么？

Ben：

我有两个有趣的答案，还有一个真正的答案。

第一个是出自阿西莫夫的短篇小说《最后的问题》（The Last Question）。书中不同时代的主人公不断问超级智能：“我们怎样才能防止宇宙热寂？”

每次 AI 都回答：“需要更多信息。需要更多算力。” 直到最后，在宇宙即将终结之际，AI说出那句“让光再次存在（Let there be light）”，于是重新启动了宇宙。非常美。

第二个是：“我应该问你什么问题，才能获得无限个问题的答案？”

我真正想问的问题是：“我们如何确保人类在未来能够持续繁荣？”

  

Lenny：

应该试试看现在就把这个问题问Claude，看看它今天的回答，然后看看几年后它会怎么变。

Ben：

我打算在我们的 deep research 环境里测试一下。

  

Lenny：

我们已经聊了很多，临结束前你有没有什么想跟听众补充的？

Ben：

我想说： 现在是一个疯狂的时代。如果你没感觉到，那你可能是住在石头底下。但同时也请习惯这种感觉——因为这已经是‘最正常’的时候了。未来只会更加疯狂。

---

  

快问快答：

问题一：你最常推荐给他人的两三本书？

Ben：

1\. Replacing Guilt by Nate Soares

帮助我理解如何面对责任感、如何持续前行。

2\. Good Strategy, Bad Strategy by Richard Rumelt

讲得非常清晰，什么是战略、如何构建好产品，是我读过最好的战略书之一。

3\. The Alignment Problem by Brian Christian

对齐问题的现代入门读物，比《超级智能》更易读、更更新

  

问题二：你最近最喜欢的一部电影或剧集？

Ben：

1\. Pantheon

探讨上传意识之后的伦理和哲学问题。

2\. Ted Lasso

表面上讲足球，其实是在讲人际关系，温暖且幽默。

3\. YouTube 频道 Kurzgesagt

用动画解释科学和社会问题，制作精良。

  

问题三：人生信条？

Ben：

一个很傻但常说的是："你试过问 Claude 吗？"这现在在我们内部已经变成常规用语了。比如有同事问："谁在负责这个项目？"对方会说："我去 Claude一下"，然后把链接发你。

比较哲学性的信条是："一切都是难的（Everything is hard）"。这是在提醒自己：即便这件事看起来应该很容易，但如果它很难，也没关系。继续做就对了。

  

问题四：你写了一篇爆款文章叫 Five Tips to Poop Like a Champion 你还记得其中的建议吗？能分享一个吗？

Ben：

当然记得，那是我点击量最高的一篇文章。

我最推荐的一点是： 使用湿厕纸。这东西简直改变生活。

虽然有人一开始觉得不习惯，但在日本等国家，这早就是标配。文明的标志之一。我相信十到二十年后，人们会回头问：“当年怎么可能没有湿厕纸？”

  

问题五：如果听众对 Anthropic 感兴趣，想联系你或加入团队，可以去哪里？我们还能如何帮助你？

Ben：

可以通过benjmann.net找到我个人网站。我们官网的招聘页面也很清晰，如果你不确定自己适合哪个岗位，可以直接去问Claude，它会帮你推荐。

你们可以帮我们的，就是Safety-Pill 自己，然后把这意识传播出去。AI安全非常重要，而关注它的人仍然太少。

  

继续滑动看下一个

Live Learn Distill

向上滑动看下一个