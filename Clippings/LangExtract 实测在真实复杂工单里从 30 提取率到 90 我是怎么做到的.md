---
title: "LangExtract 实测：在真实复杂工单里，从 30% 提取率到 90%+，我是怎么做到的?"
source: "https://mp.weixin.qq.com/s/xM_lYyQXmg4JCpd1w7kPxg"
author:
  - "[[cyqian]]"
published:
created: 2025-09-23
description: "谷歌开源的 LangExtract，能否真的在复杂工单场景里落地？我用 60+ 次实验，把提取率从 30% 提升到 90%+，还原真实案例，客观评价优势与局限，带你看清这款“既折磨人又有潜力”的工具。"
tags:
  - "LangExtract"
  - "工单处理"
  - "提取率优化"
  - "复杂场景落地"
abstract: "作者通过60多次实验优化，将谷歌开源工具LangExtract在复杂工单场景中的信息提取率从30%提升到90%以上，并分享了实际应用经验和工具评估。"
---
Original cyqian *2025年09月22日 07:30*

延续前文的话题

之前我在公众号里写过几篇文章，讨论工单智能化和社会治理里的语义提取问题，观点包括：

- 工单分类越细、标签越多，但效果往往更模糊；
- 热门问题、治理风险往往躲在“其他”类别里；
- AI 在工单/政务环境里的落地，不只是模型强，而是从需求、方法、工程落地的全链条跑通。

这次，我把目光锁在谷歌开源的 LangExtract这个库上，实测它在我们真实复杂工单环境里的表现，并系统复盘我的 60 多次优化过程。希望能给你们一个参考：什么是真的 “能用”，什么还是“样子好看”。

  

**1**

官方简介：LangExtract 是什么

在深入测评之前，我们先看这款库本身是怎么设计的，它官方标出来的强项有哪些，以及它试图解决什么问题。

根据 Google 的资料：

最近发布的LangExtract 是一个开源的 Python 库，用于把非结构化文本（客服工单、投诉、法律文本、医疗报告等）抽取成结构化信息。你可以告诉它想要的字段（schema），给它提示词 + 样例，它就帮你“拉出”你关心的那些信息。

核心特色之一是 Precise Source Grounding ——每个抽取出的实体，都会标回源文本中的确切字符区间（character offsets），方便校验与可视化。

它支持 few-shot 样例 + schema 控制的输出，通过用户提供示例来引导（“我想要这个格式、这些字段、输出应该怎样”）。

面对长文档／篇幅大文本，LangExtract 提供了 chunking（拆块处理）、parallel / 多 pass 提取策略，以避免“关键信息被埋掉”的问题。

它还有内置的 交互式可视化 功能：可以生成一个 HTML 文件，高亮原文中的抽取实体，方便人工审核与共有展示。

模型后端支持灵活：既可以用云端的大模型（例如 Google 的 Gemini 系列），也可以配合本地或开源模型（例如通过 Ollama 等途径）使用。

官方开源在 GitHub，Apache-2.0 授权，有较高的社区 star 数（约 15.5k stars），forks 也不少。说明大家对这个工具关注度很高。

  

**2**

为什么这个官方简介重要

因为它告诉我们：LangExtract 的设计初衷里，就考虑到了很多我们在工单中遇到的难点 ——长文本、杂乱语义、格式漂移、可审计性等。

测评中，我可以对照这些官方承诺来验证真实场景下能不能落地，这样既有参考价值，也能看出缺口在哪。

而且，如果只谈“我做了 60 次优化／提取率多少多少”，但不对比它的设计承诺，就难以显示专业性。

  

**3**

真实复杂工单示例

下面是我们真实工单里节选的文本（已脱敏），保留复杂结构、多诉求、指代、省略、历史关联等特性，以体现真实难度。之后是我对这些复杂示例的抽取结果，用它们做实验。只举一个例子，篇幅局限，难免不全面。

示例 ：劳动纠纷 + 多证据引用 + 时段跨度长

文本（节选，已脱敏）

我在某施工单位B工作，去年 12 月至今年 1 月加班众多，工资差额至今未补。多次与项目经理沟通无果。微信聊天记录与考勤表可提供。请人社部门协调处理，并希望在本周内给到明确处理时限。我也担心其他同事遇到同样问题。

```json
{  "person": ["（未提供姓名）"],  "date": ["去年12月—今年1月（跨月）", "本周（相对）"],  "organization": ["施工单位B", "人社部门"],  "event": [    "工资差额未补",    "多次协商未果",    "存在群体性风险"  ],  "request": [    "人社介入协调",    "给出明确处理时限",    "必要时开展企业排查"  ],  "desired_outcome": [    "核算并补发工资差额",    "规范加班与考勤"  ],  "urgency": "高（劳动权益，潜在群体性）"}
```

  

**4**

核心特色：样例机制 + 官方强项在实测中的作用

通过实测，我发现：

官方承诺的 few-shot 样例机制 是非常关键的。没有样例，仅靠提示词，很多复杂例子里“事件（event）”不容易提取，日期和紧急程度识别不稳定；

源定位（source grounding）使我们在复杂文本里能校验抽出来的是什么、来自哪段文字。比如在“白色小型车剐蹭事故 + 对方失约未接电话”的例子里，抽取出的“对方未到场”“未接电话”这些事件，在源文本中位置清楚，便于人工验证，是非常实用的；

长文拆块 + 多 pass 提取起到决定性作用，因为部分诉求／事件往往出现在段落后面或者上下文远一点。如果整篇当作一个输入，有时模型“忘掉”前面/后面内容。

换言之，如果完全不具备这几个特点，导致的结果可能是，我不得不换用更高阶模型，更大参数量，尝试更暴力地解决问题。

  

**5**

我如何从 30% 提取率 跑到 90%+：60+ 次优化实战路径

我这个应用场景不适合使用Gemini，我也不太愿意用。所以基于 Ollama本地化加载了Qwen3:8b模型作为LLM基础。

下面是我在真实复杂工单环境中走过的路径，每一步都踩坑，也每一步都有收获。

| 阶段 | 核心目标 | 关键动作 | 收益 /瓶颈 |
| --- | --- | --- | --- |
| 第 1-10 次 | 提高基础提示词清晰度 | 加详细描述、结构化 prompt，添加负例约束 | date 提取率从 ~30% 提至 ~45%；其他字段提升有限；事件字段几乎提不上来；稳定性差 |
| 第 11-20 次 | 引入样例机制（few-shot） | 为复杂工单写 8-15 条真实复杂样例；保证多样性：多诉求、多主体、省略、指代等 | 提取率整体跃升到 ~60-70%；event 字段首次稳定抽出；但 date/模糊时间仍旧弱 |
| 第 21-35 次 | 专项攻克日期 & 相对时间 | 将日期表达分为标准／相对／模糊；引入 fallback “未知”；结合工单提交时间用于推断；样例中必须有相对时间用例 | date 提取率升到 ~65%；模糊时间识别仍有误；稳定性仍受 prompt +样例组合影响 |
| 第 36-50 次 | 重构字段定义 & 边界 | 将 urgency 从 event 中拆出；明确定义什么算“事件”、什么算“请求”；避免字段之间重叠或冲突 | 字段间互扰减少； urg­ency 与 desired\_outcome 较稳定； event/request 输出格式更一致 |
| 第 51-60 次 | 建立评估体系 &稳定化 | 写自动化评估脚本；版本对比；多轮复跑一致性测试；针对高紧急与低紧急工单样本加权检查 | 稳定性大幅提升；提取率多数字段稳定在 95%+；高紧急工单几乎无漏检；但资源消耗与提示词/样例调优成本高 |

最终在我们数据集上的结果如表：

| 字段 | 初始提取率 | 优化后提取率 |
| --- | --- | --- |
| person | ~60% | ~97.8% |
| date | ~30% | ~91.3% |
| organization | ~50% | ~95.7% |
| event | ~0 | ~95.7% |
| request | ~70% | ~97.8% |
| desired\_outcome | ~40% | ~100% |
| urgency | — | ~100% |

此外，多次复跑中一致性＞95%，提示词 +样例的小改动带来的漂移被明显压缩。

样例机制与官方承诺对比：特色是否真的落地？

结合官方设计与我的实测：

| 设计特性 | 复杂场景的真实表现 | 限制或需注意 |
| --- | --- | --- |
| few-shot 示例 + schema 控制 | 非常关键；没有样例几乎不能稳定识别很多复杂事件；样例组合好坏差异大 | 样例设计门槛高；需覆盖复杂结构；样例数量太少或太单一会导致“过拟合”某类工单，泛化差 |
| 源定位 / span grounding | 提高校验与可视化效率：可以快速定位错误；人工审核变得容易 | 在文本繁杂或有 OCR 噪声或格式乱的场景，span 匹配可能偏移或乱，需要预处理 |
| 长文拆块 + 多 pass | 必不可少；补充了“遗漏事件”问题，提升 recall | 拆块策略要谨慎（边界的重叠、上下文的保留）；多 pass 要控制成本与延迟 |
| 可视化工具 &交互 html 输出 | 非常有用，利于验收与内部报告、展示给管理层看；减少人工抽查成本 | 不是每个人都用；可视化量大时处理与打开也有技术成本 |

优点 / 缺点&评分

优点：

- 强语义理解，能够应对真实工单里多诉求、省略、指代、历史关联等复杂性
- 样例机制 + schema 控制 +源定位使输出一致性好，结果可审计
- 适应长文本，有长文拆块、多轮提取策略，漏检少
- 可视化工具好用，对内部治理 /展示 /报告很有帮助

缺点：

- 上手门槛高：提示词 +样例 + schema 设计要投入时间和经验
- 资源消耗与成本高：硬件、模型调用、调试周期都不短
- 对于非常模糊或省略非常多的工单还是有挑战（部分 relative time/模糊表达场景仍有误差）
- 稳定性虽大幅提升，但一点微调就可能带来漂移，需要谨慎版本控制与回溯机制

评分 (满分 5 星)：

| 维度 | 得分 |
| --- | --- |
| 语义理解能力 | ⭐️⭐️⭐️⭐️⭐️ |
| 灵活性／可定制性 | ⭐️⭐️⭐️⭐️ |
| 易用性 | ⭐️⭐️ |
| 成本与资源要求 | ⭐️⭐️ |
| 优化潜力 | ⭐️⭐️⭐️⭐️ |

  

**6**

与前几篇文章的关系&本次实测带来的新认识

前几篇我提了“越细的分类越糊”“标签多不等于效果好”，这次实测证实了：只有当“分类/标签”背后有样例 + schema +验证机制，分类细分才有意义。

在治理/工单监控中“热点”、风险往往隐藏在那些“非标准表达”“省略”“历史指代”等复杂症状里，这次用真实复杂例子抽出来后，可以更早识别这些风险。

之前理论说“few-shot + prompt 工程是关键”，这次我用真实数据验证了样例机制 +长文处理 +源定位这些是决定性因素。

  

**7**

结语

从最初 30% 的混乱提取率，到最后 90%+ 的稳定输出，我最大的体会是：

强大模型只是基础，真正能用的是把模型设计与业务实际结合起来——有很清的字段定义，代表性且丰富的样例，严格的评估体系，以及可视化 /可校验机制。

LangExtract 是我目前见过的，在这些维度做得比较系统和接近生产环境的，有可用性的开源库。

它不是“开箱即用”的万能工具，但在你愿意投入工程能力的情况下，价值非常高。

如果你也在工单 +社会治理 +客户/政务反馈情境里尝试语义提取，欢迎留言交流具体样例／设计思路／评估指标。

再次强调一下，本着最小化够用为原则，我只用到了Qwen3:8b，并没有使用更大的模型。

如果觉得这篇对你有启发，也请转发，让更多做实战的同道看到这工具与落地细节。

  

往期推荐：

个人观点，仅供参考

内容含AI生成图片，注意甄别

继续滑动看下一个

知界Eden

向上滑动看下一个