---
title: "Chain of agent！单个AI学会\"分身术\"内部开会"
source: "https://mp.weixin.qq.com/s/18fdwINPO3Z8-v1dsbdd4w"
author:
  - "[[Tensorlong 看天下]]"
published:
created: 2025-09-09
description: "❝一句话概括：这篇论文提出了一种颠覆性的\x26quot;智能体链\x26quot;方法，通过多智能体蒸馏技术将一整个专家团队的协作智慧压缩进"
tags:
  - "多智能体蒸馏"
  - "智能体链范式"
  - "效率提升"
abstract: "该论文提出了一种通过多智能体蒸馏技术将专家团队协作智慧压缩进单个模型的颠覆性方法，在保持强大解题能力的同时实现了84.6%的效率提升。"
---
Original Tensorlong 看天下 *2025年09月09日 14:55*

![Image](https://mmbiz.qpic.cn/sz_mmbiz_png/Z24DyenWDNiaib4MuLicqGXbaxw5zh78SLTZkFQETZb3SI1Hmn6LVNFHDPztQySaczNr8LZklhrQaXb2275FCkesg/640?wx_fmt=png&from=appmsg&watermark=1&tp=webp&wxfrom=5&wx_lazy=1#imgIndex=0)

> ❝
> 
> 一句话概括：这篇论文提出了一种颠覆性的"智能体链"方法，通过多智能体蒸馏技术将一整个专家团队的协作智慧压缩进单个模型，既保持了多智能体系统的强大解题能力，又实现了84.6%的效率提升，彻底打破了"能力强就效率低"的AI界铁律。（原论文题目见文末，点击阅读原文可直接跳转至原文链接， Published on arxiv on 06 Aug 2025, by OPPO）

### 第一阶段：识别核心概念

#### 论文的motivation分析

这篇论文旨在解决当前大语言模型（LLM）在处理复杂任务时遇到的一个核心矛盾。

- **一方面，多智能体系统（Multi-Agent Systems, MAS）表现出色** 。想象一个专家团队，里面有研究员、程序员、分析师等，他们各司其职、协同合作，能够解决非常复杂的问题（比如深度研究、软件开发）。现在的多智能体系统就是模拟这个模式，用多个专门的LLM扮演不同角色来协作。但这种方式有几个显著缺点：
- **效率低下** ：智能体之间需要大量的沟通和协调，就像团队开会一样，这消耗了大量的计算资源（token），成本高昂且速度慢。
	- **难以优化** ：整个系统是靠人工设计的复杂工作流和提示词（prompt）粘合起来的，很难像训练一个神经网络那样，通过数据来让整个系统“学习”和“进化”。
	- **泛化能力差** ：遇到一个新的任务领域，往往需要重新设计整套工作流和提示词，非常麻烦。
- **另一方面，单智能体模型（Tool-Integrated Reasoning, TIR）更加高效** 。这类模型（如ReAct框架）就像一个全能的瑞士军刀，一个模型自己就能思考、调用工具（如搜索）、观察结果，然后继续思考。这种方式更加简洁、高效，并且可以端到端地进行训练和优化。但它的问题在于：
- **能力有限** ：它遵循一个相对固定的“思考-行动-观察”循环，难以模拟多专家之间那种动态、复杂的协作模式，在解决真正顶级的复杂问题时显得力不从心。

**论文的动机（Motivation）** 正是要弥合这两者之间的鸿沟： **能否创造出一种新型模型，它拥有多智能体系统的强大协作和问题分解能力，但同时又具备单智能体模型的简洁、高效和端到端可训练的优势？** 作者希望将一个“专家团队”的智慧，内化到一个“超级个体”之中。

#### 论文主要贡献点分析

- **列出论文声称的主要创新点**
- **提出了“智能体链”（Chain-of-Agents, CoA）新范式** ：这是一种新的LLM推理模式，让单个模型能够在内部模拟多智能体的协作过程，动态地激活不同的“角色”（如规划者、搜索者、代码执行者）来解决问题。
	- **开创了多智能体蒸馏（Multi-Agent Distillation）框架** ：这是实现CoA范式的核心技术。它能将一个强大的、外部的多智能体系统（老师）的复杂协作过程，“蒸馏”成可供单个模型（学生）学习的训练数据。
	- **结合了智能体强化学习（Agentic RL）** ：在蒸馏学习的基础上，通过强化学习让模型在实际任务中进行试错和优化，进一步提升其解决问题的能力。
	- \*\*训练并开源了Agent Foundation Models (AFMs)\*\*：基于上述方法，作者训练出了一系列被称为“智能体基础模型”的模型，并在多个Web智能体和代码智能体基准测试中取得了当前最先进（State-of-the-Art）的性能。
	- **全面开源** ：论文不仅开源了模型权重，还包括训练代码、评估代码和所有训练数据，为社区的后续研究提供了坚实的基础。
- **找出支撑这些创新的关键技术或方法**
- **智能体链（CoA）范式** ：其核心是模型生成特殊的“角色”或“工具”标签（如 `<plan>`, `<tools>`, `<reflection>` ），从而在一次推理中实现不同工作阶段的切换，模拟了多智能体的分工。
	- **多智能体蒸馏** ：关键在于 **轨迹转换** 。他们首先让一个强大的多智能体系统（论文中使用OAgents）去解决任务，然后把整个协作过程（谁在什么时候调用了什么工具，得到了什么结果，接下来谁接手）记录下来，转换成一个线性的、包含特殊角色标签的文本序列。这个序列就是CoA的“轨迹”，可以直接用来微调（fine-tune）单个LLM。
	- **智能体强化学习** ：使用了DAPO算法，并设计了针对Web任务和代码/数学任务的奖励函数（Reward Function）。奖励函数的设计非常关键，它引导模型不仅仅是完成任务，还要以正确和高效的方式完成。
- **论文有哪些显著性的结果**
- **性能上的显著提升** ：AFM模型在GAIA、BrowseComp、HLE、AIME25等多个高难度、多样化的基准测试上刷新了记录，大幅超越了之前的单智能体方法。
	- **效率上的巨大优势** ：与传统的多智能体系统相比，AFM在解决同样问题时，token消耗量降低了\*\*84.6%\*\*。这意味着成本和延迟都大大减少，使其更具实用价值。
	- **出色的泛化能力** ：论文中的一个实验非常亮眼，他们发现仅在代码任务上训练的AFM，能够零样本（zero-shot）地理解和使用它从未见过的Web搜索工具。这证明CoA范式和训练方法赋予了模型一种更底层的、通用的工具使用和推理能力。

#### 理解难点识别

- **分析哪些概念/方法是理解论文的关键**
- 理解 **Chain-of-Agents (CoA)** 与传统的Chain-of-Thought (CoT)和ReAct的区别是关键。CoA不是简单的思考链，而是“角色扮演链”或“工作流链”。
	- **多智能体蒸馏** 是本文方法论的核心和最大创新点。理解它如何将一个并行的、分布式的协作过程转换成一个可供学习的线性序列，是理解整篇论文的钥匙。
	- 区分 **SFT（监督微调）阶段** 和 **RL（强化学习）阶段** 的作用。SFT是“模仿学徒”阶段，RL是“实战提升”阶段。
- **找出这些概念中最具挑战性的部分**
- 最具挑战性的部分无疑是 **多智能体蒸馏** 。传统的知识蒸馏是让学生模型模仿教师模型的输出概率分布（软标签），而这里要“蒸馏”的是一个动态的、结构化的、多步骤的 **决策过程** 。如何定义这个过程，如何将其表示为数据，以及如何有效地让学生模型学会这个过程，是其难点所在。
- **确定需要重点解释的核心概念**
- **多智能体蒸馏（Multi-Agent Distillation）** ：这是本文的“魔法”所在，是连接多智能体世界和单智能体模型的桥梁。我们将以此为核心进行深入解释。

#### 概念依赖关系

- **梳理核心概念之间的关系**
- 一个强大的 **多智能体系统（MAS）** 是“知识”的源头，扮演着“老师团队”的角色。
	- **多智能体蒸馏** 是一种创新的“教学方法”，它将“老师团队”的协作过程记录并翻译成一本“学徒手册”（即CoA轨迹数据）。
	- 这本“学徒手册”中记载的推理模式，就是 **智能体链（CoA）范式** 。
	- 单个LLM通过学习这本手册（ **SFT** ），成为了一个初步的\*\*Agent Foundation Model (AFM)\*\*，学会了模仿。
	- 然后，这个初级AFM通过不断的实践和获得奖励（ **Agentic RL** ），最终成长为一个能独立解决复杂问题的专家。
- **确定解释的最佳切入点**
- 我们将从 **多智能体蒸馏** 入手。

### 第二阶段：深入解释核心概念

此阶段重点是引入比喻并将比喻和公式符号联系在一起

#### 设计生活化比喻：从“精英团队”到“全能学徒”

- **选择一个日常场景或者容易理解的活动**
- 想象一家顶级的咨询公司，接到了一个非常棘手的商业项目，比如“分析全球新能源汽车市场未来五年的发展趋势并制定投资策略”。要完成这个项目，需要一个精英团队协作：
- \*\*项目经理 (PM)\*\*：负责分解任务、制定计划。
- \*\*市场研究员 (Researcher)\*\*：负责上网搜索数据、报告和新闻。
- \*\*数据分析师 (Analyst)\*\*：负责处理数据、用Python编程进行建模和可视化。
- \*\*策略师 (Strategist)\*\*：负责整合所有信息，进行批判性思考和反思，最终形成报告。

这个精英团队就是我们的“ **老师** ”——一个强大的\*\*多智能体系统 (MAS)\*\*。

- **用这个比喻来展示核心机制是如何工作的**
- "多智能体蒸馏"就是这位学徒的学习过程。他并不直接去和团队成员一对一学习技能，而是通过一种更聪明的方式： **观摩并记录团队的整个项目流程** 。
- **确保比喻简单且直观，最好是大多数人都熟悉的场景**
- **学徒的记录过程** ：他拿着一个笔记本，从项目启动会开始，一字不漏地记录下完整的工作流程—— **开场** ：客户提出问题："分析市场趋势..."； **PM发言** ： `[PM角色]` "好的，我们的计划是：第一步，收集数据；第二步，分析数据；第三步，形成策略..."； **研究员行动** ： `[研究员角色]` "我正在搜索'2024年全球电动车销量'..." `[得到的结果]` "找到了报告A、新闻B..."； **分析师接手** ： `[分析师角色]` "收到报告A，我需要写一段代码来清洗数据并画图..." `[写代码并执行]` `[得到图表C]` ； **策略师反思** ： `[策略师角色]` "图表C显示亚洲市场增长最快，但这和新闻B的观点有点矛盾。我们需要重新审视数据源的可靠性..."； **流程循环** ：团队可能再次让研究员去查找更权威的数据...； **最终成果** ： `[最终报告]` "根据我们的分析，我们建议..."

学徒的笔记本上，记录下了一条完整的、线性的" **工作日志** "。这本日志就是通过"多智能体蒸馏"得到的 **CoA轨迹数据** 。最后，学徒通过反复研读这本日志，学习在遇到类似问题时，如何 **一个人分饰多角** ，像整个团队那样思考和行动。

#### 建立比喻与实际技术的对应关系

| 比喻中的元素 | 对应的实际技术概念 | 合理性解释 |
| --- | --- | --- |
| **精英咨询团队** | \*\*多智能体系统 (MAS)\*\*，如OAgents | 就像一个团队，MAS由多个专门的智能体（Agent）组成，它们有不同的角色和工具，协同工作解决复杂问题。 |
| **学徒** | **单个基础语言模型**  (如Qwen-2.5-32B) | 这是我们训练的对象，一个“学生”，潜力巨大但尚不具备独立解决复杂任务的能力。 |
| **棘手的商业项目** | **输入给模型的复杂问题 (Query)** | 这是驱动整个系统工作的起点。 |
| **团队成员 (PM、研究员等)** | **专门的角色/工具智能体**  (Plan Agent, Search Agent, Code Agent) | 在CoA范式中，模型通过生成特定标签来“扮演”这些角色。 |
| **学徒的“工作日志”** | **智能体链 (Chain-of-Agents, CoA) 轨迹** | 这就是蒸馏的核心产物。它将多智能体并行的、复杂的协作过程，拍平（flatten）成一个带有角色标识的、线性的文本序列，可直接用于模型训练。 |
| **研读日志并模仿** | **监督微调 (Supervised Fine-Tuning, SFT)** | 学徒（模型）学习预测“工作日志”中的下一步，从而模仿整个团队的决策流程。 |
| **独立完成项目并获反馈** | **智能体强化学习 (Agentic RL)** | 学徒（模型）在SFT学会基本流程后，通过自己动手解决新问题并根据结果的好坏（奖励）来调整和优化自己的策略。 |

#### 深入技术细节

- **从比喻过渡到实际的技术原理**
- CoA轨迹的生成
- **解释相关的数学公式或算法，对于每个数学公式：**
- **例如** ：将 "A\_i = (r\_i - mean(r))/std(r)" 替换为 "某个尝试的优势值 = (这次尝试的奖励 - 所有尝试的平均奖励)/所有尝试奖励的波动程度"
	- **说明技术实现中的关键步骤** ：这里的 对应我们比喻中的一个日志条目，例如——: "当前已经拿到了市场报告A和新闻B，计划进入数据分析阶段."；: `[数据分析师角色]` (即Code Agent)；: "代码执行后，生成了图表C。"
	- **模型如何学习这个轨迹 (SFT) **：学徒（模型）学习的目标是，给定日志的前面部分，能准确预测下一步团队会做什么。** 原始数学形式** (公式 7):$$\\mathcal{L}\_{\\text{SFT}} = - \\sum\_{t \\notin \\mathcal{O}} \\log \\pi\_\\theta(\\tau\_t|\\tau\_{<t}, q)="" $$="" 符号替换版本:
- **然后提供一个符号替换版本** ，用自然语言替换数学符号：
- **首先给出原始数学形式** ：(公式 5):

**关键点解释** ：

- : 这代表模型 在看到初始问题 和之前的所有日志 的情况下，预测出第 步的正确内容 的概率。学习的目标就是最大化这个概率，也就是最小化它的负对数。
- : 这个小细节非常重要， 代表所有“观察结果”(Observation)的集合。这意味着在训练时，模型 **不需要** 去预测工具返回的具体内容（比如网页的全部HTML代码）。这对应比喻中，学徒需要学习的是“研究员应该去搜索”，而不是背下来搜索结果的具体每一个字。这能让模型专注于学习 **决策逻辑** ，而不是记忆事实，从而防止“被环境噪声干扰”。

#### 将技术细节与比喻相互映射

| 技术细节 | 在比喻中的体现 |
| --- | --- |
| **将MAS的并行协作过程线性化为CoA轨迹** | 学徒将团队成员（他们可能同时在工作）的行为，按照逻辑顺序整理成一本 **先后有序** 的“工作日志”。 |
| **特殊标签，如 `<plan>`, `<tools>`, `<reflection>`** | 学徒在日志中用 **角色标签** `[PM角色]`, `[研究员角色]` 等来标注每一步是谁在主导。这让单一的文本流有了结构，保留了多角色的信息。 |
| **公式7中的“观察屏蔽” ()** | 学徒在学习时，重点是模仿团队的 **思考、计划和行动指令** ，而不是去死记硬背研究员找到的报告全文。他知道关键是学会“如何找”，而不是“找到了什么”。 |
| **SFT训练过程** | 学徒一遍遍地“复盘”工作日志，盖住后面的内容，练习预测“接下来PM会说什么？”或者“研究员会用什么关键词搜索？”，直到他能熟练地预测整个流程。 |

- **指出比喻的局限性（如果有的话）**
- 这个比喻非常贴切，但有一个微小的区别。在现实中，学徒学习后可能会形成自己独特的风格。而在SFT阶段，模型的目标是 **精确地复制** “老师团队”的行为模式。只有在后续的RL阶段，模型才开始基于奖励信号发展出可能超越老师的策略。

#### 总结

- **重申比喻与实际技术的核心联系**
- **多智能体蒸馏** 的本质，就是把一个专家团队（MAS）解决问题的 **完整协作流程** ，翻译成一本图文并茂、条理清晰的“ **学徒教科书** ”（CoA轨迹）。
- **强调这种对应关系如何帮助理解整个概念**
- 这个比喻让我们清晰地看到，模型学习的不是零散的知识或技能，而是一套 **结构化的、包含角色分工的、端到端的问题解决方法论** 。
- **用比喻来总结最关键的数学原理**
- 公式7 `LSFT` 的核心思想，用比喻来说就是：“学徒，你要学会预测团队的下一步 **决策** ，而不是背诵他们找到的 **资料** 。” 这就是整个蒸馏过程的精髓所在。

### 第三阶段：详细说明流程步骤

#### 流程详解：AFM的“单人团队协作”

**输入** ：一个复杂的用户请求（Query），例如：“请分析全球人口变化...并提供图表。”

**第一步：内部规划与角色激活（扮演项目经理 PM）**

1. **接收与理解** ：AFM首先接收到完整的用户请求。
2. **生成思考（ `<think>` ）** ：模型内部生成一段“思考”文本（这部分通常不会直接展示给用户）。它会分析任务需求：“好的，这个任务需要数据、分析和可视化。我需要先找到权威的人口数据，然后用代码处理这些数据，最后生成图表并整理成报告。”
3. **制定计划（ `<plan>` ）** ：基于思考，模型会生成一个结构化的计划，就像项目经理的任务分解。它会生成类似这样的内容： `<plan>`
1. 使用网络搜索找到权威的人口数据来源。
	2. 抓取相关网页内容。
	3. 使用代码解释器提取和处理关键数据。
	4. 使用代码解释器生成可视化图表。
	5. 整合信息，撰写最终报告。 `</plan>` 这个计划成为了后续所有行动的蓝图。

**第二步：信息搜集（扮演市场研究员）**

1. **激活工具（ `<tools>` ）** ：根据计划的第一步，模型判断需要使用搜索工具。它会生成一个工具调用指令，格式为： `<tools><web_search>全球人口 2024 统计数据</web_search></tools>` 。
2. **外部执行与结果返回** ：系统后台的 **工具执行器** 会捕捉到这个指令，并调用一个真实的搜索引擎（如Google Search API）执行搜索；搜索引擎返回的结果（通常是标题、链接和摘要列表）被格式化后，作为一个新的信息块输入回模型的上下文（Context）中。
3. **生成观察（ `<observation>` ）** ：模型接收到搜索结果，并将其包裹在 `<observation>` 标签内。这标志着一次"行动-观察"循环的完成。现在，模型的上下文中包含了原始问题、计划以及第一步的搜索结果。

**第三步：深度信息提取与反思（继续扮演研究员，并加入策略师的思考）**

1. **再次思考（ `<think>` ）** ：模型分析观察到的搜索结果，发现某个联合国网站的链接（比如 `population.un.org/...`）看起来最权威。它决定深入探索这个链接。
2. **激活新工具（ `<tools>` ）** ：模型生成抓取网页内容的指令： `<tools><crawl_page>https://population.un.org/...</crawl_page></tools>` 。
3. **执行与观察** ：与上一步类似，后台工具执行器抓取该网页的文本内容，并将其返回给模型。模型再次用 `<observation>` 标签包裹这些新信息。
4. **进行反思（ `<reflection>` ）** ：此时，模型可能扮演“策略师”角色进行反思。它可能会生成： `<reflection>` 网页内容非常详细，但数据是原始表格形式，直接阅读和总结很困难。最好的方法是利用我的代码能力来处理这些数据。原计划的第三步是正确的，现在开始执行。 `</reflection>` 这个反思过程是CoA范式区别于简单ReAct的关键，它允许模型进行动态的自我评估和策略调整。

**第四步：数据处理与可视化（扮演数据分析师）**

1. **激活代码工具（ `<tools>` ）** ：模型判断需要写代码。它会生成一个包含Python代码的指令： `<tools><code_interpreter>`
	```
	import pandas as pd
	import matplotlib.pyplot as plt
	# (...此处省略了从观察到的文本中解析数据并创建DataFrame的代码...)
	data = {"year": [2023, 2024], "population": [8.0, 8.1]}
	df = pd.DataFrame(data)
	# (...生成图表的代码...)
	plt.figure()
	plt.plot(df['year'], df['population'])
	plt.title('Global Population Growth')
	plt.savefig('population_chart.png')
	print("图表已生成并保存为 population_chart.png")
	```
	`</code_interpreter></tools>`
2. **沙箱执行与观察** ：这段Python代码在一个安全的 **沙箱环境** 中被执行；代码的输出（包括 `print` 语句的内容和任何生成的文件路径或图像数据）被捕获，并作为观察结果返回给模型。
3. **接收观察结果** ：模型得到观察结果，如："图表已生成并保存为 population\_chart.png"。

**第五步：整合与生成最终答案（扮演策略师和报告撰写人）**

1. **最终思考（ `<think>` ）** ：模型审视整个过程的上下文信息：原始问题、计划、搜索结果、网页内容、代码和图表生成确认。它判断所有任务都已完成，信息已经足够。
2. **生成答案（ `<answer>` ）** ：模型整合所有信息，生成最终的、格式化的报告，并将其包裹在 `<answer>` 标签中。 `<answer>` **全球人口变化年度报告**
	根据联合国最新数据，全球人口在过去一年中持续增长...
	**关键统计数据:**
	**可视化图表:**(此处可能会嵌入图表的描述或链接)
	总结...`</answer>`
- 2023年人口：80亿
	- 2024年人口：81亿

**输出** ：系统提取 `<answer>` 标签内的内容，作为最终结果呈现给用户。

#### 具体流程伪代码

通过这个流程，我们可以看到，AFM通过在一个统一的、连贯的推理链中生成不同的 **结构化标签** （ `<plan>`, `<tools>`, `<code>`, `<reflection>`, `<answer>` ），成功地 **模拟** 了一个多角色团队的协作过程。每个标签的生成都代表着一次“角色切换”或“工作模式切换”，整个过程高效、连贯，并且完全由一个模型驱动，从而实现了“ **一个模型，一个团队** ”的强大能力。

### 第四阶段：实验设计与验证分析

#### 1\. 主实验设计解读：核心论点的验证

- **核心主张** ：
- AFM在复杂的Web智能体和代码/数学智能体任务上，性能超越了现有的工具集成推理（TIR）方法。
	- AFM作为一个端到端的单模型，其性能可以媲美甚至超越那些复杂、低效的传统多智能体系统（MAS）。
- **实验设计分析** ：
- **数据集选择** ： **Web智能体任务** ——作者选择了 **MHQA** （多跳问答，如NQ, HotpotQA）作为基础能力测试，更重要的是，他们采用了三个极具挑战性的前沿基准： **GAIA** （通用AI助手，需要多步推理和工具编排）、 **BrowseComp** （高难度网页导航和信息提取）和 **HLE** （人类前沿知识问答）； **合理性** ——这个选择非常明智。MHQA是"基本盘"，而GAIA、BrowseComp和HLE是真正的"试金石"，它们能有效地区分出模型的上限，测试其在真实、复杂、开放场景下的规划和执行能力。
	- **评价指标** ：主要使用了 **Pass@1** （一次成功率）和 **avg@16** （对于小样本数学题，通过多次采样取平均来降低随机性）； **合理性** ——这些都是相应领域的标准评价指标。Pass@1直接衡量了模型在一次尝试中解决问题的能力，这在实际应用中至关重要。avg@16则是对小样本、高难度任务更稳健的评估方式。
	- **基线方法选择** ： **直接推理模型** ——如Qwen2.5-32B-Instruct，用于展示基础模型的原始能力，作为性能下限； **工具集成方法 (TIR) **——涵盖了从经典的Search-R1到最新的WebThinker, ReTool, SimpleTIR等一系列SOTA模型。这确保了比较的公平性和先进性；** 多智能体框架 (MAS) **——选择了如OAgents和OWL（基于GPT-4.1）这样的顶级框架，用于证明AFM在性能上可以与这些"重量级选手"一较高下；** 合理性** ——基线选择非常全面，覆盖了从"无工具"到"简单工具"再到"复杂多智能体"的整个技术光谱，使得AFM的性能优势能够被清晰地定位和凸显。
- **主实验结果与结论** ：
- 在 **Web任务** （Table 7），AFM-RL在GAIA、WebWalker、BrowseComp和HLE上均取得了SOTA成绩。例如，在GAIA上达到55.3%，显著高于其他开源TIR模型。这直接证明了AFM在复杂Web推理上的优越性。
	- 在 **数学和代码任务** （Table 11, 12），AFM-RL同样表现出色。在AIME25数学竞赛上，它比之前的最佳模型ReTool-32B提升了超过10个百分点，在LiveCodeBench v5代码生成任务上达到了47.9%的Pass@1，性能提升显著。
	- **结论** ：主实验的结果强有力地支撑了论文的核心论点。通过在多个领域的权威基准上与最强的基线进行对比，作者证明了AFM不仅在单一领域表现出色，而且具有跨领域的强大问题解决能力。

#### 2\. 消融实验分析：内部组件的贡献

- **关键模块验证** ：
- \*\*多智能体蒸馏 (SFT阶段)\*\*：这个模块的有效性如何？
	- \*\*智能体强化学习 (RL阶段)\*\*：在SFT的基础上，RL带来了多大的提升？
- **实验设计与结果分析** ：
- **SFT vs. RL** ：在几乎所有的结果表（如Table 6, 7, 11）中，作者都同时列出了 **AFM-SFT** 和 **AFM-RL** 的性能。 `AFM-SFT` 可以看作是只经过多智能体蒸馏模仿学习的模型，而 `AFM-RL` 则是在此基础上增加了强化学习优化的完整模型。
	- **定量证明** ：在数学任务上（正文4.2.2节），作者明确指出，对于7B模型，SFT阶段带来了22.0%的平均准确率提升，而RL阶段在SFT的基础上又带来了\*\*20.8% **的额外提升；在Web任务GAIA上（Table 7），Qwen-2.5-32B-Instruct作为基础，AFM-SFT将其得分从约31.1%提升至50.5%，而AFM-RL进一步提升至** 55.3%\*\*。
	- **结论** ：这一系列的对比清晰地证明了：1) **多智能体蒸馏（SFT）是成功的** ，它为模型注入了强大的基础规划和协作能力，是性能大幅提升的基石。2) **智能体强化学习（RL）是必要的** ，它在模仿学习的基础上，通过与环境的真实交互，进一步优化了策略，实现了性能的“最后一公里”冲刺。两个模块缺一不可，共同构成了AFM的成功。

#### 3\. 深度/创新性实验剖析：洞察方法的内在特性

- **实验一：计算效率分析 (Case Study & Efficiency)**
- **目的** ：证明AFM不仅性能高，而且效率远超传统MAS。
	- **设计** ：在GAIA数据集上随机抽取样本，对比AFM、WebThinker（TIR代表）和OAgents（MAS代表）在完成任务时的 **总token消耗量** 和 **工具调用次数** （见Figure 5）。
	- **结论** ：结果令人震惊。AFM的token消耗（约24k）远低于WebThinker（约56k），更是碾压了OAgents（约156k）。这定量地证明了AFM将协作过程“内化”后，避免了MAS大量的中间通信开销， **将性能和效率的矛盾完美统一** 。这是AFM实用性的关键证明。
- **实验二：对未见工具的泛化能力测试 (Generalization)**
- **目的** ：探究AFM学到的是否是一种通用的“智能体”能力，而不是对特定工具的死记硬背。
	- **设计** ：这是一个非常精彩的“交叉验证”实验，具体做法是将 **代码智能体** （只见过Python解释器）直接用于需要 **Web搜索** 的GAIA任务；以及将 **Web智能体** （只见过搜索和爬虫）直接用于需要 **Python解释器** 的任务。
	- **结论** ： **代码智能体成功地泛化到了Web搜索任务** ，它能正确理解并调用从未见过的搜索工具。然而， **Web智能体在调用Python解释器时失败了** ，因为它无法满足代码工具严格的格式要求。这个实验得出了一个深刻的洞见： **在具有严格语法和结构化执行逻辑的工具（如代码解释器）上进行训练，能让模型学到更强的、更具泛化性的工具使用能力** 。
- **实验三：测试时扩展能力分析 (Test-Time Scaling)**
- **目的** ：验证AFM是否能通过测试时增加计算量（如多次生成取最优）来进一步提升性能，并与其它模型进行对比。
	- **设计** ：在GAIA等基准上，比较了AFM的Pass@1性能，以及两种扩展策略下的性能： **AFM-Bo3** （生成3次，用一个强大的判别模型选出最佳答案）和 **AFM-Pass@3** （生成3次，只要有1次正确就算成功）。
	- **结论** ：AFM从这些扩展策略中获益巨大（见Table 13）。在GAIA上，Pass@3策略将性能从55.3%飙升至 **69.9% **，这个提升幅度远大于其他基线模型。这表明AFM生成的多个候选解具有** 多样性和高质量** ，为通过重排或多轮尝试来提升最终性能提供了巨大空间，进一步缩小了与闭源顶尖模型（如Claude-3）的差距。

本文题目：Chain-of-Agents: End-to-End Agent Foundation Models via Multi-Agent Distillation and Agentic RL

**欢迎Deep Learning同好与我交流、讨论、合作！**

**现已提供论文解读和idea讨论定制服务，可私信后台联系**

个人观点，仅供参考

[Read more](https://mp.weixin.qq.com/s/)

继续滑动看下一个

沈公子今天读什么

向上滑动看下一个