---
title: "TPU发展简史 | 万字长文深度解析核心技术"
source: "https://mp.weixin.qq.com/s/tagebIw6TOGe6nX4cEj8TQ"
author:
  - "[[孙沐晏]]"
published:
created: 2025-12-31
description: "2025年凛冬，关于TPU的几条新闻被炒得火热：AI独角兽Anthropic宣布扩大与谷歌云的合作，将在未来数"
tags:
  - "TPU发展"
  - "核心技术"
  - "硬件演进"
  - "系统架构"
abstract: "本文回顾了Google TPU从v1到v7的十年发展历程，深入解析了其核心技术演进、架构创新以及与AI算法发展的协同关系。"
---
Original 孙沐晏 *2025年12月25日 18:05*

![Image](https://mmbiz.qpic.cn/sz_mmbiz_png/Ned01c3wzryG81fXIl6GX8AoibAzcpS28fq8arjVyPINcm1GpiaY3tbSTAibBFWso3bibu8gSmbd0W1f5IZwJha4PA/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1#imgIndex=0)

2025年凛冬，关于TPU的几条新闻被炒得火热：

- AI独角兽Anthropic宣布扩大与谷歌云的合作，将在未来数年内部署100万颗TPU用于训练其下一代Claude模型 。
- Google正在与Meta等巨头谈判，探讨将TPU部.署在客户的数据中心，或者提供某种形式的“私有云”租赁 。
- 谷歌分布式云边缘（Google Distributed Cloud Edge）产品已经开始集成Edge TPU，支持零售、制造等场景的本地化AI推理。这表明谷歌正在尝试将TPU的触角延伸至云边界之外，直接分食NVIDIA在边缘计算市场的份额。

可看到的趋势是，TPU不再只是Google内部的“算盘”，它正在被推向客户的数据中心、云边界与生态中心，成为一桩可交易的基础设施生意。

在英伟达CUDA生态看似牢不可破的今天，Google为何早在十年前就孤注一掷地选择了自研芯片这条寂寞之路？ 从那个 功能纯粹、 仅凭脉动阵列单点突围的 TPU v1，到如今让Meta都垂涎的v7 Ironwood，Google究竟做对了什么？在它的发展历史上，有哪些关键事件和技术创新，又是在什么背景下发生的？

本文旨在回顾这十年间TPU发展的缘起和变革，用图文并茂、深入浅出的语言来硬核解析每一代TPU迭代的关键技术。  

![Image](https://mmbiz.qpic.cn/sz_mmbiz_png/Ned01c3wzrz11ib3b7hTrfquUufowgFHFZQ2I2QDswzwuE2xhFCaTLxX3WGDPib1nIYsFrvRBhXurrkoHxh6VtHA/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1#imgIndex=1)

尽管对于自研芯片的讨论早在2006年就在Google 内部有所讨论，但当时的神经网络只是边缘业务，讨论没有激起什么大的水花。真正让Google下决心开启自研芯片的，是2013年 语音识别团队准备推出的 基于深度神经网络的新一代语音模型。在项目评估时，Jeff Dean进行了 一项著名的“封底计算”（Back-of-the-napkin calculation）：

![Image](https://mmbiz.qpic.cn/sz_mmbiz_png/Ned01c3wzrz11ib3b7hTrfquUufowgFHFuuY6rkCkGDMLBE4JMCFzq0qGsWzCRZ3gPWwvjFYW6M5HrnAHtzIplQ/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1#imgIndex=2)

这一计算将事实摊在了台面上：在语音推理这种“极低延迟 + 海量请求”的规模下，通用CPU的能效与成本曲线开始失控。 Haswell架构的CPU虽然通用，但在执行神经网络推理（本质上是大量的矩阵乘法）时，大量的晶体管和能耗被浪费在了解码、缓存管理和乱序执行逻辑上，而非算术运算本身。要想在不额外支出 数十亿美元CapEx的前提下支持语音业务的发展，必须加强对AI硬件的研发。

当时Google内部形成了三个独立小组，分别代表三种不同的硬件加速路径进行竞争性提案。

方案一是 扩大NVIDIA GPU的采购形成 GPU集群。这个方案 技术成熟，软件栈（CUDA）完善，无需自研风险。但在测试时发现，GPU追求极致的吞吐量和大批次，而在语音搜索的场景中， 批处理的大小必须非常小以实现低延迟。小批次计算时， GPU的吞吐优势难以兑现，能效也不占优 。

方案二 是使用现场可编程门阵列（FPGA）。这一路线在当时被微软大力推崇，其“Project Catapult”项目成功将FPGA部署在Bing搜索服务器中 。它比GPU的延迟低很多，但 其能效比仍远低于专用芯片。同时Jeff Dean认为，10倍于CPU的性能提升，并 不足以一劳永逸地解决“数据中心翻倍”的危机。

方案三则是 从零开始设计专用集成电路（ASIC），这也是TPU的诞生地。团队成员想， 既然神经网络推理99%的计算量都是矩阵乘法和激活函数，为什么不设计一个只做这两件事的芯片？通过剥离CPU/GPU中所有通用的控制逻辑（乱序执行、分支预测、大缓存），ASIC可以将硅片面积几乎全部用于算术逻辑单元。反对的声音认为， 芯片设计周期太长， 如果芯片造出来时，AI算法已经变了，那么这批芯片就是电子垃圾。 关键时刻，Jeff Dean判断：尽管AI模型结构（CNN, LSTM, RNN）在变，但底层的数学原语（线性代数运算）在未来很长一段时间内不会改变。这一判断为ASIC方案扫清了决策障碍。

三种技术路径竞争最终的结果如今已显而易见。对长期主义的坚持，和对AI趋势的正确判断，促成了TPU的诞生。 与通用CPU相比，第一代TPU的设计哲学是 激进的“减法” ：

1、它 抛弃了CPU中用于乱序执行、分支预测、多线程调度的复杂逻辑， 将芯片上绝大多数的晶体管都用于乘累加单元，实现了极致的计算密度以及执行的确定性。  

2、为了在有限的芯片面积和功耗预算下极致提升吞吐量，TPU v1还做出了一个关键的妥协：仅支持8位整数乘法，而非GPU通用的32位浮点。这是因为，v1的设立主要是为了解决推理（而非训练）的难题，而 推理阶段 在许多成熟任务上，经校准或量化训练后，INT8带来的精度损失可控制在很小范围 。在时间短（15个月内需要完成从设计到部署）、任务重的工期下，这种现实主义可以被看做是“长期主义”与“小步快走”的混合产物。

3、虽然在设计上有所妥协，但它的核心创新—— 脉动阵列 —— 则一以贯之地从v1延续到了最新一代v7，可被称为是TPU的“心脏”。 在传统的CPU/GPU（冯·诺依曼架构）中，每次运算都需要从寄存器或缓存中读取数据，计算完再写回，这导致了巨大的读写功耗瓶颈。在 TPU的脉动阵列中，数据一旦从内存读取进入阵列，就会在运算单元之间直接传递并多次复用，大大减少了写回存储器的次数， 使得TPU的MXU（矩阵乘法单元）达到了极高的计算密度，因为它把能量主要聚焦在了“算”上，而不是花在“存取”上。

![Image](https://mp.weixin.qq.com/s/www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg%20stroke='none'%20stroke-width='1'%20fill='none'%20fill-rule='evenodd'%20fill-opacity='0'%3E%3Cg%20transform='translate(-249.000000,%20-126.000000)'%20fill='%23FFFFFF'%3E%3Crect%20x='249'%20y='126'%20width='1'%20height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

TPU项目的成功支撑了Google语音搜索、Google翻译以及后来的AlphaGo的算力需求，而无需翻倍建设数据中心。 更重要的是，它确立了Google在 AI 硬件领域的“系统级思维”： 不要试图用软件去修补硬件的缺陷，要从系统层面重构软硬件的边界。

![Image](https://mp.weixin.qq.com/s/www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg%20stroke='none'%20stroke-width='1'%20fill='none'%20fill-rule='evenodd'%20fill-opacity='0'%3E%3Cg%20transform='translate(-249.000000,%20-126.000000)'%20fill='%23FFFFFF'%3E%3Crect%20x='249'%20y='126'%20width='1'%20height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

随着机器翻译和图像识别模型的复杂度指数级上升，仅用于推理的芯片已无法满足Google Brain的研究需求。研究人员发现，GPU训练与TPU推理之间的转换损耗严重阻碍了迭代速度。Google意识到，必须掌控训练环节。

但TPU v1是专为2013年的“语音搜索算力危机”而特别设计的，它有一些瓶颈导致了它适合推理而不是训练。

怎样的芯片适合推理，怎样的芯片适合训练呢？推理对精确计算的鲁棒性更高， 在很多任务里，适度降低精度不会改变最终类别决策，却能显著换来吞吐与能效 。比如识别一张图片里的猫，只要特征足够显著，即使中间的计算数值从0.75321变成了0.75（精度损失），最终结果依然是“猫”，不会变成“狗”，在精度上可做一定取舍。而且推理时模型的权重已经确定，不需要更新权重。而训练则需要 需要计算梯度来反向更新权重。 这些梯度往往是极小的数值（例如 0.00001）。如果用整数表示，这些微小的变化会直接被“四舍五入”成 0，导致模型学不到任何东西（梯度消失）。权重更新需要不断写入，也需要成千上万个芯片高效协同，同步梯度。

从这里就可以看出， TPU v1在训练端的弱点：不支持浮点运算， 权重写入低效，芯片之间没有高速网络协同机制。TPU v2则针对这些弱点做了改进：

1、新数据格式，浮点数 BF16 的创建。  

浮点数类似于科学计数法（例如 1.23 x 10^5 ），由三个部分组成：符号位（正负）、指数位（表示数值的大小范围，10^5里指的是5）和尾数位（表示具体的精度数值，即1.23）。 标准的32位数据格式FP32精度大，但占用的 内存和带宽也很大。 半精度浮点数FP16占内存小，但 指数位只有5位， 这对于深度学习训练中经常出现的梯度累加来说，范围太窄了。Google团队创造了一个折中的格式：BF16。三者的结构对比如下：

FP32: \[符号: 1\]\[指数: 8\]\[尾数: 23\] (总共32位)

FP16: \[符号: 1\]\[指数: 5\]\[尾数: 10\] (总共16位，范围太小)

BF16: \[符号: 1\]\[指数: 8\]\[尾数: 7\] (总共16位，范围与FP32完全相同)

观察可以发现，BF16和FP32的指数位相同，即它能表示的范围与 FP32完全相同，只是把尾数暴力截断，牺牲了精度。而 深度学习对动态范围极度敏感（需要防止梯度爆炸或消失），但对精度非常宽容，而且 对于神经网络来说，这只是相当于给图像加了一点点噪点，反而有助于模型的泛化，防止过拟合 。这一特点，使 BF16非常适合进行训练，既 拥有和FP32一样的动态范围，却只有一半的内存占用。 这一格式成为了TPU v2的核心特性， 后来被业界广泛支持，成为训练常用低精度格式之一 。

2、创建 Pod集群架构， 引入ICI (Inter-Chip Interconnect)。

在引入ICI之前，芯片之前如果要交流数据， 必须先把数据发回CPU内存，CPU 再通过慢速的以太网发给另一台机器的CPU，再传给那边的TPU，这个过程是训练的极大瓶颈。  

ICI 不再将TPU视为独立的加速卡，而是将其设计为大规模并行计算机的节点。通信 接口直接集成在了TPU芯片上，芯片A的计算单元可以直接将数据写入ICI接口，通过专用线缆直达芯片B的ICI接口， 绕过了主机CPU和操作系统的干预。 256个TPU芯片可以直接互联构建成一个 二维环面 的 TPU Pod，形成一个算力高达11.5PFLOPS的超级计算机。 这使得TPU不再是独立的加速卡，而是可以组成成百上千个芯片的超级计算机（Pod）。

听起来不明觉厉是不是？形象点来说，它的算力很高， 如果让全地球70亿人每人拿一个计算器，每秒钟算一次乘法，我们需要19天不眠不休才能完成这个TPU Pod一秒钟的工作量。而二维环面则说明，任何两个芯片之间的距离都非常近。你可以理解成，最后一排的芯片想跟第一排芯片传递信息，就像 第一排芯片向 第二排芯片 传递信息一样快。 数据不需要绕远路，就像在一个圆圈表面上跑，永远是近路。

![Image](https://mp.weixin.qq.com/s/www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg%20stroke='none'%20stroke-width='1'%20fill='none'%20fill-rule='evenodd'%20fill-opacity='0'%3E%3Cg%20transform='translate(-249.000000,%20-126.000000)'%20fill='%23FFFFFF'%3E%3Crect%20x='249'%20y='126'%20width='1'%20height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E) ![Image](https://mp.weixin.qq.com/s/www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg%20stroke='none'%20stroke-width='1'%20fill='none'%20fill-rule='evenodd'%20fill-opacity='0'%3E%3Cg%20transform='translate(-249.000000,%20-126.000000)'%20fill='%23FFFFFF'%3E%3Crect%20x='249'%20y='126'%20width='1'%20height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

暂时解决了硬件问题，算法的瓶颈随即显现。

在 2017 年之前，自然语言处理（NLP）的主流架构是RNN（循环神经网络）和LSTM（长短期记忆网络）。这些模型在处理语言时，必须严格按照时间顺序进行：读取第一个字，生成隐状态，传递给第二个字，再读取第二个字……

这种时序依赖性是并行计算的死敌。哪怕你拥有10000块TPU，如果算法要求必须“做完第一步才能做第二步”，那么剩下的9999块TPU就只能在旁边闲置。这在系统工程中被称为“阿姆达尔定律”（Amdahl's Law）的诅咒。

2017 年，Google团队发表了那篇改变AI历史进程的论文——《Attention Is All You Need》。这篇论文提出了Transformer架构。它做了一件违反直觉的事情：它抛弃了循环（RNN）和卷积（CNN），只保留了“注意力机制”（Attention）。

Transformer的核心突破在于完全的并行化。它不再一个字一个字地读，而是一眼看尽整篇文章。通过“自注意力机制”（Self-Attention），模型可以同时计算句子中所有词之间的关系。每一个词都能“看到”其他所有词，无论它们相距多远。

这对硬件意味着什么？这意味着TPU的脉动阵列终于可以 长期处于高利用率状态 了 ！ Transformer 的矩阵运算特性与TPU的硬件特性完美契合。但是，这种全新的算法也对硬件提出了新的要求。当 参数增长远快于单卡显存增长 ，数据搬运成为了主要的性能扼杀者 。 当一个模型被切分到4096个芯片上分布式运行时，连接这些芯片的网络就成了系统的“阿喀琉斯之踵”。 如果互连网络速度慢或者拓扑结构僵化， 芯片将花费更多的时间等待数据而非进行计算 。

也许你要问了，前文提到的 ICI不就已经解决了带宽的问题吗？ 随着模型从ResNet-50（2500万参数）演进到BERT（3.4亿）和GPT-3（1750 亿），2D基础的裂痕开始显现，而对于 下一代AI架构的展望 ，致使ICI中的 二维环面架构，也称为了通信的瓶颈。

2021 年，Jeff Dean发表了一篇具有里程碑意义的博客文章：《Introducing Pathways: A next-generation AI architecture》（介绍 Pathways：下一代 AI 架构）。在Dean的构想中， Pathways不是一个单一的模型，而是一个能够处理多种任务、多种模态（视觉、听觉、语言），并且能够高效学习的系统架构。它是Google对未来 AI 基础设施的终极定义。

Pathways中创新性地提出了 稀疏激活与混合专家（MoE）。 人类大脑有千亿个神经元，但在任何一个时刻，只有一小部分在放电。如果你在看书，负责听觉的神经元可能就在休息。相比之下，传统的深度学习模型是“稠密”的。无论你问它“1+1等于几”还是“解释量子力学”，它都会激活整个神经网络的所有参数。这是一种极大的算力浪费。

Pathways采用了混合专家模型： 模型被切分成数千个小的“专家”网络，每个专家擅长处理不同类型的信息。 有一个“门控网络”，负责判断输入的数据应该交给哪个专家处理。 虽然模型的总参数量可能达到万亿级别，但对于每一个具体的输入，可能只需要激活其中10%甚至更少的参数。

这个新的AI架构，对硬件提出了新的要求。因为 成千上万的“专家” 是分布在不同的TPU芯片上的，有可能芯片A、B、C要发Token给芯片D，而芯片D、E、F要发Token给芯片A、B…… 这就形成了一个极其复杂的 全互联模式（ All-to-All ） ：每个芯片既是发送者也是接收者，且发送目标 取决于输入数据的内容， 是动态变化的。与之相比，稠密模型All-Reduce（全归约）的训练，数据有确定的路由路径。二维环面这时候也不够用了，随着通信需求的平方增长，供给宽带只有平方根增长， 对剖带宽（Bisection Bandwidth）遇到了瓶颈 。  

![Image](https://mp.weixin.qq.com/s/www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg%20stroke='none'%20stroke-width='1'%20fill='none'%20fill-rule='evenodd'%20fill-opacity='0'%3E%3Cg%20transform='translate(-249.000000,%20-126.000000)'%20fill='%23FFFFFF'%3E%3Crect%20x='249'%20y='126'%20width='1'%20height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E) ![Image](https://mp.weixin.qq.com/s/www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg%20stroke='none'%20stroke-width='1'%20fill='none'%20fill-rule='evenodd'%20fill-opacity='0'%3E%3Cg%20transform='translate(-249.000000,%20-126.000000)'%20fill='%23FFFFFF'%3E%3Crect%20x='249'%20y='126'%20width='1'%20height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

在同一时期， Google 发现其核心业务推荐系统，正在面临一种截然不同的算力瓶颈，即嵌入层（Embedding Layer）的”查表”效率问题 。推荐系统工作流程中有两项重要的操作：查表和计算。计算由TPU中的MXU（ 矩阵乘法单元 ）进行，这也是MXU擅长的工作。而在计算之前，还有“查表”这一动作，逐渐成为了低效率的瓶颈。 在推荐系统里，假设输入是“用户ID”或“视频ID”，你没法直接把“用户ID 10086”乘以“视频ID 9527”。计算机需要把这些干巴巴的ID，转换成一组能代表它们特征的数学向量。嵌入表查找就是这样一个过程，根据输入的ID，从内存中拿出 特征向量。这个操作并不是计算，而是一种 聚合分散式内存访问（ small gather/scatter memory accesses ） ，由MXU外的其他单元进行操作。它经历了几次迭代：  

1、一开始，这个操作是从CPU执行的。由于CPU的内存读取比TPU慢，同时，一台CPU需要为4颗TPU芯片提供查找操作，每颗芯片的MXU在很多时间都在闲置等待CPU的数据搬运。这就导致了极大的资源浪费。

2、TPU v2时代，为了针对这个问题进行改进，把这个工作交给了TPU上的VPU（向量处理单元）来做。这个单元 的主业是做一些MXU职责之外的计算，比如 激活函数、 归一化等 ， 兼职查表。也就是说，它做查表和计算这两者是有资源抢夺的。MXU等待的时间虽然比之前少了一些，但还是得等很长时间，不能最大限度发挥出计算的优势。

3、在TPU v4时代，针对查表问题的瓶颈，SparseCore（ 稀疏核心 ）应运而生了。 SparseCore是TPU 芯片上的一个功能区， 直连 TPU的高带宽存储器（HBM）， 配备了专用的“获取单元”，实现了极致的并发和查表加速。现在，各部门职能分布很明确——CPU负责处理输入， SparseCore 负责查表，MXU负责计算。MXU终于可以日以继夜、孜孜不倦地计算了。  

但这其实也带来了一个问题——算力解放后的通信风暴。 现代推荐系统的嵌入表规模动辄包含数千亿甚至上万亿个参数，数据量轻松达到TB级别。而单颗 TPU 芯片的HBM（高带宽内存）容量通常只有几十GB，根本无法独自吞下如此巨大的模型。因此，系统不得不采用“模型并行”策略，将这张巨大的表切分成多个分区（Sharding），均匀地散布在成百上千颗TPU 芯片组成的集群内存中。 当SparseCore接收到一批ID进行查表时，这些ID是完全随机分布的。在一个拥有 4096 颗芯片的集群中，特定ID所对应的数据恰好存储在“当前这颗芯片”上的概率，理论上仅为 1/4096。这意味着，每一次查表操作，SparseCore很大概率在寻找存储在另外一颗TPU芯片里的数据。 这意味着， SparseCore查表越快，网络上瞬间涌现的数据包就越密集，这对芯片间的通信带宽提出了前所未有的严苛要求。  

这种流量模式是不是很熟悉？是的，这就是 Pathways架构里提到的全互联模式——每一个芯片都要去访问其他所有芯片，同时每一个芯片也要响应来自其他所有芯片的访问。

也许你也发现了，在TPU的发展的逻辑里，业务、软件、硬件、架构的发展是相辅相成的。语音搜索业务的瓶颈催生了TPU v1，对训练的需求催生了TPU v2，而 SparseCore的诞生和Pathways的架构，使得瓶颈由 “计算受限”转向了“通信受限”。 不论是 SparseCore的 跨芯片请求 还是 Pathways中的混合专家模式，都是 All-to-All（全互联）的流量模式，TPU亟待一场更大的技术革新来解决这个问题。

![Image](https://mp.weixin.qq.com/s/www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg%20stroke='none'%20stroke-width='1'%20fill='none'%20fill-rule='evenodd'%20fill-opacity='0'%3E%3Cg%20transform='translate(-249.000000,%20-126.000000)'%20fill='%23FFFFFF'%3E%3Crect%20x='249'%20y='126'%20width='1'%20height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

就是在这个背景下，硬件不得不进行更新来应对新架构提出的新挑战。TPU v4对此进行了重大创新：引入了 OCS （ Optical Circuit Switch ）， 取代了传统机架间通信层中的电气分组交换机，并以 可重构的 3D环面 为基础拓扑结构。

![Image](https://mp.weixin.qq.com/s/www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg%20stroke='none'%20stroke-width='1'%20fill='none'%20fill-rule='evenodd'%20fill-opacity='0'%3E%3Cg%20transform='translate(-249.000000,%20-126.000000)'%20fill='%23FFFFFF'%3E%3Crect%20x='249'%20y='126'%20width='1'%20height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

在3D环面中，你像走进了一个上下通、左右通、前后通的房间。 在TPU v4中， 基础构建模块是立方体，由4x4x4=64个芯片组成。 立方体 内部使用短距离铜缆互连，形成一个坚固的电气连接单元。在此之上，为了满足多对多的需求，TPU v4还进行了数学上的创新—— 扭曲环面 。 在标准的3D环面中，连接是“对齐”的。例如，在一个4x4x4的长方体排列中，Z轴方向的第4个立方体的输出会直接连回第一个立方体的对应位置(x,y,1)。这就像乘电梯一样，你从五楼“瞬移”到了一楼，但在x和y轴上的坐标没变。而扭曲环面则在连接环面的回环链路时，引入了一个坐标偏移。这就像哈利波特电影中的螺旋楼梯一样， 你每上一层楼同时也发生了水平位移。 这种结构使得数据包在传输时能更快地“扩散”到整个网络的 X、Y 平面，就像搅拌奶茶一样，混合得更均匀。 在4x4x8的配置下，扭曲环面比标准环面提供了高出 70%的理论二分带宽， 使流量在所有链路上分布更均匀。

然而，要支持这样的3D环面网络，在布线上充满了挑战。早在TPU v3时期的二维环面布线中，虽然 机架内的连接很短，但行与行之间的连接，尤其是“环绕链路”，就需要跨越整排机架的长度， 部分环绕链路的长度已经超过了铜缆的极限。 在3D环面网络中，芯片规模成倍增长，又要 将逻辑上的 3D 立方体结构映射到物理上的 2D 机架排列，这 必然会导致大量的连接需要跨越极远的物理距离。 不仅是 X 轴和 Y 轴的环绕，Z 轴的连接在物理展开后也会变成长距离连接。此时，长距离链路不再是二维时期少数的“例外”，而是成为了系统的“常态”。再加上扭曲环面的创新，使得 连接关系更加复杂，线缆有时需要对角线跨越。这种长距离的铜线连接，不仅超越了物理极限，还会导致各种散热、维护问题。  

![Image](https://mp.weixin.qq.com/s/www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg%20stroke='none'%20stroke-width='1'%20fill='none'%20fill-rule='evenodd'%20fill-opacity='0'%3E%3Cg%20transform='translate(-249.000000,%20-126.000000)'%20fill='%23FFFFFF'%3E%3Crect%20x='249'%20y='126'%20width='1'%20height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

铜线的极限承载遇到了挑战。 突破的关键点，落在了几年前就开始的Apollo项目上。 虽然TPU v1在2015年才开始部署，但是 导致网络崩溃的流量模式——大规模、突发性、全对全（All-to-All）通信——在2013年已经由两大巨头驱动：MapReduce和DistBelief。在MapReduce中，Shuffle阶段需要将所有Map节点产生的中间数据，根据Key重新分发给所有Reduce节点。在 DistBelief系统中， 每一轮训练结束时，所有计算节点都要把梯度发给参数服务器，然后从参数服务器拉取新的权重。正是这些超量计算的浪潮促使了Apollo项目的建立， 它的初衷并 不是为了解决 TPU 的瓶颈，而是为了解决“计算”本身的瓶颈。

Apollo项目的 核心成果是代号为Palomar的光路交换机，它逐渐取代了传统的 电子交换机 。它的 的核心技术是 微机电系统 （MEMS, Micro-Electro-Mechanical Systems）。 MEMS 芯片的制作工艺跟集成电路的芯片相似，但不同之处在于，它的芯片 里有会动的机械零件（齿轮、弹簧、镜子）。 如果说标准的计算机芯片是处理信息的“大脑”，那么MEMS芯片就是拥有“感官”的系统——它能够感知物理世界并与之互动，应用场景很广泛，比如加速计、螺传感器、压力传感器、麦克风等。

![Image](https://mp.weixin.qq.com/s/www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg%20stroke='none'%20stroke-width='1'%20fill='none'%20fill-rule='evenodd'%20fill-opacity='0'%3E%3Cg%20transform='translate(-249.000000,%20-126.000000)'%20fill='%23FFFFFF'%3E%3Crect%20x='249'%20y='126'%20width='1'%20height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

在实现细节上，早期的 光开关尝试过2D MEMS技术， 镜子只能做简单的“弹起”或“躺下”动作（二进制状态）。光路在平面上通过交叉点，如果镜子弹起，光就反射一个角度；如果不弹起，光就直行。它的局限性很明显， 所需的镜面数量与端口数的平方成正比。构建一个1000x1000的开关需要一百万个微镜面，这在芯片面积和良率上都是不可能实现的。

![Image](https://mp.weixin.qq.com/s/www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg%20stroke='none'%20stroke-width='1'%20fill='none'%20fill-rule='evenodd'%20fill-opacity='0'%3E%3Cg%20transform='translate(-249.000000,%20-126.000000)'%20fill='%23FFFFFF'%3E%3Crect%20x='249'%20y='126'%20width='1'%20height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

Google采用了3D MEMS结构。这就像 把镜子装在了一个能360度转动的云台上， 可以在三维空间里任意调整角度。它可以向左偏1度，也可以向右偏5.23度，或者向上抬0.1度。 这种架构彻底解耦了镜面数量与端口数的平方关系。  

但是，光有这样一面镜子还不能将光从一个端口传输到另一个端口，光信号的完整路径呈现“Z形”，其中每组镜面都有不同的功能。如图所示，想象你要将一束光从左墙打到右墙：  

![Image](https://mp.weixin.qq.com/s/www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg%20stroke='none'%20stroke-width='1'%20fill='none'%20fill-rule='evenodd'%20fill-opacity='0'%3E%3Cg%20transform='translate(-249.000000,%20-126.000000)'%20fill='%23FFFFFF'%3E%3Crect%20x='249'%20y='126'%20width='1'%20height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

1、当光线从输入光纤输出时，光线会迅速发散。这就像花洒喷水的时候，水花溅几米就没了。因此，需要装一个准直器阵列，用来将 发散的光锥转化为平行光束（高斯光束）。这就像给花洒 装了一个强力喷嘴，把原本四散的水雾，聚成了一束笔直的水柱。现在，我们有了一束能飞很远的光柱。

2、这束光打到了MEMS阵列上的镜面。 该镜面进行了极其精细（ 毫弧度甚至 微弧度级别 ）的倾斜，将光束精确瞄准第下一个阵列中的目标镜面。

3、光达到了下一个MEMS阵列。看到这里你可能会疑惑，不是说MEMS中的镜面可以旋转任意角度吗？何必要两块镜子呢？这是因为， 光纤接收口芯径极细。可以想象成它是一根非常细的吸管，你可以斜着往吸管喷水，但 水只会打在吸管壁上飞溅出来，根本流不进去。 水必须完全平行于吸管的方向，正对着喷进去，才能顺畅传输。

这就是需要两组MEMS镜面的原因。 第一个镜面控制光束落在第二个镜面的位置，第二个镜面调整自身角度，以补偿入射角，确保光束与输出准直器的光轴完美平行。

4、在输出光纤之前，还有一块透镜，将 平行光束重新聚焦为一个极小的光点（直径约9微米）， 塞进那根比头发丝还细的光纤芯里。

OCS不仅实现了 数据的全光路由，无需进行任何光电转换，解决了数据传输的瓶颈，还依据其物理灵活性实现了 逻辑拓扑与物理位置的解耦，使得芯片集群的可维护性大大增强。

在传统的静态布线集群中，可靠性是一个概率学噩梦。假设每个TPU芯片的可用性是99.9%（这已经很高了）。在一个拥有4096个芯片的集群中，所有芯片同时正常的概率是0.999^4096=1.6%。这意味着，在一个静态连接的系统中，绝大多数时候整个系统都是“不完整”的。如果一个芯片坏了，为了保持拓扑完整性（例如环面必须是闭合的），可能需要停用整个机架甚至整个集群来进行维修，或者降级运行在不规则的拓扑上，导致性能暴跌。

而OCS的引入，使系统一旦检测到某个芯片坏了，调度器可以从资源池中随机挑出其它健康的芯片单元，再重新计算路由状态，驱动OCS的微镜，将物理上可能相距甚远的芯片单元们通过光纤连在一起。 这种机制使得 TPU v4集群的 系统可用性 达到了惊人的99.98%，远超传统架构。这也使得长时间的模型训练在芯片故障的情况下无需中断： 调度系统找到一个空闲的健康机架 -> 指令 OCS 将光路从“待维护机架”切换到“备用机架” -> 将任务状态迁移（Checkpoint/Restore） -> 断开旧机架电源，这种能力为 训练周期长达数周的大模型（如PaLM, Gemini）至关重要，因为它们无法承受频繁的中断。

![Image](https://mp.weixin.qq.com/s/www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg%20stroke='none'%20stroke-width='1'%20fill='none'%20fill-rule='evenodd'%20fill-opacity='0'%3E%3Cg%20transform='translate(-249.000000,%20-126.000000)'%20fill='%23FFFFFF'%3E%3Crect%20x='249'%20y='126'%20width='1'%20height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

通过引入光子作为载体，利用 MEMS 这一微观机械奇迹，再辅以拓扑学的数学智慧和软件定义的灵活性，Google 成功打破了“互连墙”。这是一个关于跨学科创新的生动案例：物理学、数学、光学工程和计算机科学在这里完美交汇，共同构建了支撑现代人工智能的基石。

![Image](https://mp.weixin.qq.com/s/www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg%20stroke='none'%20stroke-width='1'%20fill='none'%20fill-rule='evenodd'%20fill-opacity='0'%3E%3Cg%20transform='translate(-249.000000,%20-126.000000)'%20fill='%23FFFFFF'%3E%3Crect%20x='249'%20y='126'%20width='1'%20height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

2022年11月ChatGPT的发布是Google历史上的分水岭。这不仅是产品的危机，更是对Google垂直整合战略的直接挑战。OpenAI利用Nvidia的GPU集群实现了Google TPU未能率先完成的生成式AI的突破。作为回应，Google管理层发布了“红色代码”（Red Code），并在2023年4月做出了决定性的组织调整：合并Google Brain与DeepMind，成立Google DeepMind。

在生成式AI爆发式发展的情况下，Google的内部文化也发生了许多变化。新的一代代芯片快速迭代，“move fast”似乎已成了心照不宣的新气候。 如果说TPU v1-v4 是在“造更好的芯片”，那么从v5开始，Google的核心创新已经转向了“造更好的超级计算机”和“突破物理极限的封装”。在商业战略上， 其核心逻辑变成了：用丰富的产品组合和快速迭代，对不同维度的对手进行打击。

第一招是“产品线分层”。 TPU v5打破了单一旗舰的传统， 分化为v5p（ Performance ）与v5e（ Efficiency ）。 v5p负责性能天花板，v5e负责单位成本与可用性。Google用两张牌同时覆盖训练与推理的不同价格带，削弱 NVIDIA 单一架构通吃的优势。

2023年8月发布的v5e并非为了追求极致性能，而是为了成本效益与可用性。 在Nvidia H100一卡难求且价格高昂的背景下，Google急需一款能大规模提供给外部云客户的高性价比芯片，以防止客户流失到AWS或Azure。 v5e砍掉了一半的芯片面积，专注于推理和中等规模训练，是Google Cloud商业战略意志的体现 。

2023年12月发布的v5p则完全不同，它是 Google DeepMind 合并后的 第一次面向旗舰级训练的性能跃迁 。为了 容纳Gemini Ultra这种万亿参数模型的权重和KV缓存， 高带宽内存 从v4的32GB激增至95GB，单Pod规模翻倍至8960个芯片，Pod之间的连接（ Multislice ）则由 Titanium芯片和专用协议处理， 让数据中心网络（DCN）能够进行 极高效率 的梯度同步。 它似乎在告诉世界：在探索最强算力的路上，Google依然拥有强大的重型武器。

第二招是“品牌化与商业化”。 仅半年之后，2024年5月Google又发布了Trillium (TPU v6)， “TPU 开发十年来的集大成者”， 其设计完全围绕Transformer架构的特性进行了优化。 这是Google第一次对芯片进行了产品命名， 意味着TPU已经从一个内部使用的“基础设施组件”，变成了一个需要对外售卖、建立品牌认知的“独立产品”。 Trillium（延龄草）是一种三瓣的花，有人说，这暗示着 它在计算、内存、互联三方面的均衡。也有人说，这象征了系统在 HBM容量、 HBM带宽、 ICI带宽上对v5e的翻倍。

这一代TPU的核心变革是 将MXU（矩阵乘法单元）的脉动阵列尺寸翻倍（从 128x128 扩展至 256x256）。根据 矩阵乘法的数学特性： 计算量与矩阵维度的立方（N^3）成正比，需要读取的数据量与矩阵维度的平方（N^2）成正比。 这意味着，当阵列的物理边长N翻倍时，我们需要搬运的数据量增加了4倍，但能够执行的有效计算量却增长了8倍。 换句话说，每一比特数据被搬进芯片后，参与运算的次数翻了一倍。这种设计显著提升了数据的复用率，从物理层面摊薄了昂贵的内存访问成本。尽管这种大阵列设计增加了时钟同步与布线的工程难度，但它有效缓解了“内存墙”瓶颈，确立了Trillium在大模型训练场景下的能效优势。

第三招是“ 用封装换良率 ”。 于2025年末上市的TPU v7 “Ironwood”，首次放弃了单片设计，转而采用双芯粒封装， 从单纯追求单点算力，转向对大规模部署成本与良率的控制。 当需要部署百万量级的推理芯片时，传统的“单一大芯片”设计遭遇了物理墙。在半导体制造中，芯片面积越大，包含制造缺陷的概率呈指数级上升。 Ironwood 采用的双芯粒（Chiplet）封装技术，用封装换良率，同时 为了保证计算效率，这两颗裸片通过高密度的D2D（Die-to-Die）互联接口进行封装级连接。这种极低延迟的物理互联，使得上层软件栈无需修改，依然能将底层的两个物理单元识别为一个逻辑上完整的内存与计算空间。 这种设计策略，精准解决了大规模推理集群面临的供应链难题：它规避了制造超大芯片的高风险，使得在同等光刻产能下，能够以更低的边际成本交付更大规模的算力。在万亿参数模型走向普及的阶段，这种工业化的量产能力，成为了比单纯峰值性能更具决定性的商业竞争力。  

![Image](https://mp.weixin.qq.com/s/www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg%20stroke='none'%20stroke-width='1'%20fill='none'%20fill-rule='evenodd'%20fill-opacity='0'%3E%3Cg%20transform='translate(-249.000000,%20-126.000000)'%20fill='%23FFFFFF'%3E%3Crect%20x='249'%20y='126'%20width='1'%20height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)
- 在v1时代，面对语音搜索的算力危机，Google敢于做“极致的减法”。它用8位整数运算和脉动阵列，证明了在特定领域，专用架构可以比通用CPU高效一个数量级；
- 在v2与v3时代，面对训练的瓶颈，Google不仅引入了BF16标准，更重要的是确立了以ICI为核心的互联思维，打破了单卡的物理边界；
- 在v4时代，面对阿姆达尔定律的诅咒和通信墙的阻隔，Google跨界引入了OCS，用物理学和数学的智慧重构了网络的拓扑结构， 让数千张芯片如同一体 ；
- 而到了v5、v6乃至v7时代，面对生成式AI对万亿参数的渴求，TPU更是完成了从“计算核心”到“超级系统”的彻底蜕变。

TPU的成功证明了，在后摩尔时代，算力的提升不再单纯依赖于堆砌晶体管的数量，而取决于对“算法-软件-硬件”全栈的深度理解与协同设计。

未来的AI竞争，越来越像一场系统工程的长跑：比的不是单芯片的峰值，而是把计算、内存、互连与软件协同到极限的能力。

值得庆幸的是，这条曾经寂寞的自研之路，如今已人声鼎沸。随着AWS Trainium、Microsoft Maia以及Meta MTIA的相继入局，专用AI芯片不再是Google一家的孤勇尝试，而是成为了云计算巨头们的标配。这种百花齐放的硬件生态，打破了传统通用计算的沉闷格局，迫使软硬件的边界被不断重塑与推翻。正是这种略带混乱的勃勃生机，让此时此刻成为了硬件工程师们最令人心潮澎湃的时刻。

正如计算机体系结构宗师David Patterson所言：“这是计算机架构的黄金时代。” 而TPU，正是这个时代最耀眼的注脚之一。

  

  

作者提示: 个人观点，仅供参考

继续滑动看下一个

科技慢变量

向上滑动看下一个