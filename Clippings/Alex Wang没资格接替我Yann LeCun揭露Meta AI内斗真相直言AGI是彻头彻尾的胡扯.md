---
title: "Alex Wang“没资格接替我”！Yann LeCun揭露Meta AI“内斗”真相，直言AGI是“彻头彻尾的胡扯”"
source: "https://mp.weixin.qq.com/s/5_YuZ2gV9-wiVgwakBIPBA"
author:
  - "[[冬梅]]"
published:
created: 2025-12-16
description: "AI 在10年内能达到狗的智商已经算是乐观了。"
tags:
  - "AI路线分歧"
  - "世界模型"
  - "通用智能批判"
abstract: "图灵奖得主Yann LeCun在访谈中严厉批评当前以大语言模型为核心的AI发展路径是“胡扯”，并阐述了他基于“世界模型”和抽象表征预测的新愿景。"
---
Original 冬梅 *2025年12月16日 14:47*

![Image](https://mmbiz.qpic.cn/sz_mmbiz_png/YriaiaJPb26VOyok8AFzdI1dYk5y1nMIvCKpHVPxxATHib8NuRaxgatxGCAiaBhDFhKLbcPMmCN2Be7Er2uniaOrs4A/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1#imgIndex=0) ![Image](https://mmbiz.qpic.cn/mmbiz_gif/YriaiaJPb26VPQqHC66RJFpttVIMWG83T3lWHahUD4bvhxlKSayjeV2ibvC5ydqklP9QHDPD3qHJM07TV3IfHstjA/640?wx_fmt=gif&tp=webp&wxfrom=5&wx_lazy=1#imgIndex=1)

编译｜冬梅

“通往超级智能的那条路——无非是不断训练大语言模型、喂更多合成数据、雇上几千人做后训练、再在强化学习上搞点新花样——在我看来完全是胡扯，这条路根本行不通。”

近日，在一档名为《The Information Bottleneck》的访谈栏目中，主持人 Ravid Shwartz-Ziv 和 Allen Roush 与图灵奖得主、前 Meta 首席 AI 科学家 Yann LeCun 展开了一场近两小时的高质量对话，在访谈中，LeCun 解释了为什么会在 65 岁这个别人已经退休的年纪他还在创业，此外，他也对当前硅谷主流的人工智能发展路径给出了罕见而尖锐的评价。

结束在 Meta 长达 12 年的职业生涯后，LeCun 正将个人学术声誉与职业“遗产”押注在一套截然不同的 AI 愿景之上。他直言，业界对大语言模型规模化的执念，正在把人工智能引向一条看似高速、实则封闭的死胡同。

在 LeCun 看来，真正制约 AI 进步的关键，并不是如何更快地逼近“人类级智能”，而是如何跨越一个常被低估却极其困难的门槛—— **让机器具备“狗的智能水平”** 。这一判断挑战了当前以语言能力和知识覆盖面为中心的评估体系。在他看来，现实世界中的理解、预测和行动能力，远比生成流畅文本复杂得多，而现有以语言为核心的模型，并未真正触及这一问题的本质。

基于这一判断，LeCun 正通过其新公司 AMI 推动另一条技术路线：构建能够理解和预测世界的“世界模型”（World Models）。与主流生成模型直接在像素或文本层面进行输出不同，AMI 的核心思路是在 **抽象表征空间中对世界运行规律进行建模和预测** 。这种模型关注的不是“生成看起来像什么”，而是“世界将如何演化”，从而为机器提供更接近真实认知的基础能力。

这番表态再次凸显了 LeCun 与当前主流 AI 叙事之间的分歧。在行业普遍押注算力、数据和参数规模的背景下，他选择回到认知与感知的基本问题，试图重新定义通往通用人工智能的技术路径。对 LeCun 而言，这不仅是一场技术路线之争，也是一场关乎 AI 未来方向的长期下注。

**以下为访谈实录，经由 InfoQ 翻译及整理：**

**![Image](https://mmbiz.qpic.cn/sz_mmbiz_png/YriaiaJPb26VM1hcCib7lColHujIJVF65dAH9gkAtOwuY4sW7OXibCYw6F0Un967rmTwtuhRKd4AmDxkUjV9TAP38w/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1#imgIndex=2)**

Ravid Shwartz-Ziv ：你最近宣布，在 12 年后离开 Meta，正在创办一家新的创业公司，专注于先进的机器智能，并且致力于世界模型的研究。首先，从大公司转向从零开始创业，身处其中感觉如何？

**Yann LeCun：** 我离开 Meta 创立新公司，是看到当前 AI 投资热潮让长期研究型创业成为可能。以前这类研究只能依托于 IBM、贝尔实验室这类垄断型大企业，或是微软、谷歌、Meta 等大公司的研究院。

近年来，尽管我们推动的开放研究曾影响整个领域，但如今包括谷歌、OpenAI 甚至 Meta 在内的许多实验室正转向封闭。因此，我认为现在正是时候在 Meta 之外继续推进我所专注的研究方向。

Allen Roush：那我想确认下，您的公司 AMI（先进机器智能）是否计划以开放的方式进行研究？

**Yann LeCun：是的。就拿上游研究来说，我认为，除非公开发表成果，否则不能称之为真正的研究** 。如果你只是自己想出一个东西，觉得它前所未有的好，却不提交给学界检验，那很可能只是错觉。

我在很多工业研究实验室都多次见过这种现象：内部对一些项目过度追捧，却没有意识到别人已经在做更出色的工作。所以，如果你要求科学家发表工作，首先，这会激励他们做出更好的研究——研究方法会更严谨，结果会更可靠。这对他们自身也有益，因为一个研究项目对产品产生影响，常常需要数月、数年甚至数十年。你不可能告诉研究人员“来为我们工作，但别透露你的研究内容，或许五年后你就能对某个产品产生影响”——这在期间他们无法获得足够的动力去做真正有用的工作，而只会倾向于做有短期影响的事情。

因此， **如果你真的想要取得突破，就必须允许人们发表。没有其他途径。而这是目前很多行业都在忘记的一点** 。

Allen Roush：AMI（先进机器智能）它会推出产品吗？它仅专注于研究，还是有更广泛的规划？

**Yann LeCun：** 不，远不止于此。它最终是要推出实际产品的。这些产品将围绕世界模型和规划等核心技术展开。我们的雄心是，未来能成为智能系统的主要供应商之一。

我们认为，当前基于大语言模型的架构，虽然在语言处理上尚可，但其构建的智能体系统并不理想。它们需要海量数据来模仿人类行为，且可靠性有限。

要解决这个问题，正确的方法——也是我近十年来一直主张的——是构建能够预测 AI 行为后果的模型。然后，系统通过优化计算，找出最优行动序列来完成任务，这就是规划。我认为，智能的核心在于能预测自身行动的后果，并用于规划。

这正是我多年来专注的方向。我们结合了纽约大学和 Meta 的研究项目，已经取得了快速的进展。现在，是时候将这些构想变为现实了。

Ravid Shwartz-Ziv：那么，您认为目前还缺少哪些关键部分？为什么这个过程花了这么长时间？毕竟，正如您所说，您已经为此研究了许多年，但它目前依然没有超越大语言模型，对吧？

**Yann LeCun：** 世界模型与大语言模型是两回事。它旨在处理高维度、连续且含噪声的数据模态，而大语言模型对此完全无能为力——它们处理图像或视频表征的效果并不理想。通常，AI 的视觉能力是分开训练的，并不属于 LLM 架构的一部分。

所以，要处理这类数据，你不能使用生成模型，尤其不能使用那些将数据“切分”成离散符号的生成模型。大量实证表明，这行不通。

真正有效的方法是：学习一个抽象的表征空间。这个空间会滤除输入中大量无法预测的细节（包括噪声），然后在这个表征空间内进行预测。这就是 JEPA（联合嵌入预测架构）的核心思想。

让我回溯一下这段研究历程。我大概在近 20 年前就确信，构建智能系统的正确途径是某种形式的无监督学习。

在 21 世纪初，我开始以此为方向探索。当时的主流思路是训练自编码器来学习表征：编码器将输入转化为表征，再解码还原，以确保表征包含输入的全部信息。但后来发现，这种“表征必须包含全部信息”的直觉是错的，它并非好方法。那时我们并不知晓，尝试了多种方案，例如受限玻尔兹曼机、去噪自编码器，而我主攻稀疏自编码器——通过高维稀疏表征来构建信息瓶颈，限制表征中的信息量。

我和一些学生（包括后来成为 DeepMind 首席技术官的 Koray Kavukcuoglu）围绕此做了不少博士研究。我们当时的核心目标，是希望通过这种自编码器预训练来搭建非常深的神经网络。

然而，事情出现了转折。随着归一化、ReLU 激活函数等技术的出现，以及数据集规模的扩大，我们发现在完全有监督的方式下也能成功训练相当深的网络。于是，自监督 / 无监督学习的想法就被暂时搁置了。后来，ResNet 在 2015 年出现，基本解决了训练极深架构的问题。

但也是在 2015 年，我开始重新思考如何迈向人类级别 AI 这个初心。我意识到，强化学习等方法在样本效率上极低，难以扩展。于是，“世界模型”的想法——即系统能预测自身行动后果并进行规划——开始真正成型。

我在 2016 年 NIPS 大会的主题演讲中，就以此为核心主张。随后，我和学生开始在视频预测等领域进行具体研究。但我们（包括当时的整个领域）犯了一个同样的根本性错误：试图在像素级别进行预测。这在视频这样的高维连续空间里是几乎不可能的。

我知道因为预测本质是非确定性的，所以模型需要潜变量来表征未知信息。我们为此实验了多年，也探索了扩散模型、基于能量的模型等训练非确定性函数的方法。

最终，我领悟到根本的出路是放弃像素级预测。关键在于运行一个表征，并在表征层面进行预测，滤除所有无法预测的细节。

但我早期没有深入探索这种方法，是因为担心一个重大问题：坍缩。简单来说，如果你仅以最小化预测误差为目标来端到端地训练整个系统（编码器 + 预测器），系统会找到一个“作弊”的捷径：忽略输入，直接输出恒定的表征，使预测问题变得 trivial。这个问题在 90 年代研究连体网络（即联合嵌入架构的早期形式）时我就已经知晓。

Allen Roush：回到上世纪 90 年代，其实当时和现在用的还是同一个术语。直到最近，人们仍然在这些网络里使用它。

**Yann LeCun：** 没错，这个概念本身至今依然是成立的。你可以把问题抽象成这样：有一个 X 和一个 Y，把 X 看作是 Y 的某种退化、变换或被破坏后的版本。然后你把 X 和 Y 同时送入编码器，并告诉系统：X 和 Y 本质上是同一个事物的两个视角，因此它们对应的表征应该是相同的。

问题在于，如果你只是简单地训练两个共享权重的神经网络，让它们对同一对象的略微不同版本输出相同的表示，系统很快就会“塌缩”，学不到任何有用的东西。因此，你必须确保系统能够从输入中尽可能多地提取信息。

我们当年在 1993 年提出 Siamese Network 的时候，采用的核心思路是加入一个对比项（contrastive term）。也就是说，除了相似样本对，你还引入不相似的样本对，通过训练让系统在相似样本上拉近表示，在不相似样本上拉远表示。最终形成一个代价函数：相似样本吸引，不相似样本排斥。

这个想法最初来自一个非常实际的需求。有人找到我们，说能不能把手写签名编码成少于 80 个字节？如果可以，就能把签名写进信用卡的磁条里，用于信用卡签名验证。于是我们设计了一个神经网络，输出 80 个变量，每个变量量化成 1 个字节，通过对比学习来训练它。结果效果非常好。

但后来他们把方案拿给业务部门看，得到的反馈却是：“我们干脆让用户输入 PIN 码好了。”这对我来说是一次非常典型的教训：技术上可行，并不意味着商业上会被采纳。我当时就觉得这件事本身有点不靠谱，因为欧洲已经在使用智能卡了，技术路径完全不同。

到了 2000 年代中期，我和两位学生重新回到这个方向，提出了新的目标函数。这就是后来人们所说的对比学习方法：正样本、负样本，正样本对应低能量，负样本对应高能量，能量本质上就是表征之间的距离。我们在 2005 年和 2006 年的 CVPR 上发表了两篇论文，作者包括 Raia Hadsell（现在是 DeepMind 基础研究部门负责人）以及 Sumit Chopra（现为纽约大学教授，研究医学影像）。

这些工作在社区中引起了一定关注，也让对比学习重新“活”了过来，但效果依然不算理想。比如在图像任务中，这类方法学到的表示维度往往很低。即便在 ImageNet 上训练，表征的有效维度也就两三百，这在当时是相当令人失望的。

大约五年前，我在 MIT 的一位博士后 Stefan（Stefano）提出了一个我最初并不看好的想法：直接最大化编码器输出的信息量。我之所以怀疑，是因为早在 1980 年代，Geoff Hinton 就做过类似尝试——信息量本身是很难最大化的，因为我们通常只有上界，没有可计算的下界。

但 Stefan 提出了一个方法，后来被称为 **Barlow Twins** ，名字来自一位提出信息最大化思想的理论神经科学家。结果这个方法居然真的奏效了。这让我意识到：这个方向值得深入推进。

随后，我们又提出了 **VICReg** （Variance–Invariance–Covariance Regularization），结构更简单，效果反而更好。最近我和 Randall 还讨论了一个可以进一步工程化的方案，叫 **SigReg** ，整个系统被命名为 **Lojic-JEPA** 。SigReg 的核心思想，是约束编码器输出的向量分布接近各向同性高斯分布。

这个领域正在快速发展，我认为未来一两年内还会有显著进展。这是一条非常有前景的技术路线，用来训练能够学习抽象表征的模型，而抽象表征恰恰是关键。

1 大模型现在缺失的关键要素是什么

Ravid Shwartz-Ziv：在你看来，现在还缺失的关键要素是什么？是更多算力，还是更好的算法？你是否认同“苦涩的教训”（Bitter Lesson）？另外，你怎么看 2022 年之后互联网数据质量下降的问题？有人把 LLM 之前的数据称为“低背景辐射钢铁”，你认同这种说法吗？

**Yann LeCun：** 我觉得自己基本上不太受这个问题的影响。原因很简单：如果你要训练一个性能还算过得去的大语言模型，就必须使用几乎整个互联网中所有可获取的文本数据，再加上一些合成数据和授权数据。两三年前，一个典型模型的预训练规模大约是 30 万亿 token。一个 token 大概 3 个字节，这相当于 10¹⁴ 字节的数据量。

这意味着什么？意味着模型必须具备极大的存储能力，因为文本中包含的是大量相互孤立的事实。文本冗余度并不高，本质上是“记忆型”数据，因此你需要非常大的网络来存储并复述这些事实。

现在我们对比一下视频数据。10¹⁴ 字节的视频，大约相当于 1.5 万小时的视频内容。这听起来很多，但实际上只是 YouTube 半小时的上传量，也差不多是一个四岁孩子一生中清醒时间看到的视觉信息总量。四年的清醒时间，大约就是 1.6 万小时。

而现实世界的视频数据，结构要比文本丰富得多。我们现在已经有视频模型，比如去年发布的 V-JEPA 2，它使用的训练数据相当于一个世纪的视频量，而且全部是公开数据。尽管视频在字节层面更冗余，但正是这种冗余，使得自监督学习成为可能。

如果数据是完全随机的，自监督学习是无法进行的。你必须依赖冗余结构。正因如此，真实世界的数据，尤其是视频，远比文本更有学习价值。这也是我一直坚持的观点： **仅靠文本训练，永远不可能达到人类水平的智能。**

Allen Roush：那在“世界模型”和“现实 grounding”这个问题上，你怎么看？有人认为世界模型就是一个高度逼真的模拟器，比如像《星际迷航》里的全息甲板（Holodeck），你认同这种理想吗？

**Yann LeCun：** 这是一个非常好的问题，因为它直指问题的核心，也恰好说明了我认为当前主流认知有多么偏离方向。

很多人以为，世界模型就是对现实世界每一个细节的完整复刻，本质上是一个模拟器。由于深度学习的流行，大家自然会想到用神经网络来做模拟，尤其是视频生成，看起来非常炫酷。

但问题在于：一个视频生成模型，并不保证它真正理解了世界的底层动力学。它可能只是学会了表面统计相关性。认为模型必须复现现实的每一个细节，这是一个错误，甚至是有害的想法。

举个例子：计算流体力学（CFD）。我们用超级计算机模拟飞机周围的气流，把空间切成小立方体，在每个立方体里记录速度、密度、温度等变量，然后解偏微分方程。即便如此，这种模拟在湍流等混沌情况下也只是近似正确。

但你要注意，这已经是对真实物理过程的高度抽象了。真实世界中，气流是由分子碰撞产生的，但没有人会去逐分子地模拟，那样的计算量是不可承受的。

再往下推，你可以说应该用量子场论来描述一切，但那同样是不现实的。我们既无法测量宇宙的波函数，也无法提供所需的计算资源。

所以我们做了什么？我们发明了 **抽象层级** 。从粒子、原子、分子，到细胞、器官、个体、社会、生态系统。每一层抽象都会忽略下层的大量细节，而正是这种忽略，使我们能够进行更长期、更稳定的预测。

世界模型也一样。它不需要是现实的逐像素模拟器，而是在抽象表征空间中，只模拟与任务相关的那部分现实。

如果我问你：100 年后木星在哪里？你并不需要关于木星的全部信息，只需要 6 个数字：三个位置坐标，三个速度分量，其余信息都无关紧要。

2 合成数据和模拟环境对大模型意味什么

Allen Roush：那你怎么看合成数据和模拟环境？比如游戏数据，是否会引入偏差？

**Yann LeCun：** 合成数据当然是有价值的。孩子通过游戏学到大量知识，本质上也是在受控的模拟环境中学习。

但确实要警惕某些模拟的失真。例如很多视频游戏里的物理效果是为了“好看”，而非真实。这种偏差如果不加控制，可能会影响模型在现实世界中的泛化能力。

关键不在于是否使用模拟，而在于你在 **哪个抽象层级** 上训练模型。很多基础的世界规律——物体会一起移动、会下落、不会同时出现在两个地方——都是在非常抽象的层面上学到的，而不是通过语言学到的。

大语言模型并不真正理解这些规律，它们只是被微调到给出“看起来正确”的答案。这是复述，不是理解。

而这些概念，是可以通过非常简单、抽象的环境学会的，比如二维冒险游戏。即便环境并不完美，但它们仍然能教会模型一些极其基础、却至关重要的世界结构。

Ravid Shwartz-Ziv：所以你认为，这条路还能继续推进吗？

**Yann LeCun：** 可以，而且必须继续推进。真正重要的，是让模型学会抽象世界的结构，而不是背诵世界的表象。这才是通向真正智能系统的唯一道路。

在围棋和象棋这样的游戏中，机器显然已经远远超过人类了。原因之一是机器在博弈树搜索上的速度极快，也具备人类根本不可能拥有的记忆容量。人类根本无法进行真正的广度优先搜索。

AlphaGo 出现之前，人们曾认为顶级围棋选手距离“理想棋手”（他们称之为“神”）也许只差两三子。但事实证明并非如此——即便是世界最顶级的人类棋手，也需要让八到九子才能与机器对抗。

Allen Roush：我很荣幸能和你讨论游戏 AI 的问题。我有两个延伸问题。第一个，你提到人类在象棋等任务上其实非常不擅长。我了解到这常被称为“莫拉维克悖论”：人类在漫长的进化过程中擅长身体运动和感知，但从未进化出下棋能力。你是否认同这个解释？

第二个问题与电子游戏有关。很多玩家——包括我自己——都感觉游戏里的敌人 AI 在过去 20 年几乎没有进步，最好的例子仍然是 2000 年代初的《光环 1》和《FEAR》。你认为实验室里的 AI 研究，什么时候才能真正影响游戏体验，而不是只体现在生成式 AI 上？

**Yann LeCun：** 我以前也是玩家，虽然不算沉迷，但我的家庭几乎完全浸在这个圈子里——我有三个三十多岁的儿子，他们共同经营一家游戏设计工作室。所以我对这个行业并不陌生。

你的观察是对的。事实上，不只是游戏，动画电影制作也是类似的情况。尽管物理模拟已经相当精确，但很多动画工作室并不会使用最真实的物理模拟，因为他们更需要的是“可控性”，而不是绝对的物理准确性。

游戏同样是一种创作行为，创作者希望控制剧情走向、NPC 的行为方式，而不是让一切完全由 AI 自由演化。目前的 AI 技术在“保持可控性”方面仍然存在挑战，这也是游戏行业对深度 AI 应用保持谨慎的原因之一。

至于你提到的莫拉维克悖论，它依然完全成立。这一观点大约是在 1988 年提出的，其核心问题是：

为什么我们认为高度“智能”的任务（比如下棋、做积分）可以轻松交给计算机，而那些我们完全不觉得是“智能”的能力——比如猫走路、避障、捕猎——却极其难以让机器掌握？

即便到了今天，四十多年过去了，我们仍然无法让机器人具备猫的灵活性、创造性和适应能力。问题不在于硬件，我们完全可以造出这样的机器人，而在于我们无法让它们“足够聪明”。

正因为如此，那些宣称“一两年内实现 AGI”的说法，在我看来是完全脱离现实的幻想。真实世界的复杂度，远不是通过对世界进行 token 化、再喂给语言模型就能解决的。

3 “通用智能”简直是胡扯

Ravid Shwartz-Ziv：那你对 AGI 的时间表怎么看？你是偏乐观还是偏悲观？在当前关于 AI 风险的讨论中，你更接近哪一派？

**Yann LeCun：** 首先我要明确一点： **“通用智能”这个概念本身就是站不住脚的。**

它本质上是以“人类智能”为参照定义的，但人类智能本身高度专用化。我们擅长在现实世界中行动、与他人互动，但在下棋等任务上却表现糟糕；而很多动物在某些方面远胜人类。

我们之所以误以为自己是“通用的”，只是因为我们只能理解自己能够想象的问题。

因此，与其讨论“通用智能”，不如讨论“人类水平智能”：机器是否会在所有人类擅长的领域达到或超过人类？答案是肯定的，而且在某些领域已经发生了——例如机器可以在上千种语言之间进行双向翻译，这是任何人类都无法做到的。

但这个过程不会是一个突发事件，而是一个渐进的过程。

未来几年，我们可能在世界模型、规划能力等方面取得关键性进展。如果一切顺利、没有遇到尚未意识到的根本性障碍， **最乐观的情况是：在 5 到 10 年内，我们或许能看到接近人类，或者至少接近“狗水平”的智能系统** 。

但这只是最乐观的估计。历史告诉我们，AI 发展中总会出现新的瓶颈，可能需要 20 年甚至更久才能突破。

Ravid Shwartz-Ziv：那你认为，从现在到“狗水平智能”，是否比从“狗”到“人类”更难？

**Yann LeCun：** 恰恰相反， **最难的部分是达到“狗水平”。**

一旦你达到这个阶段，绝大多数核心要素就已经具备了。从灵长类到人类，真正新增的关键能力，可能主要是语言。而语言在大脑中只占据极小的区域，我们已经在这方面做得相当不错。

某种意义上，未来的语言模型可能扮演人脑中布罗卡区和韦尼克区的角色。而我们当前真正缺失的，是相当于“前额叶皮层”的能力——也就是世界模型、规划与行动能力。

Allen Roush：这就引出了一个绕不开的问题：安全。如果 AI 达到“狗水平”，它在嗅觉等感知能力上可能已经远超人类，这只是潜在冲击的冰山一角。再加上“超级说服”“AI 精神错乱”等现象，你是否担心 AI 会变得失控？

**Yann LeCun：** 我当然理解这些担忧，而且我本人也亲身经历过相关事件。有一次在 NYU 校园，我遇到一名情绪严重不稳定的人，他携带危险物品，被警方带走。还有高中生给我写信，说他们被“AI 灭世论”吓到，甚至不再上学。

这些现象说明，恐惧本身也会造成真实伤害。但历史告诉我们， **任何强大的技术都会带来利弊。**

以汽车为例，早期汽车极其危险，但通过安全带、溃缩区、自动刹车系统等技术演进，如今已经大幅降低了死亡率。欧盟强制配备的自动紧急制动系统，已被证明能减少 40% 的正面碰撞事故。

AI 也是如此。它既可能带来风险，也已经在医疗影像等领域挽救了大量生命。

4 当 AI 发展到某一阶段，要暂停吗？

Allen Roush：你与 Hinton、Bengio 在 AI 未来问题上的立场有所不同。你认为会不会有一天，AI 发展到某个阶段，必须暂停推进，转而只关注安全？

**Yann LeCun：安全必须与发展同步进行，而不是先停下来等“绝对安全”** 。

我常用喷气发动机作比喻：第一代喷气发动机根本不安全、不可靠，但正是在不断工程改进中，才达到了今天这种可以连续飞行 17 小时的可靠性。AI 也会走类似的路径。我们会逐步构建具备规划与行动能力的系统，同时在非常底层引入明确的安全约束。

比如，家用机器人必须始终避开人类、不能伤害人；手持刀具时必须限制动作幅度。这些都可以通过低层规则明确约束。所谓“回形针最大化”的极端案例，在工程上其实非常容易避免。

Ravid Shwartz-Ziv：有人认为，大语言模型可以通过微调来避免生成危险内容，但现实是，它们总是可以被“越狱”，总能找到某些提示词绕过限制。无论我们禁止它们做什么，总会有漏洞。你怎么看？

**Yann LeCun：** 我同意你的判断，这正是我反对继续依赖 LLM 的原因。我们不应该再指望通过微调语言模型来解决安全问题，而应该转向我之前提到的那类 **以目标驱动（objective-driven）为核心的 AI 架构** 。

在这种架构中，系统具备以下几个关键能力：

第一，它拥有 **世界模型** ，能够预测自身行为可能带来的后果；

第二，它可以规划一系列行动来完成任务；

第三，也是最关键的，它必须受到一整套 **硬性约束** 的限制，确保无论采取什么行动、预测到什么世界状态，都不会对人类造成危险，也不会产生负面副作用。

这类系统在设计层面就是安全的。因为它的输出不是靠“微调”或“内容过滤”，而是通过 **在满足约束条件的前提下，优化目标函数** 得出的。换句话说，它在结构上就不具备“逃逸”的可能性，这是一种先天安全的设计，而不是事后修补。

Allen Roush：目前在 LLM 领域，也有一些限制输出空间的技术，比如只允许模型在一个非常有限的输出集合中生成内容，这种方法在扩散模型中也有应用。你认为这些方法在现实中真的显著提升了模型的可用性吗？

**Yann LeCun：** 它们确实有一定帮助，但代价极其高昂，所以这种做法极其荒谬。这类方法的基本做法是：先让模型生成大量候选输出，再用一个过滤或排序系统进行打分，挑出“最不糟糕”“毒性最低”的那个结果。

问题在于，这种方式本质上是 **暴力搜索** ， **计算成本高得离谱** 。除非你有某种真正意义上的目标函数或价值函数，能够在生成过程中就把系统引导到“高质量、低风险”的输出，否则这种做法永远都会非常昂贵、低效，也不可规模化。

5 “Alex Wang 不是科学家，他没有接替我”

Allen Roush：我们稍微换一个话题。很多观众也关心一些更偏“人和组织”的问题。比如，在 Meta 内部，Alex Wang 似乎正在接管你过去的角色，你怎么看 Meta 未来的 AI 方向？

**Yann LeCun：他并不是在“接替我”。** Alex Wang 负责的是 Meta 所有 AI 相关的研发与产品整体运作，而不是科研本身。 **他并不是研究员或科学家，而是一个全面统筹的人** 。

在 Meta 的“超级智能实验室”体系下，大致可以分为四个部分：第一是 FAIR，负责长期基础研究；第二是 TBD Lab，主要做前沿模型，几乎完全聚焦大语言模型；第三是 AI 基础设施，包括软件和硬件；第四是产品部门，把前沿模型做成真正可用的产品，比如聊天机器人，并集成到 WhatsApp 等应用中。Alex 统管这四个方向。

我本人是 FAIR 的首席 AI 科学家，但我很快就会离开 Meta——大概再待三周左右。

Ravid Shwartz-Ziv：FAIR 的定位是否也在发生变化？

**Yan LeCun：** 是的。FAIR 目前由我们在 NYU 的同事 Rob Fergus 领导。在 Joel Pineau 离开后，FAIR 被明显推向更短期、更偏应用的研究方向，发表论文的重要性下降，更多是为 TBD Lab 的大模型工作提供支持。这也意味着 Meta 整体正在变得更“封闭”。

有些研究团队也被重新归类，比如做 SAM（Segment Anything）的团队，现在已经归到产品部门，因为他们做的是更偏向对外、实用型的技术。

6 关于“世界模型”创业公司的看法

Allen Roush：你如何看待其他试图构建世界模型的公司？比如 Physical Intelligence、Thinking Machines，或者 SSI？

**Yan LeCun：** 说实话，大多数我也不太清楚他们具体在做什么。SSI 已经成了一个行业笑话——几乎没人知道他们在干什么，包括他们自己的投资人。这只是传言，我不确定真假。

Physical Intelligence 的方向，我倒是了解一些。他们主要做的是 **几何一致的视频生成** ：场景具有持久的三维结构，你转身再回来，物体不会凭空变化。这仍然是一种生成像素的思路，而我刚刚才花了很长时间解释，为什么我认为“生成像素”本身是个错误方向。

Allen Roush：有没有你比较认可的世界模型实践？

**Yan LeCun：** 有一家叫 **Wayve** （WA-Y-V-E）的公司，总部在牛津，我是他们的顾问。他们在自动驾驶领域构建了一个世界模型：先学习一个表示空间，再在这个抽象空间中做时间预测。

他们做对了一半：对的地方在于， **预测应该发生在表示空间，而不是像素空间** ；问题在于，他们的表示空间仍然主要通过重建训练得到，这一点我认为是错误的。

尽管如此，他们的系统整体效果非常好，在这一领域已经走得相当靠前。

此外，NVIDIA 和 Sandbox AQ 也在谈类似方向。Sandbox AQ 的 CEO Jack Hidary 提出“ **大型定量模型** ”，而不是语言模型，本质上就是能够处理连续、高维、噪声数据的预测模型——这与我的主张高度一致。

Google 也做了很多世界模型，但主要仍是生成式路径。Danijar Hafner 的 Dreamer 系列模型其实走在一条正确道路上，只可惜他已经离开 Google 创业了。

Ravid Shwartz-Ziv：你曾严厉批评硅谷过度聚焦大语言模型。这是否也是你选择在巴黎启动新公司的原因之一？你认为这种现象会改变吗？

**Yan LeCun：** 我先澄清一点：我创办的是一家 **全球性公司** ，只是其中一个办公室在巴黎，在纽约等地也有布局。

硅谷存在一个非常典型的现象： **因为竞争极端激烈，所有公司都被迫做同一件事** 。如果你走一条不同的技术路线，就有“掉队”的巨大风险。这种环境会催生一种 **技术单一化（monoculture）** 。

OpenAI、Meta、Google、Anthropic，几乎所有公司都在做同样的事情。

结果就是：大家拼命在同一条战壕里向前冲，却很容易被来自“完全不同方向”的技术突破所颠覆。

我所关注的世界模型与目标驱动架构，本质上是为了解决 **语言模型根本不擅长的问题** ：连续、高维、噪声数据，比如视频、物理世界感知。这些领域里，LLM 的尝试几乎都失败了。

在硅谷，有一种说法叫“你是否已经被 LLM 洗脑”。很多人坚信，只要不断扩大模型规模、生成更多合成数据、加强 RL 微调，就一定能走向超级智能。我认为这是 **彻底错误的。**

你必须跳出这种文化。而事实上，在硅谷的大公司内部，也有不少人私下认同这一点——我现在正在把他们招过来。

7 为什么 65 岁仍然创业？

Ravid Shwartz-Ziv：你已经 65 岁，拿过图灵奖、女王奖，完全可以退休。为什么现在还要创业？

**Yan LeCun：** 因为我有使命感。我一直认为， **提升世界上的“智能总量”是一件内在正确的事情** 。智能是这个世界上最稀缺、最被需要的资源，这也是为什么人类投入如此多的成本去教育。

无论是帮助人类更聪明，还是用机器来增强人类智能，本质上都是在服务同一个目标。当然，强大技术必然伴随风险，但那是工程和治理问题，而不是不可逾越的根本障碍。

我一生的研究、教学、公共传播，几乎都围绕着同一件事：让人类变得更聪明。而机器智能，本质上也是这个目标的一部分。

Ravid Shwartz-Ziv：有没有什么你一直没来得及写下的想法，或者职业上的遗憾？

**Yan LeCun：** 太多了。我整个职业生涯都在后悔一件事： **没有花足够时间把自己的想法写下来** ，结果经常被别人抢先。

反向传播就是一个例子。我其实很早就有类似思路，但没有及时完整发表。不过我并不纠结。科学思想几乎从来不是孤立产生的，从想法到论文、到理论、到应用、到产品，本身就是一个漫长而复杂的链条。

“世界模型”这个概念也并不新。早在 1960 年代，控制论和航天工程就已经在使用世界模型来规划火箭轨道。所谓系统辨识，更是 1970 年代的老概念。

真正的难点，从来不在“最早提出”，而在于 **把一个想法真正变成可工作的系统** 。

**参考链接：**

https://www.youtube.com/watch?v=7u-DXVADyhc

声明：本文为 InfoQ 翻译整理，未经许可禁止转载。

今日好文推荐

活动推荐

AI 重塑组织的浪潮已至，Agentic 企业时代正式开启！当 AI 不再是单纯的辅助工具，而是深度融入业务核心、驱动组织形态与运作逻辑全面革新的核心力量。

把握行业变革关键节点，12 月 19 日 - 20 日，AICon 全球人工智能开发与应用大会（北京站） 即将重磅启幕！本届大会精准锚定行业前沿，聚焦大模型训练与推理、AI Agent、研发新范式与组织革新，邀您共同深入探讨：如何构建起可信赖、可规模化、可商业化的 Agentic 操作系统，让 AI 真正成为企业降本增效、突破增长天花板的核心引擎。

![Image](https://mp.weixin.qq.com/s/www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg%20stroke='none'%20stroke-width='1'%20fill='none'%20fill-rule='evenodd'%20fill-opacity='0'%3E%3Cg%20transform='translate(-249.000000,%20-126.000000)'%20fill='%23FFFFFF'%3E%3Crect%20x='249'%20y='126'%20width='1'%20height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

继续滑动看下一个

InfoQ

向上滑动看下一个