---
title: "世界首个！李飞飞团队推出物理推理基准，大模型统统不及格？"
source: "https://mp.weixin.qq.com/s?__biz=Mzg3Mzg5MjY3Nw==&chksm=cf77b89fadbcffd476a76ac62f8b940a492fa4c670690b5c25167f3575e4a1575cffd00d8518&idx=1&mid=2247525500&sn=fafd174828ae4234aa87fcff0d5cf51e#rd"
author:
  - "[[suani]]"
published:
created: 2025-12-31
description: "李飞飞VLM物理推理评估基准。"
tags:
  - "物理推理基准"
  - "定量评估"
  - "视觉语言模型"
  - "运动学计算"
abstract: "斯坦福大学团队推出首个定量评估视觉语言模型物理推理能力的基准QuantiPhy，测试发现顶尖模型在数值计算任务上表现不佳，甚至不优于人类直觉，揭示了模型依赖记忆而非实时视觉计算的根本缺陷。"
---
![cover_image](https://mmbiz.qpic.cn/mmbiz_jpg/FW3bYDODsoy8pg6G4X8ibJCpbNp6yX0GPuZPVrX7UNOy9V22o0jryxvLr0mYps4l3QeJBdnw3yjOibuCiaT5Wf8Iw/0?wx_fmt=jpeg)

suani [AIGC开放社区](https://mp.weixin.qq.com/) *2025年12月30日 08:45*

*专注AIGC领域的专业社区，关注微软&OpenAI、百度文心一言、讯飞星火等大语言模型（LLM）的发展和 *应用* 落地，聚焦LLM的市场研究和AIGC开发者生态，欢迎关注！*

斯坦福大学联合中国科学技术大学的研究团队，针对视觉语言模型（Vision-Language Models）对物理世界的理解能力，推出了世界首个定量评估基准。

![Image](https://mmbiz.qpic.cn/mmbiz_png/FW3bYDODsoy8pg6G4X8ibJCpbNp6yX0GPd6iaNDf4f2C14KJUJDMH9dWZDYlpKQCWe6dwGVXiampoRGdCMo5sJcRw/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1#imgIndex=0)

通过新基准测试发现，尽管GPT-4o或Gemini等顶尖模型在描述物理现象时看似头头是道，但在真正涉及到具体数值计算——如速度、加速度和尺寸估算时，表现甚至无法超越依靠直觉的人类。

研究指出了当前模型的一个致命弱点：它们更像是一个只会背诵课本知识的文科生，而不是一个懂得利用视觉数据进行精密计算的理科生。

### 模型物理推理的评估

通用的视觉智能系统发展至今，已经能够处理海量的现实世界数据。

这些模型在训练过程中隐式地接触到了视觉观察背后的抽象物理规律。

对于具身智能（Embodied AI）、增强现实/虚拟现实（AR/VR）以及自动驾驶等应用场景而言，仅仅定性地理解“苹果会掉下来”是远远不够的，系统必须能够精确计算“苹果以多快的速度掉下来”。

现实世界充满了复杂的物理运动。

![Image](https://mmbiz.qpic.cn/mmbiz_png/FW3bYDODsoy8pg6G4X8ibJCpbNp6yX0GPlPD6NoVutzsaPKPFxJmrmniaKyWIO7YdVyIxo66A5ibwcFynuFbxUq5A/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1#imgIndex=1)

过去的评估基准主要基于视觉问答（VQA）的模式，这类测试通常是定性的。

例如，模型只需要在多项选择题中选出答案，或者用文字描述碰撞、跌落等事件。

这种模式存在巨大的局限性。

如果一个模型在估计一辆汽车的长度时，正确答案是3米，模型回答3.1米和回答31米在多选题中可能都被判定为错误，但从物理理解的角度看，31米这一答案显示的错误性质要严重得多——它意味着模型根本没有空间尺度的概念。

为了填补这一空白，研究团队提出了QuantiPhy。

这是一个旨在定量测量视觉语言模型物理推理能力的基准测试。

它不仅仅要求模型回答发生了什么，更要求模型结合视频观察和一个给定的物理先验条件（如已知物体的尺寸），计算出其他物体的运动学属性，包括尺寸、速度和加速度。

QuantiPhy包含超过3355个视频-文本问答对，涵盖了2D和3D运动、静态和动态先验条件，以及多样化的场景。

这套测试标准化的提示词和评分系统，使得不同模型之间的数值准确性比较成为可能。

通过这一基准测试，研究者试图回答一个核心问题：当面对视频中的物体运动时，大模型是真的在运用物理定律进行计算，还是仅仅在根据上下文猜测最合理的数字？

### 科学严谨的运动学推理数据集

数据的质量决定了评估的深度。

为了确保测试的全面性和准确性，QuantiPhy的数据构建过程分为三个主要来源，兼顾了可控性和真实性。

Blender模拟是数据的第一大来源。

利用Blender软件，研究者渲染出视觉逼真且物理合理的场景，涵盖2D和3D运动。

模拟环境的最大优势在于上帝视角的完全控制权，这保证了地面真值（Ground Truth）的绝对精确。

通过脚本自动化，研究者可以提取场景中任何物体在任何时间点的精确位置、速度和加速度。

此外，模拟环境允许研究者在保持运动轨迹不变的情况下，系统地改变背景复杂度，从而探究视觉干扰对模型推理的影响。

场景的尺度跨度极大，从微观的红细胞在血管中流动，到宏观的星系运动，充分测试了模型对不同尺度的适应能力。

实验室捕捉是第二个数据来源。

研究者利用多视角立体视觉设置，重建了现实世界中的4D（3D空间+时间）物体运动。

这些捕捉涵盖了自由落体、斜坡滑动、钟摆运动以及反弹等多种物理现象，同时也引入了可滚动和可变形的物体。

对于这类数据，标注过程极为复杂，需要结合深度相机和多视角几何来获取精确的物理量。

互联网抓取则是为了扩展数据的多样性，涵盖更多野外场景。

但这部分数据的筛选标准极为严苛。入选的视频必须是单目摄像机拍摄的，且摄像机需保持相对静止，视频中必须包含已知尺寸的参考物体（如标准硬币），以便将像素坐标转换为真实世界的物理单位。

![Image](https://mmbiz.qpic.cn/mmbiz_png/FW3bYDODsoy8pg6G4X8ibJCpbNp6yX0GPot93fl1u4EGA7wshoNp2bdhM6ic5SyAZwKNENvhUNC8ciaNVlee1WT8Q/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1#imgIndex=2)

对于这些视频，研究者通过人工标注像素空间中的尺寸和位移，并利用参考物体计算出比例尺。

数据被划分为四个核心任务类别，由两个维度定义：维度（2D或3D）和物理先验（静态或动态）。

![Image](https://mmbiz.qpic.cn/mmbiz_png/FW3bYDODsoy8pg6G4X8ibJCpbNp6yX0GPYnP87XZszANQrtHYVjzdxe8RmETiaPniab6GicdZ90BknBEZ69n0WreHQ/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1#imgIndex=3)

2D运动假设物体仅在图像平面内移动，深度不变；而3D运动则包含深度变化，难度更高。

静态先验指的是提供物体的尺寸，动态先验则是提供物体在某一时刻的速度或加速度。

为了准确衡量模型预测值与真实值之间的差距，研究团队没有采用简单的精确匹配率，而是使用了平均相对准确率（Mean Relative Accuracy, MRA）。

由于物理测量本身带有连续性和噪声，MRA通过设定一系列容忍阈值，计算相对误差在阈值范围内的比例，从而提供了一个比单一阈值更稳健的评估指标。

在新的QuantiPhy基准上，即使是目前最先进的模型，如ChatGPT-5.1和Gemini 2.5 Pro，其平均得分也仅在50%左右。

![Image](https://mmbiz.qpic.cn/mmbiz_png/FW3bYDODsoy8pg6G4X8ibJCpbNp6yX0GPRmy4qmshLb4z1ydnMul7o8voxxeJP5tTSL3WBOiaXxOEf9ryK4ASSRg/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1#imgIndex=4)

这表明，定量运动学推理对当前的视觉语言模型来说仍然是一个巨大的挑战。

人类在这一测试中的平均表现为55.6分。

这产生了一个有趣的现象：理论上，计算机模型可以直接访问视频的像素张量，理应能够通过精确的计算得出比人类更准确的结果。人类只能依靠粗略的视觉估计（如数地砖格子），而模型拥有像素级的精度。

然而，最顶尖的模型也未能显著超越人类的直觉估算水平，甚至在许多子任务上落后于人类。这暗示了模型并没有充分利用其在像素层面的感知优势。

模型在处理动态任务（提供速度或加速度作为先验）时的表现通常优于静态任务（提供尺寸作为先验），这可能是因为动态任务通常涉及的时间跨度较短，或者是模型在处理时间序列信息时具有某种特定的偏好。

开源模型与闭源模型之间存在明显的性能差距。

Qwen3-VL-Instruct-32B是表现最好的开源模型，得分为46.0，已经接近甚至超过了一些中等水平的闭源模型。

模型规模的扩大确实带来了性能的提升，例如在Qwen3系列中，从2B参数增加到32B参数，得分有显著提高。

但这并没有解决根本问题，单纯的规模扩展似乎遭遇了边际效应递减，无法填补通向精确物理推理的鸿沟。

### 揭秘模型推理背后的幻觉机制

为了深入探究模型到底是如何得出这些数字的，研究团队进行了一项极具揭示性的反事实分析（Counterfactual Analysis）。

这是整项研究中最令人深思的部分。

实验设计非常巧妙：研究者修改了提示词中的物理先验数值，但保持视频内容不变。

例如，视频中是一辆普通的汽车在行驶，真实的物理先验是汽车长度为4.5米。在反事实测试中，研究者可能会告诉模型这辆汽车的长度是450米或者0.045米。

理论上，如果模型真的是在进行视觉推理，它应该根据给定的先验（哪怕这个先验很荒谬）来计算速度。

如果车长变成原来的100倍，那么计算出的速度也应该相应变成原来的100倍。

然而，实验结果令人大跌眼镜。

当先验数值被放大或缩小时，大多数模型的输出结果并没有随之线性变化。相反，模型的预测值仍然顽固地停留在该物体在现实世界中的典型数值范围内。

这意味着，当模型被问及视频中汽车的速度时，它并不是在测量像素移动了多少，然后结合给定的车长进行计算。

它是在识别出视频里是一辆汽车，然后调用其巨大的训练记忆，检索出汽车通常行驶速度的分布，最后输出一个看起来合理的数字。

模型完全忽略了用户提供的特定物理条件和视频中的实际视觉证据。

它们是在利用世界知识进行一种高级的猜谜游戏，而不是在进行物理测量。

对于需要精确交互的具身智能来说，这种特性是灾难性的。

如果一个机器人通过记忆而非实时测量来判断物体的运动，那么在遇到非标准物体（如一个巨大的模型玩具车）时，它必然会做出错误的决策。

![Image](https://mmbiz.qpic.cn/mmbiz_png/FW3bYDODsoy8pg6G4X8ibJCpbNp6yX0GP1VN8nV4ymOLhyGLDaMkL2WEs8IJZj8xa6RSG3CASMrCgELiag5qRIGQ/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1#imgIndex=5)

上表进一步证实了这一点。

在Prior only（仅提供文本先验，不看视频）的设置下，许多模型的得分竟然与Video + Prior（视频加先验）的得分相差无几。

这直接证明了视频信息在模型的推理过程中起到的作用微乎其微。

模型仅仅依靠文本描述就能猜个八九不离十，这说明它们根本就没有在看视频来解决物理问题。

### 模型表现的影响因素

针对模型推理过程的黑盒性质，研究者还尝试了使用思维链（Chain-of-Thought, CoT）提示，试图引导模型一步步完成计算。

提示被分解为四个步骤：测量像素层面的源属性、计算比例尺、测量像素层面的目标属性、计算真实世界的目标属性。

出乎意料的是，这种结构化的引导并没有系统性地提升模型性能。

在测试的21个模型中，只有3个模型（ChatGPT-5, Fuyu-8B等）表现出了轻微的提升，而其余19个模型在CoT模式下的表现甚至不如直接提问。

原因在于模型难以可靠地解决中间的数值子问题。

将任务分解反而引入了更多的误差源。

如果第一步的像素测量就错了，后续的比例计算和最终换算只会将错误放大。

当前的视觉语言模型似乎缺乏执行精确算法步骤的能力，它们更擅长直接生成最终答案，而不是进行多步的逻辑推演。

研究还分析了视觉背景对模型表现的影响。

![Image](https://mmbiz.qpic.cn/mmbiz_png/FW3bYDODsoy8pg6G4X8ibJCpbNp6yX0GPMwia6VOmdOPqwVxcuh2XHriaICIdU05pbZTZRTqWibk8USXUO6LBroQHQ/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1#imgIndex=6)

结果显示，背景的视觉复杂度对模型性能的影响相对较小。

有趣的是，在视觉复杂的场景（如真实的街道背景）中，模型的表现甚至略好于简单的纯色背景。

这可能是因为真实背景提供了更多的隐式参考线索（如地砖、窗户），帮助模型更好地推断尺度和运动。

相比之下，场景中物体的数量对性能有更清晰的影响。

包含多个移动物体的场景通常能获得更高的准确率。

这是因为更多的物体提供了更多的参考系，模型可以利用物体之间的相对关系来进行校准。

这表明，虽然模型不擅长绝对的像素测量，但它们具备一定的利用关系结构进行推理的能力。

QuantiPhy基准测试的研究结果为当下的AI热潮泼了一盆冷水，但也指明了方向。

现有的视觉语言模型在定量物理推理方面存在本质的缺陷：输入不忠实（Input Unfaithfulness）。

模型过于依赖预训练的参数化知识，导致其在面对具体的、即时的视觉物理任务时，表现得更像是一个依靠经验的估算者，而不是一个依靠数据的测量者。

这一发现对于致力于开发通用具身智能代理的研究者来说至关重要。

如果AI要在物理世界中与物体进行复杂的交互，它必须学会真正地看和算，而不是依赖记忆。

未来的研究需要构建包含旋转动力学、柔体形变以及复杂多体交互的更全面的数据集，并探索新的训练方法，如引入物理感知目标函数或在富含物理信息的专门数据上进行预训练。

只有当模型能够根据视觉输入和物理约束进行严谨的数学推理，而不是从记忆库中检索似是而非的答案时，AI才能真正走进物理世界。

参考资料：

https://quantiphy.stanford.edu/

https://arxiv.org/pdf/2512.19526

https://github.com/Paulineli/QuantiPhy

END

点击图片立即报名 👇️

  

![图片](https://mp.weixin.qq.com/www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg%20stroke='none'%20stroke-width='1'%20fill='none'%20fill-rule='evenodd'%20fill-opacity='0'%3E%3Cg%20transform='translate(-249.000000,%20-126.000000)'%20fill='%23FFFFFF'%3E%3Crect%20x='249'%20y='126'%20width='1'%20height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E) ![图片](https://mp.weixin.qq.com/www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg%20stroke='none'%20stroke-width='1'%20fill='none'%20fill-rule='evenodd'%20fill-opacity='0'%3E%3Cg%20transform='translate(-249.000000,%20-126.000000)'%20fill='%23FFFFFF'%3E%3Crect%20x='249'%20y='126'%20width='1'%20height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E) ![图片](https://mp.weixin.qq.com/www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg%20stroke='none'%20stroke-width='1'%20fill='none'%20fill-rule='evenodd'%20fill-opacity='0'%3E%3Cg%20transform='translate(-249.000000,%20-126.000000)'%20fill='%23FFFFFF'%3E%3Crect%20x='249'%20y='126'%20width='1'%20height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E) ![图片](https://mp.weixin.qq.com/www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg%20stroke='none'%20stroke-width='1'%20fill='none'%20fill-rule='evenodd'%20fill-opacity='0'%3E%3Cg%20transform='translate(-249.000000,%20-126.000000)'%20fill='%23FFFFFF'%3E%3Crect%20x='249'%20y='126'%20width='1'%20height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

继续滑动看下一个

AIGC开放社区

向上滑动看下一个