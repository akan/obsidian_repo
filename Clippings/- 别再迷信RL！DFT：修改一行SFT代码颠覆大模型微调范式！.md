---
title: "别再迷信RL！DFT：修改一行SFT代码颠覆大模型微调范式！"
source: "https://mp.weixin.qq.com/s/BmSXdSE2j8zpC7jZILzCOg"
author:
  - "[[Tensorlong 看天下]]"
published:
created: 2025-08-11
description: "❝一句话概括，给SFT的损失函数加个权重，性能就能吊打PPO，强化学习的尽头是“有文化的”监督学习。"
tags:
  - "DFT"
  - "监督微调"
  - "强化学习"
  - "泛化能力"
abstract: "DFT通过动态调整SFT损失函数的权重，显著提升模型泛化能力，性能超越PPO等复杂强化学习方法。"
---
Original Tensorlong 看天下 *2025年08月11日 08:20*

![Image](https://mmbiz.qpic.cn/sz_mmbiz_png/Z24DyenWDNiabl2hqJZMMLfK3LlJNiczT2BSjqvck7ib7zn04icRpnib6RvdZ19iaMlvtJnpoTwyZRIODJkGdPpZiap4g/640?wx_fmt=png&from=appmsg&watermark=1&tp=webp&wxfrom=5&wx_lazy=1)

> ❝
> 
> 一句话概括，给SFT的损失函数加个权重，性能就能吊打PPO，强化学习的尽头是“有文化的”监督学习。原来SFT的梯度里藏着个“反骨仔”，专门放大噪声，DFT做的就是精准“策反”这个内鬼，一行代码的修改，让SFT摆脱了"死记硬背"的标签，证明了简单的数学原理有时比复杂的炼丹工程更有效。
> 
> （原论文题目见文末，点击阅读原文可直接跳转至原文链接， Published on arxiv on 07 Aug 2025, by Southeast University, UCLA, Shanghai Jiao Tong University）

亲爱的读者们，沈公子的公众号agent🤖和base model升级到v3.0，今后公众号文章行文会更流畅，处理公式和符号也完全达到人类专家水准，会大幅减少出现错乱和显示异常的情况，提升阅读体验。enjoying:) 现已提供论文解读和idea讨论定制辅导服务，可扫描二维码后台联系

### 第一阶段：识别核心概念

#### 论文的motivation分析

当我们想让一个大型语言模型（LLM）学会一项新技能，比如解数学题。最直接的方法就是“监督微调”（SFT）：我们找来一大堆专家解题的范例（问题+标准答案），然后像教学生抄作业一样，让模型去模仿，做得越像越好。

这种方法简单粗暴，效果也不错，模型能很快学会专家的“套路”。但问题也随之而来：模型学到的只是“形似”，而不是“神似”。它可能只是死记硬背了范例的解题步骤，一旦遇到题型稍微变化的“新题”，就很容易出错。这就是所谓的 **泛化能力差** 。

相比之下，另一种更高级的训练方法——强化学习（RL），表现要好得多。RL不只是让模型模仿，而是给它一个“奖励信号”（比如“答对了”或“答错了”），让模型自己去探索、试错，从而学到更根本、更通用的解题策略。因此，RL训练出的模型通常泛化能力更强。

然而，RL的缺点也很明显：它太复杂了！不仅需要大量的计算资源，对参数调节非常敏感，而且还需要一个明确的奖励机制，这在很多场景下是难以获得的。

这就引出了作者的核心动机： **我们能不能不引入强化学习的复杂性，仅仅通过改造简单易行的SFT方法本身，就能让它的泛化能力追上甚至超越RL呢？** 尤其是在我们只有专家范例，没有额外奖励信号或负样本的情况下，这显得尤为重要。

#### 论文主要贡献点分析

- **主要创新点**
- **理论突破** ：首次从数学上严谨地证明，标准的SFT可以被看作是一种“有缺陷”的强化学习。
	- **问题定位** ：明确指出了SFT泛化能力差的“病根”——其梯度更新中隐藏着一个不合理的“奖励结构”。这个结构会过分放大模型对低概率（即模型觉得很难、不确定）的正确词元（token）的模仿，导致模型“钻牛角尖”，从而损害了泛化能力。
	- **提出新方法（DFT）** ：基于上述理论分析，提出了一种名为“动态微调”（Dynamic Fine-Tuning, DFT）的新方法。该方法通过一个极其简单的操作，动态地调整每个词元的学习权重，从而“修正”了这个有缺陷的奖励结构。
	- **“一行代码”的奇迹** ：作者强调，DFT的实现异常简单，相较于标准SFT的训练代码，几乎只需要增加一行。
- **支撑创新的关键技术或方法**
- **重要性采样（Importance Sampling）** ：这是连接SFT和RL的理论桥梁。作者利用这个数学工具，巧妙地将SFT的梯度公式“变形”，改写成了强化学习中策略梯度的形式。
	- **策略梯度理论（Policy Gradient Theory）** ：通过将SFT梯度与标准的策略梯度公式进行对比，作者才得以发现那个隐藏的、与词元概率成反比的“病态”权重。
	- **动态重加权（Dynamic Reweighting）** ：这是DFT方法的核心机制。具体来说，就是在计算每个词元的损失时，乘以该词元自身的生成概率。这个操作恰好抵消了那个“病态”的反比权重，从而稳定了训练过程。
- **论文的显著性结果**
- **性能大幅超越SFT** ：在多个极具挑战性的数学推理基准测试上，DFT带来的性能提升远超标准SFT。在某些SFT甚至导致性能下降（过拟合）的难题上，DFT依然能取得显著的正向收益。
	- **媲美甚至超越RL方法** ：令人惊讶的是，当把DFT应用于一个有奖励信号的“离线强化学习”场景时，这个简单的改进版SFT，其性能竟然超过了DPO、PPO等一众复杂且成熟的RL算法。这证明了其方法的普适性和强大效果。
	- **学习更高效** ：实验表明，DFT不仅效果好，收敛速度也比SFT更快，能用更少的训练数据达到更好的效果。

#### 理解难点识别

- **理解论文的关键概念**
- 理解SFT（监督微调）和RL（特别是策略梯度）的基本思想。
	- **最关键、最核心的理解点** ：论文如何将SFT的梯度更新与RL的策略梯度联系起来。这是整篇论文的理论基石。
	- 理解“重要性采样”在这一理论推导中扮演的角色。
	- 理解DFT方法是如何通过“动态重加权”来修正SFT的内在缺陷的。
- **最具挑战性的部分**
- 最具挑战性的部分无疑是 **SFT梯度到策略梯度的数学推导过程（论文中的公式5和6）** 。对于没有深入了解过强化学习理论的读者来说，这一步会显得非常抽象和突然。为什么一个看似简单的“求最大似然”的SFT，内在逻辑会和“带奖励的试错学习”的RL扯上关系？
- **需要重点解释的核心概念**
- 基于以上分析，最需要深入解释的核心概念就是： **“SFT在本质上是一种带有‘病态’隐式奖励的强化学习”** 。

#### 概念依赖关系

- **核心概念间的关系**
- 首先要明白 **SFT** （模仿学习）和 **RL** （奖励学习）是两种不同的范式。
	- 论文的核心洞见是通过 **重要性采样** 这个“数学魔术”，揭示了 **SFT的内在机制** 可以被等价地视为一种 **RL机制** 。
	- 这个等价关系暴露了SFT的 **“病根”** ：一个不合理的、会导致训练不稳定的 **隐式奖励** 。
	- 为了“治病”，论文提出了 **DFT** 方案，其目标就是“修正”这个奖励。
- **解释的最佳切入点**
- 最佳切入点是聚焦于 **SFT和RL之间的那座理论桥梁** ，讲清楚它是如何被建立的，以及它暴露了什么问题。

---

### 第二阶段：深入解释核心概念

#### 设计生活化比喻

- **场景选择：学生厨师学艺记**
- 想象一位 **学生厨师** （Student Chef），他正在学习制作一道由 **米其林大师** （Master Chef）创造的传世名菜。他手里的教材是一本详尽的 **《大师菜谱》** （Recipe Book）。
	- 这道菜非常复杂，有几十个步骤。学生的目标就是完美复刻。
	- 负责监督他训练的是一位严格的 **主厨** （Head Chef）。
- **核心机制的比喻**
- **传统SFT教学法（严苛的模仿）** ：主厨的教学方法非常死板。他拿着菜谱，对着学生的每一步操作进行检查。如果学生哪一步做错了，主厨就会严厉批评。但这位主厨有个怪癖：对于菜谱里那些 **特别难、特别反直觉的步骤** （比如，要求在特定温度下加入一种罕见香料），如果学生做对了，主厨会觉得“嗯，还行”；但如果学生稍有犹豫或者做错了，主厨就会 **勃然大怒，用比平时高八度的声音咆哮** 。这种教学方式的后果是：学生变得异常紧张，把所有精力都用来攻克那几个最难的步骤，生怕被主厨骂。他最终成了一个完美的模仿机器，但对于食材的特性、烹饪的原理一无所知。这就是SFT的死记硬背，泛化能力差。
	- **DFT教学法（智慧的引导）** ：现在，主厨换了一种更智慧的教学方法。他仍然要求学生遵循菜谱，但在评估时，他会 **考虑学生的“信心”** 。如果一个步骤，学生 **非常不确定、战战兢兢地才做对** （低概率事件），主厨会温和地鼓励：“做得不错，继续保持。”如果一个步骤，学生 **非常自信、一气呵成地做对了** （高概率事件），主厨会给予更强烈的肯定：“太棒了！这步是关键，你掌握得很好！”这种“动态调整”的教学法，让学生不再过度恐惧和纠结于少数几个难点，而是能够更均衡地理解整个烹饪流程。这就是DFT带来的稳定学习和强大泛化能力。

#### 建立比喻与实际技术的对应关系

- **比喻中的关键元素**
- **学生厨师** → **语言模型（LLM）** ，其参数为 。
	- **大师菜谱** → \*\*专家示范数据集 \*\*，其中 是标准答案。
	- **主厨的评估** → **梯度更新（Gradient Update）** ，即模型参数调整的方向和幅度。
	- **主厨的咆哮** （对难点反应过度） → 标准SFT梯度中隐藏的 权重。当模型对某个正确词元的预测概率 很低时（学生很没信心），这个权重就会变得非常大，导致梯度爆炸式增长（主厨咆哮）。
	- **主厨的“智慧引导”（动态调整反馈强度）** → **DFT的动态重加权机制** 。通过给损失乘以 ，正好抵消了那个 的“咆哮”因子。
	- **学生对某个步骤的“信心”** → \*\*模型对某个词元的预测概率 \*\*。
- **对应关系的合理性**
- 这个比喻非常贴切地抓住了核心矛盾。标准SFT的交叉熵损失函数 的特性就是，当概率 趋近于0时，损失会急剧增大。论文的理论推导将这种特性显式地表达为了强化学习梯度中的 权重，这与“对不确定的事情惩罚更重（咆哮）”完美对应。而DFT的核心操作就是用 去乘以 ，有效地抑制了 很小时的剧烈变化，这与“智慧引导，温和鼓励”的理念完全一致。

#### 深入技术细节

- **从SFT到RL的“变形记”** 标准的SFT目标是最大化模型生成专家答案 的对数概率。其损失函数为：
	其梯度非常直观：
	接下来是点石成金的一步。作者运用 **重要性采样** ，将这个在“专家数据分布 ”下的期望，强行转换为了一个在“模型自身生成分布 ”下的期望。
	这个变形后的公式完全就是强化学习中 **REINFORCE算法** 的策略梯度形式！
	这个 的权重意味着： **模型生成某个答案的概率越低，一旦它碰巧猜对了（ ），它获得的“有效奖励”就越高。** 这就完美地对应了我们比喻中“主厨的咆哮”：学生越是没信心的事，一旦做对了，受到的刺激（梯度）就越大。
- ：这是策略梯度的核心，告诉我们参数更新的方向。
	- ：这就是 **奖励函数** 。当模型生成的答案 和专家答案 完全一样时，奖励为1，否则为0。这是一个非常稀疏的奖励。
	- ： **这就是“病根”所在！** 它是一个 **重要性权重** ，但在RL的视角下，它成了一个 **乘在奖励前面的系数** 。
- **原始数学形式** (Eq. 5 & 6):
	- **符号替换版本:**`SFT损失的梯度 = - 对模型自己生成的答案y的期望 [ (1 / 模型生成这个答案y的概率) * (如果y和专家答案y*一样，则为1，否则为0) * (调整模型参数以增加“生成这个答案y的概率”)的方向 ]`
- **原始数学形式** (Eq. 2):
	- **符号替换版本:**`SFT损失的梯度 = 对于所有专家数据(问题, 答案*)的平均 [ - (调整模型参数以增加“生成正确答案*的概率”) 的方向 ]`
- **原始数学形式** (Eq. 1):
	- **符号替换版本:**`SFT的总损失 = 对于所有专家数据(问题, 答案*)的平均 [ -log(模型在给定问题时，生成正确答案*的概率) ]`
- **DFT的“对症下药”** DFT的做法简单而优雅：在计算损失时，直接给原始的SFT损失乘以一个修正因子 。
	这个修改带来的效果是，当计算梯度时，这个 修正因子会保留下来，正好与SFT梯度中那个隐藏的 权重相乘，两者几乎抵消（在 的情况下），从而移除了“病态”奖励。
- **原始数学形式** (Token-level DFT Loss, Eq. 9):
	- **符号替换版本:**`DFT的总损失 = 对于所有专家数据(问题, 答案*)的平均 [ - (对答案*中的每一个词元t求和 [ (把“模型在正确历史下生成正确词元t的概率”当作一个固定数值) * log(模型在正确历史下生成正确词元t的概率) ]) ]`

#### 将技术细节与比喻相互映射

- 公式(6)中的 ↔️ **主厨的咆哮** ：学生越没信心（ 越小），主厨的批评声越大（ 越大）。
- 公式(9)中的 ↔️ **主厨的智慧引导** ：主厨根据学生的信心（ ）来调整自己的反馈音量。
- 操作 ↔️ **主厨先决定音量再开口** ：主厨调整音量这个行为本身，并不会改变他要说的批评内容（ 的梯度），只是改变了说这番话的方式（强度）。
- **比喻的局限性** ：这个比喻将梯度简化为了“反馈的强度/音量”。真实的梯度是一个高维向量，决定了模型参数更新的复杂方向，而不仅仅是一个标量强度。

#### 总结

- **核心联系重申** ：“学生厨师学艺”的比喻生动地揭示了：标准SFT就像一个严苛但方法有问题的教练。而DFT则像一个智慧的教练，他懂得因材施教，通过动态调整反馈强度，让学生学得更稳、更全面。
- **对应关系的关键** ：这种教学方法的差异，在数学上就体现为SFT梯度中隐藏了一个与概率成反比的 权重，而DFT通过乘以 巧妙地化解了这个问题。
- **比喻总结数学原理** ： **不要因为学生在难题上表现出不自信就过度惩罚他。** 在数学上，这意味着应该 **降低那些低概率（高不确定性）正确词元的损失权重** ，从而创造一个更稳定、更鲁棒的学习信号。

---

### 第三阶段：详细说明流程步骤

1. **输入准备**
- **模型** ：加载预训练语言模型及其分词器（Tokenizer）。
	- **数据** ：加载专家示范数据集 。每条记录是一个（输入, 输出）对，记为 。
3. **批处理与分词**
- 从数据集中随机抽取一小批（a mini-batch）数据。
	- 使用分词器将输入 和专家输出 都转换成词元ID序列，如 被转换为 。
5. **核心计算流程：前向传播** 在“教师强制（Teacher Forcing）”模式下，对于专家答案 中的每一个词元，从 到 ，循环执行：
- **构建模型输入** ：将输入 和正确答案前缀 拼接起来。
	- **获取模型预测** ：将输入送入LLM，得到对下一个词元的概率分布。
	- **提取关键信息** ：从分布中找到专家词元 对应的概率值，记为 。
	- **计算标准损失** ：计算该词元的标准交叉熵损失： 。
7. **DFT的魔法：动态重加权**
- **计算DFT词元损失** ：将上一步的概率和损失相乘，得到该词元的DFT损失： 。
	- **应用Stop-Gradient** ：在反向传播中，系统将 视作一个常数，其梯度不被计算。
9. **损失聚合与反向传播**
- **计算序列总损失** ：将样本中所有词元的DFT损失加起来： 。
	- **计算批次总损失** ：对mini-batch中所有样本的 求平均。
	- **执行反向传播** ：基于最终的批次损失，计算模型所有参数 的梯度。
11. **参数更新**
- 使用优化器（例如AdamW），根据梯度更新模型参数 。
	- **循环** ：清空梯度，处理下一个mini-batch。
13. **最终输出**
- 训练结束后，得到一个经过DFT微调的、泛化能力更强的语言模型。

---

### 第四阶段：实验设计与验证分析

#### 主实验设计解读：核心论点的验证

- **核心主张**
- DFT能够显著提升模型的泛化能力，尤其是在SFT容易过拟合的挑战性任务上，其表现远超标准SFT。
- **实验设计**
- 作者选择了多个开源大模型，分别使用 **标准SFT** 和 **DFT** 在同一个数学推理数据集上进行微调。最后，在五个独立的、高难度的数学测试集上评估性能。
- **选择的合理性分析**
- **数据集** ：训练集（NuminaMath CoT）是大规模、高质量的数学思想链数据集。测试集（Math500, Minerva Math, Olympiad Bench, AIME, AMC）是数学推理领域的 **公认基准** ，难度高，能有效检验泛化能力。
	- **评价指标** ： **Average@16** ，即模型生成16个答案，任一对即算通过。对于复杂推理任务，这是比单次正确率更稳定、更公平的评价方式。
	- **基线方法** (Baselines)： **标准SFT** 作为最核心的比较对象。 **基座模型** 用于对比微调带来的影响。 **iw-SFT** 作为同期的相关工作，证明DFT的相对优势。
- **实验结果如何支撑核心贡献**
- **Table 1** 的数据是铁证。 **普遍优越性** 体现在所有模型上，DFT的平均分都显著高于SFT。 **解决过拟合问题** 体现在高难度测试集上，SFT微调后性能反而下降，而DFT则能大幅提升性能。这种“反败为胜”的对比，强有力地证明了DFT的核心价值： **抑制过拟合，提升泛化** 。

#### 消融实验分析：内部组件的贡献

论文通过 **参数敏感性分析（Figure 3）** ，排除了超参数的干扰。

- **被系统性改变的部分**
- **训练超参数** ，主要是 **学习率** 和 **批量大小** 。
- **对应的创新点**
- 这个实验旨在回答质疑：“DFT效果好，是否只是因为超参数调得好？”
- **结果如何证明必要性**
- Figure 3的图表显示，在多组不同的超参数设置下， **DFT的性能曲线始终稳定地位于SFT的上方** 。这证明了DFT的性能增益是其内在机制带来的，是 **根本性** 的，而非偶然现象。

#### 深度/创新性实验剖析：洞察方法的内在特性

- **1\. 可视化分析：词元概率分布（Figure 2）**
- **实验目的** ：直观展示不同方法如何改变词元概率，从机理上解释DFT为何能更好地泛化。
	- **巧妙设计** ：将训练后模型对训练集答案的预测概率绘制成直方图，并使用对数尺度观察低概率区域。
	- **实验结论** ：SFT倾向于“普涨”，而DFT表现出 **“极化”效应** ——让模型对核心语义词元 **极度自信** ，同时对语法连接词等“填充词” **更不自信** 。这揭示了DFT的学习策略是从“死记硬背”到“理解学习”的转变。
- **2\. 探究性实验：在离线RL场景下的表现（Table 3）**
- **实验目的** ：进行“压力测试”，探索DFT在纯SFT场景之外的普适性。
	- **巧妙设计** ：在一个有奖励信号的离线RL数据上训练DFT，并与DPO、PPO等专业的RL算法进行比较。
	- **实验结论** ：DFT这个“SFT的简单改进版”，在RL的“客场”中，不仅 **远超** 了离线RL方法，甚至还 **击败** 了复杂的在线RL方法。这暗示了DFT遵循的“稳定梯度信号”是一个更为 **普适和强大** 的学习法则。
- **3\. 效率分析：学习动态曲线（Figure 1）**
- **实验目的** ：证明DFT不仅效果好，而且学习效率高。
	- **巧妙设计** ：绘制模型准确率随训练步数变化的曲线。
	- **实验结论** ：DFT的性能曲线攀升得更快，能在更少的训练步数内达到甚至超过SFT的最佳性能，说明其梯度信号 **信息密度更高、噪声更小** 。

---

本文题目：On the Generalization of SFT: A Reinforcement Learning Perspective with Reward Rectification

**欢迎Deep Learning同好与我交流、讨论、合作！**

**现已提供论文解读和idea讨论定制服务，可私信后台联系**

**计算机顶会论文代表了该领域最前沿的研究进展，为帮助大家了解最新的方法创新、模型架构和应用场景，我精心整理了【 2023-2025年顶会获奖论文合集 】，包括CVPR、Neurips、AAAI、ACL、ICLR、EMNLP等，共计100+篇。**

**扫码免费获取顶会获奖论文合集**

**![图片](https://mp.weixin.qq.com/s/www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg%20stroke='none'%20stroke-width='1'%20fill='none'%20fill-rule='evenodd'%20fill-opacity='0'%3E%3Cg%20transform='translate(-249.000000,%20-126.000000)'%20fill='%23FFFFFF'%3E%3Crect%20x='249'%20y='126'%20width='1'%20height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)**

**![图片](https://mp.weixin.qq.com/s/www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg%20stroke='none'%20stroke-width='1'%20fill='none'%20fill-rule='evenodd'%20fill-opacity='0'%3E%3Cg%20transform='translate(-249.000000,%20-126.000000)'%20fill='%23FFFFFF'%3E%3Crect%20x='249'%20y='126'%20width='1'%20height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)**

**公众号广告位招租，欢迎咨询👏**

  

个人观点，仅供参考

[Read more](https://mp.weixin.qq.com/s/)

继续滑动看下一个

沈公子今天读什么

向上滑动看下一个