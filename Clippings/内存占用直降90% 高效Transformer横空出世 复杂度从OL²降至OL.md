---
title: "内存占用直降90% | 高效Transformer横空出世！复杂度从O(L²)降至O(L)"
source: "https://mp.weixin.qq.com/s/iIGxfNTlcTimv8op3s035Q"
author:
  - "[[小书童]]"
published:
created: 2025-12-08
description: "精简阅读版本本文主要解决了什么问题1. 标准自注意力机制的时间和内存复杂度相对于输入序列长度呈平方级增长，这"
tags:
  - "高效注意力"
  - "线性复杂度"
  - "稀疏计算"
  - "长上下文建模"
abstract: "本文系统综述了通过线性注意力与稀疏注意力两大范式来降低Transformer自注意力机制的二次复杂度，以实现高效长上下文建模的最新研究进展、硬件实现及其在大语言模型中的整合应用。"
---
Original 小书童 *2025年12月1日 09:00*

![Image](https://mmbiz.qpic.cn/sz_mmbiz_jpg/5ooHoYt0tgmSAYzIsN8yDG6GxJe8fV0pXwiaJuDwzxfGgP0mQtKCoicMsTXD7S3SP6icic2uycbicDFZSBicF4Axq3Sw/640?wx_fmt=jpeg&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1#imgIndex=0)

## 精简阅读版本

### 本文主要解决了什么问题

1. 1\. 标准自注意力机制的时间和内存复杂度相对于输入序列长度呈平方级增长，这成为扩展大语言模型处理长上下文的基本障碍。
2. 2\. 如何在保持模型性能的同时，提高注意力机制的效率，使其能够处理更长的序列。
3. 3\. 如何将高效注意力机制集成到大规模预训练语言模型中，包括纯高效架构和混合设计。
4. 4\. 如何在算法创新和硬件实现之间取得平衡，以实现高效部署。

### 本文的核心创新是什么

1. 1\. 系统性地分类和总结了两大类高效注意力机制：线性注意力和稀疏注意力，并详细分析了它们的算法原理和实现方式。
2. 2\. 提出了线性注意力的三大主要范式：核线性注意力、带遗忘机制的线性注意力、以及将线性注意力作为上下文学习器的方法。
3. 3\. 将稀疏注意力分为固定模式稀疏性、块稀疏性和基于聚类的稀疏性，并分析了各自的特点和适用场景。
4. 4\. 探讨了高效注意力机制的硬件实现考虑，包括并行表示、循环表示和分块递归表示，以及它们在计算复杂度、内存占用和训练/推理工作流程兼容性方面的权衡。
5. 5\. 分析了高效注意力机制在大规模预训练语言模型中的整合方式，包括统一高效架构和混合设计，为可扩展高效语言模型的设计提供了基础参考。

### 结果相较于以前的方法有哪些提升

1. 1\. 计算复杂度从O(L²)降低到O(L)或更低，使得模型能够处理更长的序列，如MiniCPM-4能够扩展到极长的序列。
2. 2\. 内存使用显著减少，使得在有限硬件资源上部署大语言模型成为可能，如Falcon Mamba和Codestral Mamba支持256k token的上下文长度。
3. 3\. 推理速度提升，某些线性注意力模型如RetNet、Eagle和Lightning Attention实现了常数量级的推理复杂度。
4. 4\. 在保持或接近标准Transformer性能的同时，提高了模型的可扩展性，如Mamba在语言建模任务上可以超越同等规模甚至更大规模的Transformer模型。
5. 5\. 混合注意力设计如Jamba、MiniMax-01、Gemma 3等在计算效率和上下文建模能力之间取得了更好的平衡，通过交替使用不同类型的注意力层实现了更高效的架构。

### 局限性总结

1. 1\. 线性注意力方法通常在表达能力上有所妥协，可能导致性能下降，如元素线性注意力受到状态大小的瓶颈限制。
2. 2\. 双向线性注意力机制在因果设置中应用时面临重大挑战，因为基于全局池化的方法往往计算成本高昂，不适合大语言模型。
3. 3\. 稀疏注意力方法可能忽略某些重要的全局依赖关系，影响模型性能，特别是在需要全面上下文理解的任务中。
4. 4\. 多通道线性注意力等高级方法带来了额外的计算开销，需要在训练效率和性能之间进行权衡。
5. 5\. 某些复杂变体（如TTT和Titans）构建显式的分块形式仍然存在挑战，通常依赖较大的批量大小进行内存更新。
6. 6\. 硬件实现与算法设计之间的匹配仍然是一个挑战，需要针对不同的高效注意力机制开发专门的优化技术。

## 深入阅读版本

## 导读

基于Transformer的架构已成为大语言模型的主流 Backbone 。然而，自注意力机制的二次时间与内存复杂度仍然是高效长上下文建模的一个基本障碍。为了解决这一限制，近期研究引入了两种主要的效率注意力机制。线性注意力方法通过核近似、循环公式或快速权重动态实现线性复杂度，从而实现计算开销降低的可扩展推理。相比之下，Sparse注意力技术基于固定模式、块状路由或聚类策略，将注意力计算限制在选定的token子集上，在保持上下文覆盖的同时提升效率。本综述系统地全面概述了这些进展，整合了算法创新和硬件层面的考量。此外，作者分析了高效注意力机制在大规模预训练语言模型中的整合，包括完全基于高效注意力的架构和结合局部与全局组件的混合设计。通过将理论基础与实际部署策略相结合，本研究旨在为推进可扩展高效语言模型的设计提供基础参考。

## 1\. 引言

基于Transformer的架构\[59\]已成为现代大语言模型（LLMs）的 Backbone 选择。尽管取得了成功，标准的自注意力机制仍然是一个显著的计算 Bottleneck ，其时间和内存复杂度相对于输入序列长度呈平方级增长。这一局限性为扩展LLMs以处理越来越长的上下文，同时保持高性能和高效率，带来了重大挑战。

为解决这一问题，已出现两大主要方向以降低softmax Attention的时间和空间复杂度。第一种机制是线性Attention \[17, 22, 48, 51, 68, 70\]，通过重新参数化或近似将softmax attention视为线性操作来降低attention复杂度。第二种候选方案是SparseAttention \[8, 21, 33, 54, 71\]，基于固定或动态Sparse模式将attention计算限制在完整key空间的一个子集上。尽管这两种方法都旨在提高效率，但在公式化、设计选择和硬件影响方面存在显著差异。

本综述全面回顾了高效注意力机制（Efficient Attention mechanisms）的最新进展，重点关注算法原理和系统级实现。在此基础上，作者还研究了采用这些高效注意力机制的预训练大语言模型（Pre-trained LLM）。

作者将线性注意力方法分为三大主要范式。首先，核线性注意力通过特征空间中的内积逼近softmax核，通过随机特征映射\[9, 40\]或固定正映射\[22\]实现线性复杂度。其次，具有遗忘机制的循环线性注意力引入位置感知循环，通过数据无关\[51\]或数据相关衰减\[17, 68\]实现长序列建模，这些机制控制过去信息随时间衰减的方式。第三，快速权重和元学习范式将线性注意力重新解释为在线优化的记忆更新过程，DeltaNet\[48, 70\]和TTT\[49, 50\]等模型将快速学习动态直接整合到状态演化中。作者还考察了线性注意力的硬件友好表示形式——包括并行、循环和分块形式——并突出了它们在计算复杂度、内存占用以及与训练或推理工作流程兼容性方面的各自权衡。

作者将Sparse注意力分类为固定模式Sparse性、块Sparse性和基于聚类的Sparse性。固定模式Sparse性采用静态的token级 Mask ，如滑动窗口、扩张位置或指定的全局token，具有简单性和硬件友好性\[4, 8, 14, 63\]。块Sparse性在块粒度上选择或路由注意力，通过启发式评分\[33, 54, 64\]或可训练的门控\[15, 71\]，实现结构化内存访问和高效的GPU利用。基于聚类的Sparse性使用基于内容或位置感知的分组方法（如k-means或LSH）组织键值对，促进语义感知检索并减少内存开销\[7, 24, 31\]。最后，作者还讨论了双向Sparse设计，将Sparse模式扩展到编码器风格的模型。这些方法在Sparse粒度、选择机制及其与硬件原语（如FlashAttention\[10\]）的匹配度上有所不同，共同构成了现代Transformer中高效长上下文建模的基础。

近期有研究致力于将高效的注意力机制集成到工业级预训练语言模型中。这些研究包括纯高效架构——如线性注意力和状态空间模型，以及结合局部和全局注意力模式的混合设计。EAGLE \[37\]、Falcon Mamba \[77\] 和 MiniCPM4 \[56\] 等模型展示了纯线性或Sparse方法在百亿参数规模上的可扩展性，以常数量级推理提供高性能。同时，混合模型 \[5, 26, 28, 32, 52, 55, 65\] 交替使用密集、Sparse和局部注意力，以平衡计算效率与上下文建模能力，反映了现代大语言模型中组合式、硬件感知注意力设计的增长趋势。

作者的目标是提供一个统一的框架，用于理解在算法和硬件约束下注意力机制的发展演变，以及这些设计如何被集成到可扩展的大语言模型架构中。通过将理论洞察与实践实现相结合，作者希望这篇综述能为致力于高效和可部署模型设计的科研行人和实践者提供有价值的参考。

## 2\. 线性注意力

### 2.1. 核线性注意力

传统的线性注意力方法旨在以与序列长度线性相关的形式近似基于softmax的注意力机制。其核心思想是用基于核的注意力权重近似来替换昂贵的softmax计算。在标准自注意力机制中，每个输出是值 的加权求和，权重由 Query -键相似度的softmax给出：

其中 （ 表示序列长度， 表示每个头的模型维度）。Softmax 生成 Query 和键 的权重 。而核线性注意力则找到一个特征映射 ，使得softmax核在诱导的特征空间中近似为一个简单的点积： \[58\]。给定这样的 ，可以重写注意力为：

通常被选择以产生非负输出，因为 的值域为非负，因此也应用一个归一化除数来模拟softmax概率。这种重新表述将复杂度从 降低到 （甚至可以通过合适的特征维度降低达到 ），因为昂贵且未明确形成的 注意力矩阵被避免了。

线性 Transformer \[22\]用固定的正特征映射替换了softmax核。在实践中，他们设置 。ELU()在整个定义区域内可微，与原始ReLU(.)函数相比，表现出更好的性能。

Performer \[9\] 引入了 \- 一种随机特征方案，该方案无偏地逼近softmax核。它采样随机特征映射 ，使得 。这产生了一个仅使用 运算即可证明无偏的完整softmax注意力估计器。特别是，Performer使用正交随机特征，这减少了近似中的方差。

随机特征注意力\[40\]是一种通过随机傅里叶特征构建的线性注意力机制，用于softmax核。类似于Performer，RFA利用随机映射和三角激活函数来近似softmax。RFA在随机投影前进一步对 Query 和键进行归一化以减少方差。RFA还有一个变体RFA-Gate，该变体添加了一个可选的门控机制以实现时效性偏差。

cosFormer \[41\] 提出使用余弦函数来近似softmax。由于 ，cosFormer 将余弦重新加权注意力 分解为线性注意力形式。

HedgeDog \[75\] 利用尖峰核 ，因为他们观察到Transformer与线性Transformer之间的性能差距是由于缺乏尖峰和单调特性。HedgeDog表现出更好的注意力熵和单调性。

### 2.2. 带遗忘机制的线性注意力

近期的研究工作通过循环神经网络或连续状态空间模型来解释注意力机制。传统线性注意力通常是无位置的，其中递归顺序对输出没有影响，而现代线性注意力则更类似于具有状态跟踪和隐藏记忆的循环神经网络。因此，这些模型明确地结合了递归、门控或状态动态来处理具有线性复杂度的长序列。衰减因子是引入遗忘机制的最重要因素。

#### 2.2.1. 数据无关衰减

记忆网络（RetNet）\[51\] 引入了一种记忆机制，该机制使用固定的衰减系数，以循环式更新替代注意力机制。在RetNet层中，每个时间步t维护一个状态向量 ，该向量通过指数遗忘来聚合过去的输入。递归式可以表示为

当 为一个学习到的衰减因子（每个保留头一个）且 为当前 Token 的新贡献（ 是 的值投影， 是键投影）。输出通过线性“ Query ”投影获得： 。展开方程 3 给出了保留的显式公式：

这表明在时间步 t 时，来自 Token 的贡献通过因子 指数衰减。关键在于，y 是一种与数据无关的衰减，是层的固定参数（在多头保留中通常是每个头一个参数），而不是输入内容的函数。它赋予 RetNet 类似 RNN 的 O(1) 内存更新特性，同时在训练期间仍能通过等效矩阵公式实现并行计算。（例如，可以证明方程 3 等价于“保留矩阵”形式，Retention ，其中 对于 实现了衰减和因果 Mask 。）RetNet 的保留机制与其他与数据无关的递归模型共享主题。

Eagle Eagle \[37\] 通过外积记忆改进了RWKV设计，该设计等同于线性注意力机制。在RWKV系列中，衰减因子被参数化为 ，其中 是一个与数据无关的可学习因子。在实践中，RetNet和Eagle均通过使用固定衰减来遗忘旧信息，实现了线性推理扩展，并展现出具有竞争力的性能。经验上，RetNet为每个注意力头使用固定的标量 （通常每个层包含多个具有不同 值的保留头，形成一种多尺度衰减形式），而Eagle则使用可学习的标量 来参数化衰减因子。

闪电注意力\[42, 44\]还提出了一种线性注意力层，通过每个头固定标量衰减来实现长度不变的计算速度。在闪电注意力中，隐藏状态本质上是 ，其中 是某个常数（由模型学习或设定），这与RetNet的 具有相同的精神，但针对硬件效率进行了优化。

H3 \[12\] 将循环状态空间模型 \[18\] 引入线性注意力机制，通过SSM学习数据无关的指数衰减来处理键值外积隐藏状态。虽然线性注意力机制通过分块计算实现高效训练，但H3需要为SSM计算进行显式状态扩展，从而限制了 Head 维度，导致表达能力受限。

综上所述，数据无关衰减方法维持一个随时间以预定速率衰减的持久状态，从而实现 的回归和每步恒定的内存。它们牺牲了一定的适应性，这促使在更近期的模型中引入数据相关机制。

#### 2.2.2. 数据相关衰减

固定衰减虽然提供了简单性和速度，但可能未能充分利用输入流的信息。门控或数据依赖方法将遗忘因子本身作为当前输入的学习函数。此类循环更新的通用形式为：

其中 表示前一个状态， 是由 Token 确定的门控张量。如果 在某些分量上接近0，那么在该分量上的过去状态在时间 时会被很大程度上遗忘；如果 ，过去的信息则被保留。与RetNet中的常数 不同，这里的 通过 随时间 变化。在大语言模型设计中，采用这种策略的两个显著例子是Mamba \[11, 17\] 和门控线性注意力（GLA）\[68\]。

Mamba是一种循环状态空间模型，其状态衰减率具有输入依赖性。在每个Mamba层中，基础状态演化类似于S4\[18\]，但状态矩阵被有效动态化。 是一个介于0到1之间的组向量，作为动态遗忘门，它弥合了注意力机制与纯状态空间模型之间的差距。实验结果表明，Mamba2在语言建模任务上可以超越同等规模甚至更大规模的Transformer模型，突显了数据依赖衰减在长序列建模中的强大能力。

GLA直接在Linear Attention中引入门控机制，其中门控函数嵌入到线性化注意力层中以提高其表达能力。GLA通过可学习的逐元素遗忘门控 修改保留递归。除此之外，还有其他一些模型同样为其递归赋予了内容相关的门控。

xLSTM \[1\] 将标准sigmoid遗忘门替换为线性门信号的指数变换（经归一化），从而在其单元状态上产生平滑的、输入条件化的衰减。

GateLoop \[23\] 采用基于保留的逐头门控机制，实现了简单而有效的数据相关衰减，同时保持了高效的硬件实现。

HGRN \[43\] 在线性RNN中引入了门控循环。HGRN2 \[45\] 进一步将状态扩展添加到HGRN框架中。状态扩展相当于线性注意力机制中的键值外积。

Finch \[37\] 在Eagle上采用了数据依赖门控。由于Eagle与其他正交修改相似于Retention，Finch也显示出与上述模型之间的深度联系。

综上所述，数据相关衰减模型通过内容门控机制增强了线性注意力机制或RNN式架构，从而控制信息流。论文中的结果表明，这些模型在语言任务上通常能够达到或超越Transformer的性能，并且能够扩展到非常长的输入。

### 2.3. 线性注意力作为上下文学习器

线性注意力机制带来的效率提升之外，一个显著的进步在于它们在增强情境学习中的应用。这指的是模型能够通过给定 Prompt 快速适应或学习，而无需对其预训练权重进行显式的梯度更新。

大型Transformer模型通过将 Prompt 解释为一种训练数据，本质上表现出情境学习。近期创新将快速学习规则直接集成到注意力机制中，有效将序列处理视为在线训练过程。FWP \[48\] 建立了现有线性注意力机制与快速权重编程器之间的形式等价关系。在FWP范式下，一个慢速神经网络学习编程另一个网络的"快速权重"，通常通过ego发明的键值模式的外积加和实现。本节探讨了几种模型，包括DeltaNet \[69, 70\]、Longhorn \[29\] 测试时训练（TTT）层 \[49, 50\]、Titans \[3\]，这些模型通过快速权重更新等机制，以元学习视角展现了利用线性注意力作为情境学习者的范例。

#### 2.3.1. 学习目标

从元学习的角度来看，这些模型定义了一个在推理过程中进行优化的隐式学习目标。将 表示在时间步 t 的 Query 、 Key和Value ，上下文记忆 通过以下目标进行优化：

DeltaNet集成了经典的delta规则\[48\]，其中 。其更新规则为 。该规则可通过最小化当前记忆检索 与新值 之间的误差推导得出。这标志着向在线学习键值映射迈出一步，有效基于即时上下文优化记忆。

TTT \[50\] 通过不同的建模架构推广了元学习目标：

上下文网络 增强了情境元学习的功能。然而，由于 的梯度比简单的线性投影复杂得多，因此在线更新不能以简单的规则来表示。

批量更新

批量更新旨在解决当fs作为神经网络工作时训练并行性的困难。通常，上下文记忆以批量大小为1进行元学习，这对通用TTT模型并不可行。相反，类似于块并行性，TTT将整个块视为一个批量。批量内部不会发生状态更新（即S保持不变）。处理完批量后，使用来自批量中所有样本的聚合梯度或更新信号对S进行一次更新。这种策略在保持并行效率的同时，满足了更复杂架构的训练需求。

Momentum Titans \[3\] 引入动量，该动量在优化中常用，以增强记忆更新机制的能力：

动量项允许记忆逐渐累积信息，并在状态s上应用指数移动平均。这可以被视为一种元学习形式，其中更新规则本身学会在长序列中变得更加稳定和鲁棒。

权重衰减权重衰减是训练过程中的另一种正则化技术，对应于线性注意力模型中的遗忘机制。门控DeltaNet \[69\] 和 Titans 在其记忆更新中采用权重衰减，将其作为学习到的遗忘门来限制非常旧或噪声数据的影响。这对应于RetNet \[51\] 和 Mamba \[17\] 等架构中发现的具有选择性状态保持机制，其中衰减机制被证明对语言建模性能至关重要：

综上所述，线性注意力机制的最新进展通过将其架构中明确融入元学习原理，正推动情境学习边界的拓展。借助快速权重更新、复杂的内存管理技术和在线学习规则，这些模型正迈向一个训练与推理界限日益模糊的范式，从而催生出更高效、适应性更强的巨型语言模型，使其能够直接从情境中学习和利用知识。

### 2.4. 其他设计讨论

#### 2.4.1. 元素线性注意力

注意力无Transformer \[73\] 利用简单的权重指数 而不是 ..

其中 是学习到的成对位置偏差。在AFT变体中，AFT-Simple移除了 ，实现了线性化推理模式。由于 和 的乘积是逐元素的，循环状态大小为 而不是外积状态 。RWKV \[36\] 利用AFT-Simple的衰减机制。具体而言，RWKV通过指数衰减改进AFT的位置偏差，即 。指数形式在引入位置偏差的同时保留了递归特性。

逐元素线性注意力机制带来了强大的推理优势。然而，它受到状态大小的 Bottleneck 限制，在矩阵状态大小下表现不佳。此外，尽管逐元素内存比外积内存快得多，但由于其他组件占用了超过95%的延迟\[51\]，端到端的优势仍然微乎其微。

#### 2.4.2. 多路线性注意力

具有有界内存控制的注意力考虑将线性注意力视为一个有界内存模块：

在实现中，ABC可以简化为两阶段线性注意力机制。

门控槽注意力\[76\]进一步将GLA引入ABC框架\[39\]。由于 作为隐式线性注意力工作，GSA将更新改进为门控形式：

Multi-Pass是一种有效提升线性注意力（Linear Attention）表达能力的方法。然而，它也带来了额外的计算开销，使得架构设计成为在训练效率和性能之间进行权衡的选择。

#### 2.4.3. 双向线性注意力

双向注意力在BERT \[13\]等编码器式架构中发挥着重要作用。单向注意力和双向注意力在线性公式中的关键区别在于推理 Bottleneck 和计算模式。仅编码器模型通常表现出 复杂度。此外，仅编码器模型中的每个 Token 都能访问全局信息。因此，双向线性注意力通常保持一个固定长度的全局 Token 池，以降低复杂度，同时保留softmax函数的使用。

例如，Linformer \[61\] 通过额外的矩阵投影将 Key和Value 的数量减少到常数长度。Luna \[34\] 通过在模型层中编码全局 Token 池进一步扩展了Linformer的设计。

双向线性注意力机制对于仅编码器架构是有效的，但在因果设置中应用这些设计时面临重大挑战，因为基于全局池化的方法往往计算成本高昂。因此，此类架构不适合大语言模型。

### 2.5. 硬件实现

并行表示 作者定义具有门控衰减的因果线性注意力为：

其中 控制衰减的尖锐程度。矩阵 编码了具有衰减模式的因果 Mask ，确保信息的单向 Stream 。

当衰减与数据无关时， 。请注意，GroupNorm \[62\] 在 Linear Attention 之后已经是一个强制组件 \[51\]，因此方程 2 中 Kernelized Linear Attention 的显式除数是不必要的。

并行表示简单且易于理解，但存在两个缺点。首先，并行形式仍然保留了 复杂度，与 softmax 注意力相同。其次，当在 2.3 节中表示 ICL 风格的线性注意力时，其复杂度会增加。

循环表征上述的并行公式可以等价地表示为用于逐步解码的循环形式，如图2b所示。在每个时间步 ，输出计算如下：

![Image](https://mp.weixin.qq.com/s/www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg%20stroke='none'%20stroke-width='1'%20fill='none'%20fill-rule='evenodd'%20fill-opacity='0'%3E%3Cg%20transform='translate(-249.000000,%20-126.000000)'%20fill='%23FFFFFF'%3E%3Crect%20x='249'%20y='126'%20width='1'%20height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

这种循环公式通过维持单个状态向量 实现了高效的自动回归生成，且内存消耗恒定。

循环表示将计算复杂度从 降低到 ，但在训练过程中会产生显著的内存开销。这是因为 需要存储 和 的外积，对于长序列来说这是非常昂贵的。因此，循环形式通常仅限于解码阶段。

分块递归表示

分块表示结合了线性复杂度和硬件友好的并行性优势\[20, 51\]。如图2a所示，以衰减式线性注意力为例，给定分块大小 ，令 表示第 个分块。定义分块内的累积衰减为：

块级记忆状态 的计算方式如下：

对于块 的输出由以下给出：

这种公式提供了一个关于递归和并行性的统一视角：第一项捕获了块内依赖关系，而第二项通过单个矩阵-向量乘法传播块间记忆。由于其效率和并行性，分块表示通常在训练和预填充阶段被采用。

对于ICL风格的线性注意力机制，已通过Householder变换开发出硬件友好的分块表示方法\[69, 70\]。然而，对于TTT和Titans等更复杂的变体，构建显式的分块形式仍然存在挑战。相反，这些架构通常依赖较大的批量大小进行内存更新，通过固定的超参数有效地模拟分块计算。

Kernel 级优化对于实现高性能至关重要。广泛采用的FLA\[67\]为许多常见的线性注意力模块提供了基于Triton的实现。此外，开发者还提供了CUDA或TileLang\[57\]中的自定义实现，这些实现可用于进一步加速。

## 3\. Sparse注意力

Sparse注意力方法利用了注意力计算中的固有Sparse特性，通过以下方式近似完整注意力：

其中 是 Query 向量 所关注的部分索引的子集。不同的方法针对 设计不同的选择标准，同时考虑选择准确性和硬件效率。对于预填充，将其复杂度降低至次线性或线性；对于解码，设定固定预算。

### 3.1. 固定模式Sparse注意力

一些研究利用了token Level 的Sparse结构的模式来构建用于注意力计算的Fix模式Sparse Mask 。

局部窗口注意力

局部窗口注意力将每个 Query 限制仅在固定滑动窗口 内与其相邻的token进行交互，从而降低内存和计算量，同时保留局部上下文。

SparseTransformer \[8\] 首先应用局部窗口（行）注意力机制，其中 接近 ，然后通过额外的列注意力机制总结先前位置并全局传播信息。GPT-3 \[5\] 也采用与SparseTransformer类似的Sparse注意力模式。

StreamingLLM \[63\] 发现大量注意力分数分配给了输入序列的初始 Token ，他们称之为“注意力陷阱”。他们提出了一种简单的固定模式注意力机制，仅保留陷阱 Token 和滑动窗口 Token 。例如，给定长度为 的输入序列，StreamingLLM 中 Query Token 的选定 Token 子集 表示为

其中 表示汇接 Token 大小， 表示滑动窗口大小。为了提高硬件效率，StreamingLLM 采用块粒度 \[19\] 将汇接 Token 和局部 Token 以块的形式存储，从而实现高效的内存加载和计算。

Dilated Attention LongNet \[14\] 引入扩张注意力作为固定Sparse模式用于长上下文训练和推理。扩张注意力随着距离的增长指数级扩展注意力范围，从而将注意力的复杂度从 降低到 。具体而言，将输入沿序列维度划分为长度为 的片段后，从每个片段中以间隔 选择扩张Sparse索引。片段 的选中索引为：

Sparse化的片段 并行输入注意力机制，得到注意力输出 。结合不同片段大小和扩张率 的注意力输出，最终注意力计算如下：

其中 表示 的注意力softmax分母。LogSparse \[27\] 采用指数Sparse注意力方案，每个位置仅关注 logN 个token，这可以看作是指数扩展注意力的一个实例。

### 3.2. 块Sparse注意力

给定长度为 的输入序列和块大小 ，作者可以将 每个分为 个块，每个块大小为 。目标是通过近似一个块 Level 的 Mask来选择计算中关键的块，如图3所示。

![Image](https://mp.weixin.qq.com/s/www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg%20stroke='none'%20stroke-width='1'%20fill='none'%20fill-rule='evenodd'%20fill-opacity='0'%3E%3Cg%20transform='translate(-249.000000,%20-126.000000)'%20fill='%23FFFFFF'%3E%3Crect%20x='249'%20y='126'%20width='1'%20height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

分块选择对于在现代GPU上实现高效计算至关重要。

#### 3.2.1. 块Sparse注意力机制用于预填充

使用块Sparse注意力（Block-Sparse Attention）方法预填充近似Top-K块，这些块覆盖了大部分注意力分数，并具有高召回率，从而将注意力的计算复杂度从 降低到 。

其中 是作者定义的块状Sparse Mask ，如上所述，c 是一个很大的常数，例如 1e5，确保在 softmax 计算后，不太重要的注意力权重接近于零。块状Sparse注意力的目标是实现更高的加速比，同时最小化开销，并尽可能保留注意力权重。

MInference \[21\]观察到注意力权重中存在三种模式： Stream （A形状）模式、垂直斜杠模式和块Sparse模式。它为每个注意力头离线确定最佳模式，并在推理过程中根据分配的模式动态构建Sparse索引。

FlexPrefill \[25\] 提出了一种上下文感知的Sparse注意力机制，该机制能够实时动态调整注意力模式和计算预算。

XAttention \[64\] 提出了一种块Sparse注意力框架，该框架利用反对角线评分来预测注意力块的重要性，能够高效地识别并剪除非必要的块，从而实现高Sparse性并带来显著的计算增益。

SpargeAttn \[74\] 在预填充阶段也采用了块级Sparse注意力机制，该机制通过双阶段在线过滤过程实现：第一阶段快速预测注意力图以跳过某些矩阵乘法，第二阶段应用softmax感知过滤器以进一步消除不必要的计算。

#### 3.2.2. 块Sparse注意力解码

采用块Sparse注意力（Block-Sparse Attention）进行解码的方法，在动态解码过程中会选取包含每个解码步骤中最关键token的向量子集 中的 ，从而降低内存负载并提升效率。

Quest \[54\] 通过计算注意力权重的上界来近似每个块的关键性。对于块 ，作者通过元素级的最小值和最大值 Key 和 进行维护。

在维度 上， 和 对每个元素分别应用Given Query ，块 的近似注意力分数由下式给出

然后它选择得分最高的Top-K块作为Sparse子集 进行注意力计算。

DoubleSparsity \[66\] 通过降低计算 乘积的矩阵乘法维度，高效地近似关键token。它首先离线计算 中的异常通道，记为 。然后，它选择近似注意力分数 最高的Top-K token作为Sparse子集 。

ReSA \[53\] 结合了无训练块Sparse估计和GQA共享，提升了效率。此外，ReSA提出了一种校正阶段来控制KV缓存累积误差。ReSA在长序列生成任务上表现优异。

#### 3.2.3. 基于路由的块Sparse注意力

基于路由的块Sparse注意力通过训练学习每个token块的重要性。

能够作为推理时门控网络选择关键块的MLP层

可学习Sparse性在预训练模型SeerAttention \[15, 16\] 中通过自蒸馏方式训练门控网络。为获取每个块的重要性分数，它首先沿序列维度对 和 进行池化，记为 和 。下采样的 然后通过可学习的线性层 和 。投影后的 和 的矩阵乘积结果作为门控过程通过softmax算子：

可学习的线性层通过自蒸馏的方式训练，使其与原始LLM的2D最大池化结果对齐。蒸馏损失的计算公式为：

在推理过程中，门控分数用于通过Top-K或阈值方法预测块级Sparse性，以实现Sparse计算和效率。

训练感知Sparse注意力MoBA \[33\] 将可训练的Sparse注意力整合到预训练阶段。它提出了混合块注意力机制，该机制应用MoE中的Top-K机制作为门控机制，以决定每个 Query Token 的关键块。

每个块的重要性得分是通过 Query Token 与块 沿着 Token 维度计算出的均值池化结果之间的内积来计算的：

然后，在计算注意力时，选择具有最高s分数的Top-K块用于 。

值得注意的是，MoBA使用的Top-K块选择是不可微分的。因此，在预训练阶段，Sparse模式仍然以无训练模式进行估计，从而实现高效的推理和加速训练。

NSA \[71\] 提出了一种具有训练感知的混合粒度Sparse注意力机制，该机制包含三个分支 ，分别对应压缩、选择和滑动窗口策略。NSA 利用可微分的压缩分支来学习块选择分数。

结合三个分支，NSA的注意力输出由给出

对于压缩分支 ，块 的键 通过可学习的MLP层 被压缩成一个单键 。对于选择分支 slc，基于块重要性得分 选择Top-K块，该得分可直接从压缩分支获得。

InfLLM-v2 \[56\] 采用了一种与MoBA类似的、基于训练感知的Top-K块Sparse注意力机制。为了提高Top-K块选择的准确性，它将块划分为具有重叠的小粒度 Kernel ，并在每个块内对 Kernel 重要性分数进行聚合。

#### 3.2.4. 系统级设计选择

学习感知Sparse注意力\[33, 56, 71\]开始考虑 Kernel 实现和高效执行。为了高效实现块Sparse注意力，FlashAttention\[10\]用于在高效的tiling机制中进行注意力计算，引入了更好的利用硬件资源的要求和机会，包括：

- • 为避免内存访问的不一致性，在SeerAttention \[15\]和MInference \[21\]中，块大小 通常设置为至少64的相对较大的值。
- • 为满足GPU张量核心对分组矩阵乘法指令的最小要求，在NSA \[71\]和InfLLM-v2 \[56\]中， Query 组内的K、V头的数量被设置为至少16。
- • 为减少内存访问，NSA \[71\] 和 InfLLM-v2 \[56\] 强制在 Query 组之间共享选定的块，这是通过在 Query 组内的块级重要性分数上执行池化操作来完成的。

### 3.3. 聚类注意力

与块Sparse注意力类似，聚类注意力旨在为解码选择最关键的token，但其将token组织在数据结构中以获得更好的语义属性或采样效率。

检索注意力\[30\]采用近似最近邻搜索（ANNS）来选择关键的K个簇。为了应对注意力机制中 Query 向量和键向量之间分布差异的挑战，它引入了一种注意力感知向量搜索算法，该算法能够适应 Query 向量的分布。

ClusterKV \[31\] 在语义簇粒度上选择token，克服了Quest等页面级检索方法内部碎片化的问题。在预填充阶段后，通过K-means算法对token进行聚类。token i与j之间的语义相似度通过关键向量的余弦相似度D(i, j) = 1 - <kik. <kiy?进行度量。语义簇由其质心 表示。在每次解码步骤中，根据 Query token q与质心 的注意力权重排名选择簇，即 。MagicPIG \[7\] 利用局部敏感哈希（LSH）采样高效近似注意力计算。它采用LSH将相似的 Query 和关键向量映射到相同的哈希桶，并将存储和部分计算卸载到CPU以解决KV缓存 Bottleneck 。它还引入Oracle Top-K采样作为比暴力Top-K更好的策略。

### 3.4. 双向Sparse注意力

双向Sparse注意力机制基于编码器式架构，利用静态模式或块级Sparse性来加速注意力计算。

块Sparse性在双向Sparse注意力机制中被广泛应用。BigBird \[72\] 采用块状随机注意力机制，该机制作为桥梁缩短了token之间的间接路径。Longformer \[4\] 使用静态全局-局部混合注意力机制，同时依赖块级Sparse性并附加全局和随机链接，以促进结构化计算和内存高效的并行性。

基于聚类的算法也被应用于双向Sparse注意力机制中。Reformer \[24\] 使用局部敏感哈希（LSH）将相似的token分配到同一个桶中。Routing Transformer \[47\] 每层执行在线k均值聚类。ClusterFormer \[60\] 引入了一个可微分的聚类模块，该模块与下游目标一同进行联合训练。这些方法通过将相关的token分组来减少计算量，同时通过学习到的适应性来保持性能。

## 4\. 预训练的LLM高效注意力机制

### 4.1. 统一高效注意力预训练模型

早期的线性注意力探索通常局限于小规模模型，而最近的进展已证明其成功扩展至数十亿参数范围，确立了它们作为标准Transformer的可行且高效的替代方案。这些模型完全基于线性注意力或其架构等效物，如状态空间模型（SSMs）和循环神经网络（RNNs），即使在大型模型中仍保持其标志性的推理效率。

基于RWKV的模型RWKV项目代表了一种持续且具有影响力的努力，旨在创建一种可扩展的循环神经网络（RNN）架构，该架构结合了Transformer的可并行化训练和传统RNN的高效推理\[36\]。例如，EAGLE（RWKV-5）系列引入了矩阵值状态以增加容量，而后续迭代如Finch（RWKV-6）\[37\]和Goose（RWKV-7）\[38\]则引入了动态递归和表达性状态演化机制（例如，增量规则），以实现更复杂、数据依赖的状态转换。

基于Mamba的模型Mamba架构\[17\]的成功，凭借其数据依赖选择机制（第2.2.2节），激发了各大研究实验室的广泛采用和扩展计划。Falcon Mamba\[77\]基于纯粹的Mamba架构，在多种通用语言基准测试中展现出与领先Transformer模型相当的性能，验证了该架构在处理此类任务时的可行性，同时保留了其标志性的恒定时间推理特性。Codestral Mamba\[35\]基于Mamba-2架构，进一步证明了这一范式的潜力。尽管专门用于代码生成，它在相关基准测试中取得了最先进的结果，并支持256k token的上下文长度，展示了SSM方法在复杂结构化领域中的可扩展性和有效性。

基于Sparse的模型MiniCPM-4 \[56\] 引入了一种两阶段Sparse注意力机制，该机制基于语义相似性动态地为每个 Query 词元选择相关的键值块。MiniCPM-4利用InfLLM-v2这一块状Sparse注意力变体来替代标准注意力机制。此外，一种轻量级的LogSumExp近似方法能够高效地进行topk选择，使得该方法能够扩展到极长的序列。综合这些技术，MiniCPM-4能够在细粒度上下文感知与可处理的内存和计算需求之间取得平衡，使其成为长上下文建模的强有力候选者。

### 4.2. 带有混合高效注意力的预训练模型

随着对高效长上下文建模和多样化计算范式的需求日益增长，近期研究已广泛探索混合注意力机制。此类策略结合全局和局部注意力组件，通常通过交错专用层来平衡计算成本和性能。

Sparse混合模型GPT-3\[5\]通过交替密集和局部带状Sparse注意力层，集成了混合注意力机制，灵感来源于SparseTransformer\[8\]。密集注意力提供全上下文建模，而Sparse层采用固定或步进模式来减少被注意到的token数量。这种设计使GPT-3能够使用2048个token的固定上下文窗口高效扩展到大型模型规模，在建模能力和计算效率之间取得平衡。

线性全混合模型Jamba \[28\] 和MiniMax-01 \[26\] 结合线性层和全注意力层，以在吞吐量和表达能力之间实现高效权衡。MiniMax-01在大多数层中使用Lightning Attention，每八层插入基于Softmax的全注意力层。Jamba采用类似的比例，在每八层的Mamba块中插入一个Transformer层。两者通过限制计算密集型全注意力的使用，实现了更快的解码速度和改进的长序列性能。

局部完整混合模型Gemma 3\[55\]、Command A\[65\]和LLaMA-4-Maverick\[32\]在局部和全局注意力层之间交替使用，其设计理念一致，即Sparse使用全局层，例如每4-6层使用一次，以提高效率。局部层采用滑动窗口模式，而关键差异在于位置编码策略。Gemma 3调整RoPE基础频率——为局部层分配10K，为全局层分配1M——以更好地捕捉长距离依赖关系。Command A和LLaMA-4-Maverick混合基于RoPE的局部层与完全省略位置嵌入的全局注意力层，从而实现更强的长序列性能。

High-Level混合模型Character.AI \[6\]将局部注意力与滑动窗口以及每六层应用一次的Sparse全局注意力层交织在一起。特别是，他们重用全局注意力层的键值表示在多个非相邻层之间。这种键值共享机制能够实现高效的上下文处理，同时减少内存和延迟开销。

YOCO \[52\] 和 Phi-4-mini-flash \[46\] 采用了双解码器架构，将预填充和生成阶段分离。Self-Decoder 利用 RetNet 和滑动窗口注意力等线性注意力机制进行预填充和生成，而 Cross-Decoder 仅在生成阶段被激活。整个过程中使用单层全局 KV 缓存，支持线性时间预填充，并以极低的 GPU 内存消耗实现高效解码。

综上所述，这些最近的进展突显了混合注意力机制的趋势，旨在在不同计算约束和序列长度下实现均衡性能。每种架构都独特地贡献了对有效结合局部细节管理与全局上下文整合的见解，从而为未来注意力机制的发展提供了有价值的框架。

## 5\. 结论

本综述全面概述了高效的注意力机制，重点关注其算法基础、实际实现以及在大规模预训练语言模型中的集成。通过将线性注意力机制和Sparse注意力机制归类为明确的范式，作者识别出实现可扩展性、计算效率和长上下文能力的关键设计原则。作者还分析了这些机制在当前最先进模型中的应用方式，无论是作为独立架构还是作为平衡局部和全局计算的混合设计的一部分。随着基于注意力机制的模型持续扩展和多样化，作者预期算法创新和硬件感知优化之间将出现进一步融合。作者希望本综述能为追求高效、高性能语言模型未来的研究和系统开发提供有用的基础。

## 参考

\[1\]. Efficient Attention Mechanisms for Large Language Models: A Survey

  

继续滑动看下一个

集智书童

向上滑动看下一个