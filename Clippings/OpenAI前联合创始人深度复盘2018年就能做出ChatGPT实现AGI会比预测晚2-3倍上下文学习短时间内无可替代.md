---
title: "OpenAI前联合创始人深度复盘：2018年就能做出ChatGPT，实现AGI会比预测晚2-3倍，上下文学习短时间内无可替代！"
source: "https://mp.weixin.qq.com/s/aAIKqkKeVMn9Scj7RJIFEg"
author:
  - "[[听雨]]"
published:
created: 2025-12-24
description: "把现在的完整研究方法带回2018-2019年，几个人、一年就能做出GPT-3.5级别的系统。"
tags:
  - "ChatGPT"
  - "AGI预测"
  - "强化学习"
  - "上下文学习"
  - "工程能力"
abstract: "OpenAI前联合创始人John Schulman在采访中复盘，认为凭借后见之明，ChatGPT本可在2018-2019年由少数人做出，并指出AGI的实现可能比主流预测晚2-3倍，同时强调了上下文学习的短期不可替代性和工程能力在当前AI研究中的重要性。"
---
Original 听雨 *2025年12月24日 14:03*

![Image](https://mmbiz.qpic.cn/mmbiz_gif/MOwlO0INfQoIDJ0nx1IhNibpIpYLrpUE0kIP9qbF1iaY7EoZpaic6IojvbXibd5ZGiatxmjtibQRcVbGAPM9Ijvp66yQ/640?wx_fmt=gif&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1#imgIndex=0) ![Image](https://mmbiz.qpic.cn/mmbiz_png/MOwlO0INfQpdr8iagR6G8EWksK6gmH31UbQp4ibe9L403gMfmUbkRsTRzkd5RlIVmQWkJ77INXvufykT7UmZEwnw/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1#imgIndex=1)

### 编辑｜听雨

##### “如果早知道 Scaling 的回报这么高，那ChatGPT完全可以更早做出来！”

这是OpenAI的前联合创始人、Thinking Machines首席科学家John Schulman 在最新采访中的论断。

以他的判断， 放在2018-2019年，只要几位非常优秀的人工作一年左右，就可以做出 **接近 ChatGPT-3.5 的系统。**

**![Image](https://mmbiz.qpic.cn/mmbiz_png/MOwlO0INfQpdr8iagR6G8EWksK6gmH31UCTC1KWJrUibcXsDeWs2EEiaCJKhZgQzibceqlicmH2JDMgNp6h1y9q80yw/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1#imgIndex=2)**

**这里简单介绍一下，John Schulman是强化学习领域的重量级人物。他是 OpenAI 的联合创始人， 前期在OpenAI领导强化学习团队，开发了著名的PPO算法（近端策略优化）和TRPO（信任区域策略优化 ）算法，并与Ilya Sutskever共同领导超级对齐团队，负责ChatGPT的后训练。**

Schulman 在博士期间师从 强化学习大牛、曾经吴恩达的第一批博士生 Pieter Abbeel。 他的代表作、也是最高引论文PPO发表于2017年，这是ChatGPT核心技术RLHF中选用的强化学习算法。  

![Image](https://mmbiz.qpic.cn/mmbiz_png/MOwlO0INfQpdr8iagR6G8EWksK6gmH31UAcOXoMhkequyichtMwemFdOHeaMwn0E4s0x7iaS7rKmtpkZnVZK4qHibg/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1#imgIndex=3)

**Schulman于 2024年8月起短暂加入了Anthropic，之后又加入OpenAI 前 CTO Mira Murati 创办的 Thinking Machines Lab，并担任首席科学家。**

**本期访谈的主持人是 Cursor 首席执行官 Michael Truell，他与 Schulman 一起回顾了OpenAI的发家史。**

**早期的OpenAI是一个松散、非正式、偏学术的组织，有很多时间进行探索，因此能够产生公司整体愿景的“涌现”—— **Schulman** 透露现在的Thinking Machines和当时的OpenAI很相似。**

Schulman 指出， 现在的新公司 往往必须先追赶 SOTA，才能谈探索， 这并不是一件好事 ：

> 我一直非常警惕一件事，如果一个组织长期只处在“追赶模式”，是很难在之后建立起真正的探索性研究文化和能力的。

Schulman 还谈论了强化学习的规模化。他指出在当前 RLHF 场景中，传统的价值函数并没有发挥太大作用，但预计未来仍有可能重新回到舞台中央。同时， 上下文学习在短时间尺度几乎无可替代，而长期记忆和持续学习，最终仍需要权重更新和参数微调来支撑 。

有关最近讨论得十分火热的“AGI时间表”， Schulman则泼了一盆冷水：他认为按照经验法则， AGI 的真正到来可能比大多数预测慢 2–3 倍 。

对于进入AI领域的研究人才， Schulman 强调工程能力的重要性显著上升 ，而纯探索式研究和个人品味的重要性相对下降。他指出， 在今天具有扎实软件工程背景的人往往更具优势 。

有关未来的训练范式， Schulman 认为 联合训练生成器与验证器、多智能体博弈训练 ，可能会成为未来 LLM 和 RL 系统的核心方法。此外， 离线强化学习 也是 Schulman 看来非常有潜力的方向。

最后， Schulman 也预告了一波 Thinking Machines 的研究进展： 明年将发布新模型；在 Tinker 方面将 增加更多模型功能，支持多模态训练与输入输出，并显著提升可支持的任务规模。

小编整理了整期对话实录，在不改变原意的情况下做了部分删减和润色，enjoy！

  

ChatGPT可以更早做出来

## 主持人：

如果最早创建 OpenAI 的那批人回到 2015、2016 年，目标是 **极速通关 **做出 ChatGPT，那么你觉得** 最快能有多快** ？以及， **真正限制他们更快做到这一点的瓶颈是什么** ？如果他们有“完全的后见之明”，会采取哪些 **不同于历史实际发生的关键决策** ？

## John Schulman：

我认为，如果目标是用 **更少的算力** 做出类似 ChatGPT 的东西，其实是可以做到的。我们已经看到像 **nanoGPT** 这样的例子。

有时候，用更多算力做事情更直接；但如果你愿意引入更多 **巧妙的工程和训练技巧** ，同样的效果也可以用更少算力实现。

另外， 如果我们当时 **知道 scaling 的回报会这么高** ，那其实是可以更早、更激进地扩展规模的。

如果你真的 **提前知道完整配方** ，那么几乎肯定可以更早做出来。 你可以直接搭建一个大集群，先预训练一个模型；然后结合我们现在对后训练的理解，通过更好的微调和数据构造，大幅“放大有效算力”。

即使你需要一个 GPT-3 级别的模型才能得到一个不错的 few-shot 对话模型，只要你愿意在微调和数据构造上花足够心思，其实可以让一个 **更小的模型** 表现得非常好。

（注： *nanoGPT 是 Andrej Karpathy 创建的* 一个用于对中等规模的生成式预训练Transformer进行训练和调优的框架）

主持人：

你觉得需要多少人？大概在哪一年能做到？以及需要多少 GPU？

## John Schulman：

如果我们假设 **完全的后见之明** 的话，nanoGPT 基本上是一个人写的，在一台机器上跑，大概花了半年时间。这至少给出了一个 **上限** 。

当然，这是在 H100 上跑的；如果回到更早的年代，可能只能用 V100 之类的硬件。但即便如此，我认为如果有 **几台 GPU 机器** ，还是可以做到一些东西的。

我猜， 在 **2018 或 2019 年** ，只要有 **几位非常优秀的人** ，工作一年左右，就有可能做出一个 **接近 ChatGPT-3.5 水平** 的系统。 当然，我可能低估了整个技术栈中不同部分的复杂性。

![Image](https://mp.weixin.qq.com/s/www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg%20stroke='none'%20stroke-width='1'%20fill='none'%20fill-rule='evenodd'%20fill-opacity='0'%3E%3Cg%20transform='translate(-249.000000,%20-126.000000)'%20fill='%23FFFFFF'%3E%3Crect%20x='249'%20y='126'%20width='1'%20height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

而且这也建立在一个前提之上：你是站在别人已经构建好的 **预训练数据集和网页抓取成果** 之上的。

所以我并没有完全想透这个问题，但我的直觉是： 2018–2019 年，少数几个人，是有可能做到 GPT-3.5 级别的东西的。

而且未来甚至可能更极端，也许会出现那种“demo scene”风格的 ChatGPT：一个文件，自动抓取网页、自动训练模型，一天内完成全部流程。

  

早期OpenAI像个“学术草台班子”

主持人：

现在 OpenAI 从市值和资本开支角度看，已经是世界上最大的公司之一了。但人们很容易忽略的是， **早期 OpenAI 其实是一个非常非正式、甚至有点“草台班子”的团队。你同意这种看法吗？能不能帮我们具体描绘一下** **2016–2017 年左右的 OpenAI 是什么样子** ？

另外，你能否分享一个 **完全失败的早期项目，** 一个现在在 2025 年几乎没人再提起的“死胡同”？

## John Schulman：

我同意。 早期确实更像一个 **松散的、甚至有点学术气质的团体** 。 当时有很多研究项目，往往是由个人兴趣驱动的。很多人是一两个人一组，做某个研究课题，最后可能产出一篇论文或博客。

最初几年，OpenAI 很大程度上就是这种氛围。

当然，我们从一开始也有一个想法：相比学术界，我们可以通过 **更强的工程能力** 和 **更大的团队协作** ，把项目推进得更远。这一点也受到了 DeepMind（比如 AlphaGo）的影响。

所以整体来说，OpenAI 一直是两种模式的混合：

- 小规模、探索性的研究项目
- 大规模、工程驱动的重点项目

而显然，并不是所有项目都会成功。事实上，大多数研究项目最终都 **不会成为技术主线的一部分** 。

一个比较典型的失败项目是一个叫 **Universe** 的早期项目。

它的设想是：构建一个包含大量 RL 环境的数据集——视频游戏、网页导航任务等等；如果你在所有这些环境上联合训练，就能学到一种真正 **通用的强化学习智能体** 。

现在回头看，我甚至觉得这个想法本身是 **深刻正确的** ，只是 **至少早了十年** 。 当时很多关键前置条件都不存在。

结果是系统极其笨重，不适合做 RL 实验，而且模型都是从零训练的，泛化能力也很弱。

后来我们发现，把问题 **收缩到可控范围** 更有效。比如我后来负责的 RL 团队，专注于 **模拟器中的视频游戏环境** ，而不是“电脑前能做的一切”。这就好很多。

还有一些项目，比如 **机器人方向** ，对公司主线来说算是死胡同，但在长期上对培养工程和研究能力仍然有价值。

## 主持人：

在 2020 年之前，OpenAI 最大的工程项目是什么样的？有没有哪个研究基础设施系统特别复杂、特别关键，或者经常把研究员折磨得很惨？

![Image](https://mp.weixin.qq.com/s/www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg%20stroke='none'%20stroke-width='1'%20fill='none'%20fill-rule='evenodd'%20fill-opacity='0'%3E%3Cg%20transform='translate(-249.000000,%20-126.000000)'%20fill='%23FFFFFF'%3E%3Crect%20x='249'%20y='126'%20width='1'%20height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

## John Schulman：

Dota 项目可能是最早一个真正意义上的 **大型成功项目** ，用到了大量算力。

这类项目通常是两部分结合：一部分是 **ML 系统工程** （庞大的代码库和训练系统），另一部分是 **特定范式下的研究** （比如大规模 RL）。

工程上既包括如何接管 Dota、构建训练环境，也包括训练系统本身：并行 rollout、异步 RL、大规模训练等。理论上这些应当解耦，但现实中往往很难完全分开。

## 主持人：

你如何定义一个 **理想的研究型管理者** ？在 ML 这种“新型大科学”环境里，团队越来越大、个性差异巨大，这个角色似乎非常特殊。

## John Schulman：

这是个很难的问题，因为我见过 **完全不同的管理风格都能成功** ，而且这个问题本身也是非平稳的，七八年前有效的方式，今天可能已经不合适了。

我见过两种典型模式：

1. **高度技术参与型**  
	管理者亲自写代码、读所有代码、给出非常具体的技术反馈。
2. **高度放手型**  
	管理者更像是教练或顾问，关注职业发展、动机和方向，而不是技术细节。

如果是探索性研究、成员经验丰富，放手往往更好；如果目标明确、团队经验不足，或者执行压力大，更强的技术管理可能更合适。

## 主持人：

OpenAI 的 “Member of Technical Staff” 这个说法，是否受到 Bell Labs 的影响？你们是否从 Xerox PARC、Bell Labs 这些历史研究机构中汲取灵感？

## John Schulman：

坦率说，并没有特别系统地研究这些机构。

实际上，我们更多是受到 **自己过往经历** 的影响——研究生阶段、Google Brain、DeepMind。几乎所有人都在 Google 工作过某个阶段。

确实有人讨论过曼哈顿计划之类的例子，但并不存在一个刻意复刻历史研究机构的设计。

## 主持人：

你如何比较早期 OpenAI、Anthropic、Thinking Machines、Google 这些环境？它们分别更适合解决什么样的问题？

## John Schulman：

这是一个非常宏大的问题，而且这些组织都在不断变化。

但我会说， **早期 OpenAI 和现在的 Thinking Machines 其实有相似之处** ：很多项目并行推进，公司的整体愿景是从这些项目中“涌现”出来的。

不同的是：早期 OpenAI 处在一个相对“和平时期”，没有一个所有人都在追逐的明确 scaling 轴； 而现在的新公司往往必须先 **追赶 SOTA** ，才能谈探索。

![Image](https://mp.weixin.qq.com/s/www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg%20stroke='none'%20stroke-width='1'%20fill='none'%20fill-rule='evenodd'%20fill-opacity='0'%3E%3Cg%20transform='translate(-249.000000,%20-126.000000)'%20fill='%23FFFFFF'%3E%3Crect%20x='249'%20y='126'%20width='1'%20height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

我一直非常警惕一件事，如果一个组织长期只处在“追赶模式”，是很难在之后建立起真正的 **探索性研究文化和能力** 的。 而这种文化，是需要从一开始就慢慢培养的。

  

## 如何看待强化学习和持续学习

## 主持人：为什么现在强化学习里，价值函数好像不太流行了？

## John Schulman：

我觉得一个主要原因是： **在当前人们实际在做的 RL 场景中，价值函数并没有带来太多帮助** 。

比如现在常见的设置包括：基于人类反馈的强化学习（RLHF）、在 **可验证奖励 **上的 RL，而且这些任务的** 时间跨度相对较短** 。

当然，我不想简单地说“现在我们只在做短时间跨度的任务”，因为如果你一次采样的是 **上万 token** ，那其实已经是一个相当长的时间尺度了。

但总体来看，在当前这批 RL 任务上， **价值函数似乎就是没有发挥出太大作用** 。原因并不是特别清楚。

从理论上说，价值函数的主要作用是 **降低方差** ，而在我们现在关心的这些任务里，似乎并没有获得太多方差降低的收益。  
相比之下，在一些传统的 RL 研究任务中，价值函数的方差降低效果是非常显著的。

至于为什么会出现这种差异，我也说不出一个明确的原因。但我个人 **预计价值函数未来还会重新回到舞台中央** 。

## 主持人：你觉得持续学习问题最终会怎么解决？你认为 LoRA 会在其中扮演重要角色吗？

## John Schulman：

我觉得“持续学习”这个概念本身可以指很多不同的东西。如果用心理学作类比：有 **运动学习、有** **情景记忆、有** **程序性知识，不同类型的学习，可能需要** **完全不同的机制** 。

我预计上下文内学习 ****和** 上下文管理** 会持续改进，长上下文能力仍然会非常重要。至于 LoRA，我更倾向于认为它会 **叠加在这些能力之上** 。

![Image](https://mp.weixin.qq.com/s/www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg%20stroke='none'%20stroke-width='1'%20fill='none'%20fill-rule='evenodd'%20fill-opacity='0'%3E%3Cg%20transform='translate(-249.000000,%20-126.000000)'%20fill='%23FFFFFF'%3E%3Crect%20x='249'%20y='126'%20width='1'%20height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

也就是说， **参数级微调** 会在某些记忆类型上更有效，尤其是那些需要大量容量、需要吸收大量知识的场景。但到底哪些任务最适合参数微调，其实并不完全清楚。

## 主持人：

你觉得，仅靠把合适的内容放进上下文窗口，再加一点参数微调，真的足以支撑模型部署到现实世界，并在运行中持续学新东西吗？还是说我们最终还需要一些 **完全不同的思路** ？

## John Schulman：

这其实很难判断。一方面，如果我们持续扩大模型规模、持续提升模型能力，那么几乎 **所有我们定义的指标都会自然变好** 。甚至有可能即使我们不改变方法论、不做参数微调，随着规模增长，也会“顺带”解决很多问题。

但另一方面，也很可能会出现一些 **新方法** ，它们能更快解决同样的问题，甚至带来 **不同的 scaling law** ：

- 要么是一个固定倍数的“有效算力提升”
- 要么是直接改变 scaling 曲线的斜率

所以我确实能想象，某些新方法可以带来 **更高效的持续学习能力** 。

我的直觉是， 在短时间尺度 上，上下文学习非常强，几乎无可替代；而更长时间尺度 上，权重更新最终会胜出。

![Image](https://mp.weixin.qq.com/s/www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg%20stroke='none'%20stroke-width='1'%20fill='none'%20fill-rule='evenodd'%20fill-opacity='0'%3E%3Cg%20transform='translate(-249.000000,%20-126.000000)'%20fill='%23FFFFFF'%3E%3Crect%20x='249'%20y='126'%20width='1'%20height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

## 主持人：你是否担心“泛化能力不足”会成为实现通用 AI 的真正障碍？会不会出现这样的情况：预训练把模型推到一个高度，然后 RL 只在它训练过的分布里有效，跨领域迁移能力很弱？

## John Schulman：

这是一个很难清晰回答的问题。一方面，在 **上下文学习** 场景中，模型的样本效率非常高，在某些方面甚至可以达到或超过人类。但另一方面，确实存在一些学习类型，模型所需的数据量 **远远超过人类** 才能学会同样的东西。

所以可以说，在某些方面， 模型比人类 **脆弱得多 ，但要精确描述“脆弱性究竟体现在哪里”，其实并不容易。**

人类在 **长时间尺度** 上的表现明显更强，这很大程度上是因为我们经过进化，被优化用于一个 **80 年左右的时间跨度** 。我们拥有大量自我纠错机制。人类当然不完美，但在纠错方面确实做得相当好。而且只要你给人一个目标和动机，他们会非常有资源性，尝试各种不同路径。

模型也可以非常“坚持”，有时甚至比人类更坚持；但它们在执行 **大块任务** 时，更容易陷入某种局部状态，难以跳出来。

所以问题在于：这只是一个暂时现象？模型的时间跨度是否即将大幅提升？还是说这是一个需要非常长时间才能追平人类的根本性弱点？

而这个问题本身几乎无法快速验证，如果讨论的是 **几十年的时间跨度** ，那就真的需要几十年才能观察。

## 主持人：

## 如果未来越来越多地采用“生成器 + 判别器 / 验证器”共同训练的方式，比如用模型来给模型打分、判断奖励，你觉得 GAN 时代的哪些想法会重新变得重要吗？

## John Schulman：

我非常认同 **联合训练生成器和验证器** 这个方向。 理论上，你可以形成一种 **自我强化循环** ：如果验证器本身具备推理能力、指令理解能力，并用它来给生成模型提供学习信号，那么生成模型越强，验证器也越强，形成一个良性循环。

我也非常喜欢 **多智能体训练** 、博弈 这类思路。博弈有很多非常好的性质：自动课程学习，如果你和“自己”的拷贝对战，对手会随着你一起变强。

从理论计算机科学的角度，也有很多理由支持这种方式：有一些复杂度类别，本身就是通过 **双人零和博弈** 来定义的。你可以用一个计算上很便宜的机制，设计一个博弈，使得其均衡解对应于 **解决一个非常困难的问题** 。

这在对齐文献中也出现过，比如 **辩论模型** 。我一直觉得这是个非常有说服力的想法，而且我预计这类思路未来会越来越重要。

## 今天如何做研究：把 LLM 融入到研究流程中

## 主持人：

**你认为在 2019 或 2020 年做有效研究所需要的技能，和现在相比有什么变化吗？**  
尤其是你在 2020 年写过一篇关于“如何做有效研究”的博客。你现在是否有新的建议？还是说你认为那篇文章基本经得起时间考验？

## John Schulman：

回顾那篇博客，我当时谈到了几类研究方式，比如目标导向型研究，以及更偏理想化的研究；还提到要坚持记录研究笔记，以及通过大量阅读论文来培养研究品味。我认为这些建议 **总体上依然成立** ，到今天我仍然认可它们。

如果说有什么变化，我觉得 **实验室笔记现在反而比以前更重要了** 。因为在有了大语言模型之后，“上下文”变得极其关键。如果你希望得到高质量的反馈，其实可以直接把你的研究笔记贴给模型，让它基于完整背景给你建议。

如果一定要说最大的变化，那就是： **现在你需要认真思考如何把 LLM 融入到自己的研究流程中** 。 但说实话，我自己也没有完全想清楚，除了那些能普遍提升工程效率的方法之外，究竟什么才是“专门用于加速研究”的最佳方式。这一点并不显然。

![Image](https://mp.weixin.qq.com/s/www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg%20stroke='none'%20stroke-width='1'%20fill='none'%20fill-rule='evenodd'%20fill-opacity='0'%3E%3Cg%20transform='translate(-249.000000,%20-126.000000)'%20fill='%23FFFFFF'%3E%3Crect%20x='249'%20y='126'%20width='1'%20height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

我觉得研究和其他软件工程工作之间，建议可能是不同的。因为在研究中， **理解代码的每一行究竟在做什么非常重要** 。相比让模型一次性写出大量你从未仔细阅读过的代码，一个你完全理解、结构非常简单的实现，往往更有价值。

那种“AI 辅助编程”的方式，你只给一个规格说明，让模型把整个实现都写出来，在某些工程领域可能非常有效。 但在研究中，我认为 **真正做出最好成果的人，往往对系统的每一个细节、每一行代码都了如指掌** ，这种对“底层机制”的理解，自 2012 年以来一直都是高质量研究的共同特征。

  

工程能力的重要性明显上升

## 主持人：自 2020 年规模定律兴起以来，无论学界还是工业界，进入 ML 领域的研究者数量都大幅增加。但看起来，真正“改变格局”的核心想法出现的频率似乎并没有明显加快。你如何看待这种现象？

## John Schulman：

我对“量化科学进展速度”这类问题一向比较谨慎。首先，早期的低垂果实会被逐步摘完；其次， **对于最近几年发生的事情，我们其实还不知道哪些想法最终会被证明是重要的** ，因此很难准确评估。

所以我并不太愿意直接下结论说：即便研究人员数量大幅增加，进展速度仍然是恒定的、没有加速的。如果你回顾 70、80、90 年代的论文，会发现当时的 **实验严谨性明显更低** 。如今，在实验设计、基线对比、跨任务验证等方面，标准已经高了很多。

过去你可能会看到一篇强化学习论文，提出了一套非常复杂的想法，但只在一个非常简单、甚至有些可疑的玩具任务上做了一个实验，而那样的论文仍然可能成为经典。那时很多数学思想本身也并不算特别成熟。

因此，我一点也不惊讶： **随着研究者数量的增加，想法产生的速度实际上是提升了的，同时研究质量和标准在某些方面也显著提高了** 。这基本符合我的直觉。

## 主持人：

**那你如何看待学术出版体系？相比大型 AI 公司内部通过 Slack、内部报告等方式进行的“同行评审”，你觉得二者各自有什么优劣？有没有什么经验可以从工业界迁移到开放学术界？**

## John Schulman：

这是一个很有意思的问题。我会说，大型研究实验室内部对结果的评估，在某些方面 **比学术出版体系更好** ，在另一些方面则更差。

好的地方在于： **内部研究往往更擅长得出真实、可靠的结论** ，例如什么真的能改进预训练效果。这些实验通常更贴近真实目标，而不是为了“发论文”。成功的公司在方法论上确实更成熟。

![Image](https://mp.weixin.qq.com/s/www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg%20stroke='none'%20stroke-width='1'%20fill='none'%20fill-rule='evenodd'%20fill-opacity='0'%3E%3Cg%20transform='translate(-249.000000,%20-126.000000)'%20fill='%23FFFFFF'%3E%3Crect%20x='249'%20y='126'%20width='1'%20height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

但问题在于： **几乎没有人会在公司内部写出和学术论文同样详细的技术报告** 。内部文档通常不会那么完整、系统。虽然结论在“准确性”上可能更高，但实验的全面性往往不如最好的学术论文，比如不会系统性地尝试大量基线。

当然，学术论文里也经常存在“被削弱的基线”，结果并不完全可信。但至少在最优秀的工作中，确实能看到非常扎实、全面的比较。

总体来看， 学术界的写作更详尽、某些方面更全面，但准确性可能较低；工业界则相反。 我个人一直很希望能在这些机构中推动 **更好的研究写作文化** ，鼓励大家写真正深入科学本身的技术报告，而不是只记录“可交付的最小改进”。但这和公司的激励机制之间确实存在张力。

## 主持人：

**进入这个领域的研究者本身，有没有发生变化？相比 2015～2017 年，现在的人在能力结构、工程水平、创造性等方面是否不同？**

## John Schulman：

我会说， **早期进入这个领域的人整体上更“怪”一点** 。现在大家都很清楚：AI 是一件极其重要的事情，因此会吸引更多走传统职业路径、风险偏好更低的人。

很难直接比较两代人的“人才分布”，但仅从数量上看， **进入门槛确实变高了** ，因为竞争者实在太多了。

我还认为， 如今 **工程能力的重要性明显上升** ，相对而言，研究品味和纯探索式研究能力的重要性下降了一些。这是因为一方面，规模化本身就能带来大量改进；另一方面，领域已经成熟，你通常是在大型现有代码库和基础设施上工作，而不是在 Jupyter Notebook 里从零写代码。

![Image](https://mp.weixin.qq.com/s/www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg%20stroke='none'%20stroke-width='1'%20fill='none'%20fill-rule='evenodd'%20fill-opacity='0'%3E%3Cg%20transform='translate(-249.000000,%20-126.000000)'%20fill='%23FFFFFF'%3E%3Crect%20x='249'%20y='126'%20width='1'%20height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

因此， 在今天， **具有扎实软件工程背景的人往往更具优势** 。

## 如何看待强化学习的未来

## 主持人：

**你如何看待强化学习研究的未来？在语言模型中，真正奏效的方法看起来反而相当简单，而且和其他领域成功的方法非常相似。你觉得 RL 研究还有多少空间？未来最强的 RL+LM 系统会和过去的思路有本质不同吗？**

## John Schulman

正如你说的，很多想法会反复流行。有些概念出现得太早，没能兑现承诺，但后来又在新的背景下重新变得重要。我预计这种情况还会继续发生。

很难预测哪些想法最终会最重要，但我认为 **离线强化学习是一组非常有潜力的方向** 。在某种程度上 ，我们现在在 LLM 领域做的事情，很像机器人领域中的“仿真到现实”：在大量模拟环境中进行大规模训练，通过足够的多样性实现泛化 。

![Image](https://mp.weixin.qq.com/s/www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg%20stroke='none'%20stroke-width='1'%20fill='none'%20fill-rule='evenodd'%20fill-opacity='0'%3E%3Cg%20transform='translate(-249.000000,%20-126.000000)'%20fill='%23FFFFFF'%3E%3Crect%20x='249'%20y='126'%20width='1'%20height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

事实上，sim-to-real 在机器人领域仍然非常有效，并没有被否定。同时，我也认为 **从真实世界中学习依然极其重要** 。我预计，未来在 LLM 的部署阶段，我们还会重新探索如何更好地从真实交互中进行学习。

## 主持人：

**如果一些最大的 AI 实验室真的开发出了非常、非常强大的 AI 系统，强大到必须彼此协调，并且还需要与政府等社会中其他关键机构进行协调，你对它们是否能够顺利合作有多大信心？你又有多担心它们最终无法协调、无法达成一致？**

## John Schulman：

我的感觉是介于“担心”和“有信心”之间，大概算是 **中等程度** 吧。

我会说，在领先的 AI 实验室之间， **在总体世界观和愿景层面，还是存在相当程度的共识的** 。此外，最近一段时间，这些实验室之间在安全相关的问题上，也确实已经有了一些合作。

不过，我也必须承认， **实验室之间确实存在一些“历史恩怨”或者说人际层面的紧张关系** 。这些涉及到具体个人和性格的问题，可能会让协调变得更困难一些。

但总体而言，如果未来有一天，这种跨实验室、跨机构协调被明确地视为“这是正确的、必须要做的事情”，那我认为 **它是有可能运作起来的** 。

## AGI会比预测来得慢2-3倍

## 主持人：

**在这样一个技术进步速度极快的时期，关于 AI 未来发展的讨论非常多，尤其是关于“AI 会多快变得更强”以及“AGI 什么时候到来”的预测。很多人谈论 AGI 时，指的是这样一种状态：几乎所有基于计算机的知识工作，都可以由 AI 而不是人类来完成。**

**你如何看待这些时间表预测？你是否认为它们系统性地低估了实现 AGI 所需的时间？**

## John Schulman：

我过去思考这个问题的一种方式是： **AGI 看起来像是一个极其庞大、复杂的工程与研究项目** 。而根据我个人的经验，工程师和研究人员在预测项目完成时间方面，往往表现得非常糟糕，即便是在远比 AGI 小得多的项目上也是如此。

我观察到的一种非常一致的系统性偏差是： **工程师几乎总是认为事情会比实际情况更早完成** 。如果让我给一个经验法则，我可能会说： 你需要在他们给出的时间预测基础上，再乘以一个 2 到 3 倍的系数，才能更接近真实的完成时间。

![Image](https://mp.weixin.qq.com/s/www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg%20stroke='none'%20stroke-width='1'%20fill='none'%20fill-rule='evenodd'%20fill-opacity='0'%3E%3Cg%20transform='translate(-249.000000,%20-126.000000)'%20fill='%23FFFFFF'%3E%3Crect%20x='249'%20y='126'%20width='1'%20height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

从这个角度看，我认为这确实是对很多 AGI 时间线预测的一个 **合理批评** 。基于这种启发式规则，推断 AGI 比很多人预测的时间要更晚一些，我觉得是有道理的。

一个最接近的类比案例，可能是 **自动驾驶** 。我们已经看到，全自动驾驶、Robotaxi 等目标，花费的时间明显比早期人们预期的要长得多。所以从这个角度来说，我认为“AGI 会比预测更慢”是一个合理的假设。

但另一方面，也确实存在一个 **可能打破直觉的因素** ： AI 会反过来加速自身的研发过程，形成正反馈循环。 那些把这种“自我加速效应”纳入考虑的人，往往会得出 **相当短的时间线预测** ，而我也认为这是一条 **有一定说服力的推理路径** 。

最终的问题在于：AI 究竟能在多大程度上提升研发效率？是否会出现瓶颈，比如人类是否还能理解系统内部发生了什么？这些因素的不确定性都非常大。

所以老实说， **我不会对 AGI 的时间点做出非常自信的预测** 。

## 关于 Thinking Machine和Tinker

## 主持人：

**你和 Thinking Machines 最近发布了 Tinker。它是什么？主要是为谁设计的？**

## John Schulman

Tinker 是一个 **偏底层的微调API** 。 它提供了一小组低层次的原语，用于训练和采样，但足以表达你可能想要实现的几乎所有后训练算法。

与此同时，它帮你 **屏蔽掉了很多底层复杂性** ：你不需要关心 GPU 或其他加速器，也不需要操心分布式系统相关的大量工程问题。

我们认为这是一个非常合适的抽象层级。一般来说，人们并不太会把“模型训练”当成一种服务来使用；而现有的训练服务通常又是非常高层的。所以在这个层级上做成服务，本身是比较新颖的。

一个最接近的类比，其实是你今天使用 OpenAI、Anthropic 等提供的 **模型采样 API** ：你不需要自己搭建 GPU 服务器，只需要在 Python 或 JavaScript 里发起一次 API 调用即可。Tinker 希望在“训练”这件事上，提供类似的体验。

## 主持人：

**你的目标是否是：未来一群研究者成立新的 AI 公司时，可以直接基于 Tinker 来构建？**

## John Schulman：

是的，我确实希望如此。 我希望很多公司都可以 **直接构建在 Tinker 之上** ，而不必再从零开始搭建自己的基础设施。 你可以在 Tinker 的基础上构建非常复杂、定制化的模型。

![Image](https://mp.weixin.qq.com/s/www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg%20stroke='none'%20stroke-width='1'%20fill='none'%20fill-rule='evenodd'%20fill-opacity='0'%3E%3Cg%20transform='translate(-249.000000,%20-126.000000)'%20fill='%23FFFFFF'%3E%3Crect%20x='249'%20y='126'%20width='1'%20height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

至于“它适合谁”，我会说： **在当前阶段，Tinker 更适合那些在机器学习方面已经相当成熟的用户** ，也就是那些愿意直接接触和使用底层原语的人。

当然，我们也提供了大量与 Tinker 配套的开源代码，所以你并不一定要从头实现所有训练算法。但总体来说，Tinker 最适合那些愿意深入理解细节、愿意“下探到底层”的人。

随着时间推移，我们会让 Tinker 变得越来越易用，在其之上构建更多工具和更高层的组件。最终的目标是： **即使你不是 ML 专家，也可以使用它** 。你只需要清楚自己要解决什么业务问题，或者你希望构建什么样的模型，其余的事情可以交给我们提供的软件来完成。

## 主持人：

**未来一年左右，我们可以期待 Thinking Machines 带来什么？有什么可以公开分享的吗？**

## John Schulman：

你们会在明年看到一些 **我们自己训练和发布的模型** 。 同时，在 Tinker 方面，我们也会持续改进：增加更多模型功能，支持多模态训练与输入输出，并显著提升可支持的任务规模。

参考链接：

https://www.youtube.com/watch?v=29BYxvvF1iM

——好文推荐——

[智谱GLM-4.7深夜炸场！登顶开源编程模型王座，代码能力超越GPT-5，仅需 Claude 1/7的价格！](https://mp.weixin.qq.com/s?__biz=MjM5ODI5Njc2MA==&mid=2655933760&idx=1&sn=2307267886be4060ba52274967303dcf&scene=21#wechat_redirect)

[辛顿亲口承认：当年低估了Transformer！知识蒸馏曾被拒稿；Jeff Dean：谷歌早有8万员工用聊天机器人，但没敢发布](https://mp.weixin.qq.com/s?__biz=MjM5ODI5Njc2MA==&mid=2655933459&idx=1&sn=2d6b9c35d2172d879672541fdd18e33b&scene=21#wechat_redirect)

[Gemini3预训练负责人：训练数据一开始就用了大量合成数据，范式已经变化：研究越来越像工程！不太担心业界刷榜会造成过拟合](https://mp.weixin.qq.com/s?__biz=MjM5ODI5Njc2MA==&mid=2655933632&idx=1&sn=b15dee9e126c3abd3a4040dd48dd79e9&scene=21#wechat_redirect)

![Image](https://mp.weixin.qq.com/s/www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg%20stroke='none'%20stroke-width='1'%20fill='none'%20fill-rule='evenodd'%20fill-opacity='0'%3E%3Cg%20transform='translate(-249.000000,%20-126.000000)'%20fill='%23FFFFFF'%3E%3Crect%20x='249'%20y='126'%20width='1'%20height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

继续滑动看下一个

51CTO技术栈

向上滑动看下一个