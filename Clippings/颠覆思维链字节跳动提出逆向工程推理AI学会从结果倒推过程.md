---
title: "颠覆思维链！字节跳动提出逆向工程推理！AI学会从结果倒推过程"
source: "https://mp.weixin.qq.com/s/TtybQNtk4AbT6IIyjnqGQA"
author:
  - "[[Tensorlong 看天下]]"
published:
created: 2025-09-15
description: "❝本文颠覆了传统的AI推理训练范式，不再正向“构建”思维链，而是从结果出发“逆向工程”出思维链，证明了即使在没"
tags:
  - "逆向推理"
  - "困惑度优化"
  - "思维链生成"
abstract: "字节跳动提出逆向工程推理方法，通过从结果倒推过程来训练AI模型在创造性任务中的深度思考能力。"
---
Original Tensorlong 看天下 *2025年09月15日 14:20*

![Image](https://mmbiz.qpic.cn/sz_mmbiz_png/Z24DyenWDNgvl37siaPpLS4ZSLaOQOcjPZfzSZPTiciciaYiaiaHruAvHM38GDvia1NWNy68hqqR7nWoficCB173XUP9og/640?wx_fmt=png&from=appmsg&watermark=1&tp=webp&wxfrom=5&wx_lazy=1#imgIndex=0)

> ❝
> 
> 本文颠覆了传统的AI推理训练范式，不再正向“构建”思维链，而是从结果出发“逆向工程”出思维链，证明了即使在没有奖励信号的创意写作中，也能通过这种方式教会模型复杂的规划能力。（原论文题目见文末，点击阅读原文可直接跳转至原文链接， Published on arxiv on 07 Sep 2025, by ByteDance）

### 第一阶段：识别核心概念

#### 论文的motivation分析

这篇论文的出发点非常明确，旨在解决一个当前大语言模型（LLM）领域的核心难题：如何让模型在 **开放式、创造性任务** （如写故事、写文章）中也具备“深度思考”的能力。

我们知道，像“思维链”（Chain-of-Thought）这样的技术已经让模型在有明确对错答案的领域（如数学计算、代码编写）取得了巨大成功。这些模型能够通过一步步的推理过程，最终找到正确答案。这主要得益于两种主流训练方法：

- **强化学习（RL）** ：模型不断尝试，如果最终答案对了，就给予奖励。这就像一个学生做数学题，做对了老师就给糖吃。
- **指令蒸馏（Distillation）** ：让一个强大的“教师模型”（如GPT-4）先写出解题过程，然后让“学生模型”去学习模仿。

然而，当任务变成写一篇小说时，这两个方法就失灵了。

- **强化学习的困境** ：一篇小说没有“标准答案”，也就没有明确的“奖励信号”。我们很难用一个简单的分数来评判一篇文章是“好”还是“坏”，因为这涉及到主观的艺术审美。
- **指令蒸馏的困境** ：让教师模型为海量主题都生成一遍高质量的思考过程，成本极其高昂，而且学生模型的能力上限永远无法超越教师模型。

因此，当前的“深度思考”能力被困在了有标准答案的“可验证领域”。这篇论文的动机，就是要 **打破这个瓶颈，为开放式、创造性任务找到一条全新的、可行的、能教会模型深度思考的道路** 。

#### 论文主要贡献点分析

- **列出论文声称的主要创新点**
1. **提出一个全新的推理范式** ：作者开创了一种名为“逆向工程推理”（REverse-Engineered Reasoning, REER）的新方法。这是一种全新的思路，它不向前“构建”推理，而是向后“恢复”推理。
	2. **发布一个大规模高质量数据集** ：基于REER方法，作者构建并开源了 `DeepWriting-20K` 数据集，包含2万条针对开放式任务的深度思考轨迹。这极大地缓解了该领域高质量训练数据的稀缺问题。
	3. **训练出一个性能强大的开源模型** ：作者利用该数据集从头训练了一个80亿参数的模型 `DeepWriter-8B` ，证明了该方法的有效性。
- **找出支撑这些创新的关键技术或方法**
- **REER的核心机制** ：整个范式的基石是将“寻找最佳思考过程”这个问题， **重新定义为一个基于困惑度（Perplexity）的梯度无关搜索问题** 。简单来说，它认为一个好的思考过程（或计划），应该能让一个已知的、高质量的最终答案看起来“理所当然、毫不意外”。
	- **迭代式局部搜索算法** ：为了实现REER，作者设计了一个具体的算法。该算法从一个粗糙的初始思考计划开始，然后像打磨一件艺术品一样， **一次只专注于一小部分，不断地进行局部优化和迭代** ，直到整个思考过程变得非常完善。这个过程的“导航信号”就是上面提到的“困惑度”。
- **论文有哪些显著性的结果**
- **性能上的重大突破** ：最显著的结果是，一个仅有80亿参数的开源模型 `DeepWriter-8B` ，在多个复杂的写作和生成任务上，不仅远超同等规模的开源模型， **甚至在某些方面能够媲美甚至超越像GPT-4o和Claude 3.5这样的顶级专有模型** 。
	- **范式上的重大意义** ：这篇论文展示了 **第三条路** 。除了昂贵的“模仿老师”（蒸馏）和在开放领域难以实施的“试错学习”（RL），现在有了一种更具扩展性、成本更低、且自动化程度高的方法来培养模型的深度思考能力。这为中小型模型拥有强大推理能力提供了可能，具有“民主化”先进AI能力的潜力。

#### 理解难点识别

- **分析哪些概念/方法是理解论文的关键**
- **核心概念** ： **逆向工程推理 (REER) 的哲学思想** 。读者必须首先理解“从结果倒推过程”这一核心转变。
	- **核心机制** ： **如何用“困惑度（Perplexity）”来指导一个搜索过程** 。这是将REER哲学思想转化为可执行算法的关键。为什么低困惑度就意味着好的思考过程？这是理解技术细节的核心。
	- **核心算法** ： **迭代式局部搜索（Iterative Refinement via Local Search）** 。这是实现上述机制的具体步骤，理解它是如何一步步优化思考轨迹的。
- **找出这些概念中最具挑战性的部分**
- 最具挑战性的部分在于理解 **“困惑度（Perplexity）”** 这个信息论概念在这里的巧妙应用。通常，困惑度用来衡量一个语言模型对一段文本的预测能力好坏。而在这里，它被“反过来”用作一个 **评价标准** ，去评判一个“思考过程”的质量。这种角色的转换是理解本文技术核心的难点。
- **确定需要重点解释的核心概念**
- 我们将重点解释： **REER如何被形式化为一个由困惑度（Perplexity）指导的搜索问题** 。这是连接论文思想与实现之间的桥梁，也是最具创新性的部分。

#### 概念依赖关系

- **起点** ：首先要理解 **REER的顶层思想** ——我们不是在“写”思考过程，而是在“找”一个最能解释已知优秀答案的思考过程。
- **核心** ：然后深入到 **搜索问题的定义** 。要“找”东西，就需要一个目标。这个目标就是 **最小化困惑度** 。这解释了“为什么”要这么做。
- **实现** ：最后介绍 **迭代式局部搜索算法** 。有了目标，我们还需要一个方法去实现它。这个算法就是“如何”一步步找到那个困惑度最低的思考过程。

这个逻辑链条清晰地从“思想”到“目标”再到“方法”，构成了我们下一阶段深入解释的最佳路径。

### 第二阶段：深入解释核心概念

#### 设计生活化比喻：侦探破案

想象一下，一个非常复杂的密室谋杀案已经发生。作为一名天才侦探，你来到了犯罪现场。

- **最终成品** ：你眼前的是 **犯罪现场的最终状态** ——受害人倒在地上，门窗紧锁，房间里的一切都摆放有序（或者混乱无序）。这对应论文中的 **一篇已知的、高质量的范文（ `y` ）** 。
- **初始任务** ：你接到的任务是“侦破此案”。这对应论文中的 **用户的写作请求（ `x` ）** ，比如“写一个关于...的故事”。
- **你要找的东西** ：你并不知道罪犯是怎么作案的。你需要做的，是 **逆向推理** 出整个犯罪过程的 **最合乎逻辑的作案计划** 。这个“作案计划”就对应论文中的 **深度思考轨迹（ `z` ）** 。

那么，一个好的“作案计划”应该具备什么特点？它必须能够完美地解释现场的所有证据，让整个案件的发生看起来 **顺理成章、毫不离奇** 。如果你的推理需要假设罪犯会飞、能穿墙，那这个“作案计划”就太“离奇”了，可信度很低。

这里的“离奇程度”或“惊奇程度”，就恰好对应了我们技术中的核心—— **困惑度（Perplexity）** 。

#### 建立比喻与实际技术的对应关系

| **比喻中的元素 (侦探破案)** | **实际技术概念** | **合理解释** |
| --- | --- | --- |
| 犯罪现场的最终状态 | 一篇高质量的范文 () | 这是我们已知的、需要被“解释”的最终结果。 |
| 侦破案件的任务 | 用户的写作请求 () | 这是整个任务的起点和约束。 |
| **罪犯的作案计划**  (未知) | **深度思考轨迹 ()** | 这是我们想要通过逆向工程找到的潜在过程。 |
| **侦探的调查过程** | **寻找最优思考轨迹 () 的搜索过程** | 侦探通过不断假设和验证，寻找最合理的作案计划。 |
| **作案计划的“惊奇程度”** | \*\*困惑度 $PPL(y | x, z)$\*\* |
| 侦探的助手/分析工具 | 生成思考轨迹的LLM | 侦探利用这个工具来提出各种可能的作案细节。 |
| **侦探逐步完善推理链条** | **迭代式局部搜索算法** | 侦探不会一次想出所有细节，而是先有一个大概框架，然后逐个环节推敲、优化。 |

#### 深入技术细节

论文将寻找最佳思考轨迹 的过程，形式化为以下优化问题：

- **原始数学形式**
- **符号替换版本（自然语言解释）**
	> ❝
	> 
	> **最佳的思考轨迹 ()** \= 是从所有可能的思考轨迹集合 () 中，找到\*\*那个能让下面这个值最小的轨迹 ()\*\*：
	> 
	> \*\*某个答案的“惊奇程度”()\*\*，这个答案是 \*\*已知的范文 ()\*\*，产生的条件是 **基于用户的原始请求 ()** 和 \*\*我们假设的这个思考轨迹 ()\*\*。

简单来说，这条公式的含义是： **一个最好的思考计划，就是那个能让我们的高质量范文看起来最顺理成章、最不令人意外的计划。**

#### 将技术细节与比喻相互映射

这个数学公式在我们的“侦探破案”比喻中是如何体现的呢？

1. **公式的含义映射** ：
- 在比喻中就是： **侦探的目标，是找到一个作案计划 ()，这个计划能让最终的犯罪现场 () 在给定的案件背景 () 下，看起来“惊奇程度”()最低** 。
	- 一个需要假设“罪犯会瞬移”的计划，会导致 值极高，因为它太“惊奇”了，所以会被侦探排除。
	- 一个能完美解释所有指纹、脚印和作案时间的计划，则会让犯罪现场的出现显得非常自然， 值就会很低，这正是侦探要找的。
3. **算法步骤映射（迭代式局部搜索）** ：
- **评估A** ：现场窗户完好无损，所以“爬窗”这个计划让现场看起来 **非常惊奇（PPL高）** 。排除！
	- **评估B** ：门锁上有轻微划痕，所以“万能钥匙”这个计划让现场证据 **显得比较合理（PPL较低）** 。保留！
	- **评估C** ：没有搏斗痕迹，所以“受害者开门”也 **比较合理（PPL较低）** 。保留！
- **步骤1：初始化 (Initialization)**
	**技术上** ：模型针对任务()和范文()，先生成一个非常粗糙、不完美的初始思考轨迹 。
	**比喻中** ：侦探到达现场后，提出一个初步的、简单的假设：“这可能是一起入室抢劫杀人案”。这个假设就是 。
	- **步骤2：节点扩展 (Node Expansion)**
	**技术上** ：算法选择思考轨迹中的一小段（比如第 `i` 个步骤 ），然后让LLM针对这一小段，生成多个更详细、更完善的候选版本。
	**比喻中** ：侦探开始聚焦于某个具体环节，比如“罪犯是如何进入房间的？”（这是 ）。他让助手提出几种可能性：“A. 从窗户爬进来的 B. 用万能钥匙开的门 C. 受害者自己开的门”。
	- **步骤3：节点评估与选择 (Node Evaluation and Selection)**
	**技术上** ：对于每个候选版本 ，算法都用它替换掉原来的 ，形成一个临时的完整轨迹 ，然后计算 。算法会选择那个让 值最低的候选版本。
	**比喻中** ：侦探逐一评估这些可能性：
	侦探会选择那个让所有证据链条最通顺、最不“惊奇”的那个版本作为当前环节的结论。
	- **步骤4：终止 (Termination)**
	**技术上** ：重复步骤2和3，不断打磨思考轨迹的每一部分，直到整体的 值低于一个预设的阈值，或者达到了最大迭代次数。
	**比喻中** ：侦探对作案计划的每一个环节（如何进入、如何行凶、如何离开等）都进行了上述推敲，直到整个故事天衣无缝，能够完美解释所有现场证据，不再有任何“惊奇”之处。此时，调查结束，找到了最可能的作案计划 。
- **比喻的局限性**
- 这个比喻非常贴切，但有一个微小的区别：在现实中，侦探的目标是找到“真相”，而REER的目标是找到一个“逻辑上最 plausible（最貌似可信）”的思考过程。这个过程不一定是作者当初的真实想法，但它必须是能够逻辑自洽地导出最终作品的有效路径。

#### 总结

通过“侦探破案”的比喻，我们可以清晰地理解REER的核心：

- **核心联系** ： **寻找最佳思考过程，就像侦探还原最合理的作案计划。**
- **关键原理** ：评价一个“计划”好坏的标准，不是它是否真实，而是它能否让“最终结果”看起来 **最不令人意外** 。这个“不意外”的程度，就是通过 **最小化困惑度（Perplexity）** 来数学化地实现的。

这个巧妙的设计，让模型得以在没有明确“对错”标签的创意领域，也能通过一个可计算、可优化的目标，学会如何进行深度、结构化的思考。

### 第三阶段：详细说明流程步骤

整个流程可以分为三个主要阶段： **1\. 源数据采购** \-> **2\. 思考轨迹合成** \-> **3\. 模型训练** 。

#### 阶段一：源数据采购 (Sourcing of (Query, Solution) Pairs)

这一阶段的目标是收集大量高质量的“任务-答案”对 ()，作为后续逆向工程的原材料。

- **输入** ：无特定输入，目标是广泛搜集。
- **处理过程** ：作者从三个渠道搜集数据，以确保多样性：
1. **公共写作平台** ：从像 `r/WritingPrompts` 这样的网络社区抓取“写作提示（Prompt）”和用户创作的“故事”。在这里，“写作提示”就是任务 ，“高赞故事”就是高质量答案 。社区的点赞数可以作为初始的质量筛选标准。
	2. **公共领域文学作品** ：使用古登堡计划（Project Gutenberg）中的经典文学作品作为高质量答案 。然后，让一个强大的LLM（如GPT-4o）阅读这些作品的开头段落，反向推断出一个可能引出这段文字的“写作任务”或“查询” 。
	3. **现有公共数据集** ：从一些指令微调数据集中（如WildChat, LongWriter6K）筛选出适合开放式生成的问答对，扩充数据集。
- **输出** ：大量原始的 () 对，即“任务-范文”对。

#### 阶段二：思考轨迹合成 (Trajectory Synthesis)

这是整个方案的核心，即执行REER算法，为每一个 () 对生成一个高质量的思考轨迹 。

- **输入** ：一个从阶段一筛选出的高质量 () 对。
- **处理过程** ：这里就是我们在第二阶段详细描述的 **迭代式局部搜索算法** 的执行过程。我们再详细地串讲一遍：
- 这个循环会进行多次，直到满足终止条件。在每一次循环中：
	- **1\. 选择片段** ：从当前的思考轨迹 中选择一个片段 进行优化。
	- **2\. 生成候选** ：将任务 、范文 、以及 周围的上下文（即 的其他部分）再次喂给生成模型。使用一个专门用于“局部优化”的Prompt（参考Listing 2），要求模型仅针对 这一片段，生成多个（比如 个）更详细、更合理的候选版本 。
	- **3\. 评估候选** ：现在开始"打分"。对于每一个候选版本 ，构建一个临时的、完整的思考轨迹 （方法是把原轨迹 中的 替换为 ），然后计算这个临时轨迹的"惊奇分数"，即\*\*困惑度 \*\*。这个分数越低，说明 这个思考片段越能让最终的范文 显得合理。
	- **4\. 更新轨迹** ：比较所有候选版本（包括原始的 ）的困惑度分数，选择那个得分最低的候选版本 。用 正式替换掉 ，形成本次循环后更新的、质量更高的思考轨迹 。
	- **5\. 检查终止** ：判断 的整体困惑度是否已经低于设定的目标阈值，或者是否已达到最大循环次数。如果满足，则退出循环；否则，将 作为下一次循环的输入，继续优化其他片段。
- 将 () 对喂给一个生成模型（Generator LLM，论文中使用Qwen2.5-32B-Instruct）。
	- 使用一个精心设计的“思想启发”提示（Prompt，参考论文附录Listing 1），要求模型生成一个初步的、不完美的思考过程 。这个Prompt会引导模型思考用户意图、内容结构、头脑风暴等，并鼓励使用“嗯...”、“等等，也许...”这类人类思考模式的短语。
	- **当前状态** ：我们得到了一个粗糙的计划 。
1. **步骤 A: 初始化 (Initialization)**
	2. **步骤 B: 迭代优化循环 (Iterative Refinement Loop)**
- **输出** ：一个经过千锤百炼的、高质量的思考轨迹 。此时，我们得到了一个完整的训练样本三元组 ()。

#### 阶段三：模型训练 (Final Dataset Assembly for Fine-Tuning)

这一阶段的目标是利用合成好的数据，训练出最终的 `DeepWriter-8B` 模型。

- **输入** ：海量通过阶段二生成的 () 三元组。
- **处理过程** ：
- 将每一个三元组 () 格式化成一个标准的训练Prompt。格式大致如下：
	```
	[用户问题 x]
	<think>
	[思考轨迹 z*]
	</think>
	<answer>
	[最终答案 y]
	</answer>
	```
	- 使用这个最终的混合数据集，对一个基础模型（Qwen2-8B-Base）进行全参数微调（Fine-Tuning）。这个训练过程明确地教模型：在生成最终答案之前，先在 `<think>` 标签内进行一步步的深入思考。
- **思考终点过滤** ：丢弃那些在思考轨迹的最后10%部分仍然出现大量“嗯...”这类探索性短语的样本。这表明模型可能陷入了循环思考，未能形成最终决断。
	- **重复性过滤** ：计算每个轨迹中高频n-gram的重复率，过滤掉那些内容高度重复、语言贫乏的样本。
1. **数据过滤** ：为了确保最高质量，作者设计了两道过滤程序。
	2. **数据集混合** ：作者发现，如果只用自己生成的创意写作数据进行训练，模型可能会忘记其他领域的通用知识（这被称为“灾难性遗忘”）。为了避免这一点，他们将自己生成的 `DeepWriting-20K` 数据集与一个包含数学、科学、编程等领域思考过程的公共数据集（OpenThoughts）进行混合。
	3. **格式化与微调** ：
- **输出** ：最终的、学会了逆向工程推理能力的模型——\*\* `DeepWriter-8B` \*\*。

通过这三个环环相扣的阶段，论文作者构建了一个从数据采集到模型训练的完整闭环，成功地将REER这一创新思想落地，并验证了其有效性。一个没有看过这篇论文的人，根据这个流程，就能清晰地理解整个方案的技术路径和实施细节。

### 第四阶段：实验设计与验证分析

#### 1\. 主实验设计解读：核心论点的验证

- **核心主张** ：
- 论文的核心主张是：通过REER范式，可以在不依赖RL或昂贵蒸馏的情况下，从头训练一个中等规模（8B）的开源模型，使其在复杂的开放式生成任务上，达到甚至超越顶级的专有模型和现有开源模型的水平。
- **实验设计分析** ：
- **数据集** ：作者选择了三个具有代表性的基准测试集（Benchmark）：
	- **评价指标** ：
	由于开放式生成任务的评价是主观的，作者采用了当前该领域的 **通用标准** ：使用更强大的LLM（GPT-4o, Claude 3.7）作为裁判进行打分。虽然LLM裁判存在一定偏见，但在缺乏大规模人工标注的情况下，这是目前最可扩展和一致的评估方法。作者明确指出了所用裁判模型，保证了实验的可复现性。
	- \*\*基线方法 (Baselines)\*\*：
	**合理性** ：基线的选择兼顾了"与同级最佳比较"和"挑战行业顶级"，使得实验结果的说服力非常强。
1. **开源基线** ： `LongWriter-8B` 。这是一个在长文本写作领域非常强大的开源模型，作为同量级的比较对象，能够突显REER方法相对于传统微调方法的优势。
	2. **专有模型** ： `GPT-4o`, `Claude 3.5`, `Claude 3.7` 。这些是当前公认的最先进模型（SOTA），将 `DeepWriter-8B` 与它们进行比较，是想证明该方法不仅在开源社区领先，而且足以挑战行业顶级水平。
1. \*\*LongBench-Write (LB)\*\*：这是一个“压力测试”，专门评估模型生成超长文本（>10,000词）时保持连贯性的能力。选择它非常合理，因为它直接考验了深度规划和长期一致性，这正是深度思考要解决的核心问题之一。
	2. **HelloBench (HB) **：这个数据集包含了大量“真实世界”的用户查询，分为** HB-A (开放式问答) **和** HB-B (启发式文本生成) **。选择它是为了验证模型的** 泛化能力** 和 **实用性** ，证明模型不只是在特定任务上表现好，而是能应对五花八门的真实需求。
	3. **WritingBench (WB) **：这是一个专业领域的写作测试集，涵盖学术、金融、法律、文学等六个领域。选择它旨在检验模型的** 领域专业性** 和 **遵循复杂指令的能力** ，这是高级生成能力的重要体现。 **合理性** ：这三个数据集的选择覆盖了 **长度** 、 **广度** 和 **深度** 三个维度，构成了非常全面的评估体系，能够有力地支撑实验结论。
- \*\*主实验结果与结论 (Table 1)\*\*：
- **结果支撑** ：表格数据显示， `DeepWriter-8B` 在 **所有** 测试集上都 **显著优于** 开源基线 `LongWriter-8B` ，尤其在专业的WritingBench上平均提升超过18分，这直接证明了REER方法的优越性。更引人注目的是， `DeepWriter-8B` 在长文本生成（LB）上甚至超过了GPT-4o和Claude 3.5；在专业写作（WB）上全面超越Claude 3.5，并与GPT-4o和Claude 3.7高度可比。
	- **结论** ：主实验强有力地验证了论文的核心论点。REER是一个非常有效的方法，它能够赋能中等规模模型，使其具备与巨型模型相抗衡的深度思考和复杂文本生成能力。

#### 2\. 消融实验分析：内部组件的贡献

消融实验（见Table 2）的目的是“拆解”REER方法，验证每个设计环节的必要性。

- **被消融的关键模块** ：
1. \*\*移除合成数据 (Remove Synthesis Data) **：只用公共的思考数据集（如OpenThoughts）进行训练，完全不用 `DeepWriting-20K` 。这对应了** "自制数据本身"\*\*的贡献。
	**结果** ：性能出现 **断崖式下跌** ，是所有消融项中影响最大的。这定量地证明了： **不是任何思考数据都有用，为开放式任务量身定制的、通过REER合成的高质量数据才是性能提升的根本原因** 。
	2. \*\*移除迭代搜索 (Remove Iterative Search) **：使用最初生成的、未经优化的思考轨迹 `z(0)` 来训练模型。这对应了** "迭代优化过程"\*\*的贡献。
	**结果** ：性能同样显著下降，尤其在需要精细规划的WritingBench任务上。这证明了： **通过困惑度指导的迭代优化过程是必不可少的，它能发现比初始计划更优的推理路径，从而教给模型更强的生成能力** 。
	3. \*\*移除反思性词元 (Remove Reflection Tokens) **：在数据合成阶段，不鼓励模型使用"Hmm...", "Wait, that's..."。这对应了** "人性化思考模式注入"\*\*的贡献。
	**结果** ：整体性能轻微下降，但在最具创造性的"文学与艺术"领域（WB-D）下降最明显。这巧妙地证明了： **这些模拟人类探索、纠错的思考模式，对于培养模型的创造性灵活性至关重要** 。
	4. \*\*移除特定领域数据 (Remove Literature & Arts data)\*\*：
	**结果** ：移除创造性和叙事性任务的数据后，模型在 **所有** 基准上的表现都下降了，而不仅仅是在文学领域。这揭示了一个深刻的洞见： **训练模型处理创意和叙事任务，能够培养一种更通用的、处理细微差别和结构化思维的能力，这种能力可以迁移到其他看似无关的领域。**

#### 3\. 深度/创新性实验剖析：洞察方法的内在特性

除了常规实验，作者还设计了两个非常巧妙的实验来提供更深层次的洞察。

1. **实验一：深度思考质量的定性分析 (Qualitative Analysis - Figure 6)**
- **实验目的** ：超越单一的分数，从多个维度 **定性地** 评估模型思考过程的“质量”，回答“我们的模型究竟强在哪里？”这个问题。
	- **实验设计** ：设计了五个与深度思考直接相关的评估维度（问题解构、逻辑一致性、分析深度、表达清晰度、事实性）。然后让裁判对不同模型的输出，按照这五个维度进行打分，并绘制成雷达图进行可视化对比。
	- **实验结论** ：雷达图直观地展示了 `DeepWriter-8B` （深蓝色区域）的思考能力 **远比** 基线 `LongWriter-8B` （红色区域） **更全面、更强大** ，其“能力多边形”几乎完全包住了后者。同时，它的形状和大小与 `GPT-4o` （绿色区域）非常接近。这个可视化结果极具说服力，它证明了REER带来的提升是 **全方位的、深层次的** ，而不仅仅是表面文字的流畅。
3. **实验二：思考模式频率对比分析 (Qualitative Comparison of Thinking Patterns - Figure 7)**
- **左图（完整模型）** 的短语分布 **更多样、更均衡** ，大量出现了“let me think”, “maybe”, “hmm”, “wait”这类探索性和反思性的词语。
	- **右图（消融模型）** 的短语分布则 **非常僵化、公式化** ，高度依赖于少数几个词，如“so”, “next”, “first”。
- **实验目的** ：为了验证“注入人性化思考模式”这一设计（即消融实验的第3点）是否真的改变了模型的行为。这是一个探究 **因果关系** 的实验。
	- **实验设计** ：这个设计非常聪明。作者对比了 **完整模型** 和 **移除了“反思性词元”训练的模型** ，在生成思考过程时，所使用的思考短语（如 'let me think', 'maybe', 'first'）的频率分布。
	- **实验结论** ：结果一目了然。
	这个实验提供了 **直接证据** ，证明了作者在数据合成阶段的Prompt Engineering **成功地塑造了模型更灵活、更具探索性的“思考风格”** ，而不是一种死板的流程。

本文题目：Reverse-Engineered Reasoning for Open-Ended Generation

**欢迎Deep Learning同好与我交流、讨论、合作！**

**现已提供论文解读和idea讨论定制服务，可私信后台联系**

  

个人观点，仅供参考

[Read more](https://mp.weixin.qq.com/s/)

继续滑动看下一个

沈公子今天读什么

向上滑动看下一个