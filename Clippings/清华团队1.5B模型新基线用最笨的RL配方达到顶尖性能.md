---
title: "清华团队：1.5B 模型新基线！用「最笨」的 RL 配方达到顶尖性能"
source: "https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&chksm=8552fb11695b9442b625f1e81e1de3d79f2dac83e179e65dd78874b33e978e2eb7e564b56033&idx=2&mid=2651001318&sn=b476f1d721b202a9dfdaf60e008cea4f&poc_token=HCs0FWmjnD-zleoOBjQl0w2_5O7e7iJVJe8Eyrbs"
author:
  - "[[Weixin Official Accounts Platform]]"
published:
created: 2025-11-13
description: "\x26quot;如无必要，勿增实体\x26quot;"
tags:
  - "极简配方"
  - "单阶段训练"
  - "固定超参数"
abstract: "清华团队证明，仅使用最基础的强化学习配方，无需复杂技术，就能让1.5B模型在数学推理任务上达到顶尖性能。"
---
*2025年11月13日 07:51*

![Image](https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic1GuW68DykycvknmG9tyBvLRsVGY4rRKCGuKKSkOqnGrvGwXxqqDxHlia88ZCbqyicswl2HC89BcZA/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1#imgIndex=0)

  

如果有人告诉你：不用分阶段做强化学习、不搞课程学习、不动态调参，只用最基础的 RL 配方就能达到小模型数学推理能力 SOTA，你信吗？

  

清华团队用两个 1.5B 模型给出了答案： 不仅可行 ，还特 别高效 。

  

- 核心发现 ： 单阶段训练 + 固定超参数 = SOTA 性能 + 省一半算力
- 意外之 喜 ： 训练曲线平滑得像教科书，4000 步没遇到任何 "典型问题"
- 关键启示 ： 充分 scale 的简单 baseline，可能比我们想象的强大得多

  

![Image](https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9MJaGgnUWevWy2EePDqGWm8nQe37yP1ztzFoOe0AIwj0iaae4rNhhS05k2dT2gtbMgYAMGkKPWe5Q/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1#imgIndex=1)

  

![Image](https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9MJaGgnUWevWy2EePDqGWmwZRN2MibkyJaR6ibVwq3U6khpb1t8YDDOia4nicP2ZSm44YWibpGn86Awdg/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1#imgIndex=2)

  

- 技术博客：https://relieved-cafe-fe1.notion.site/JustRL-Scaling-a-1-5B-LLM-with-a-Simple-RL-Recipe-24f6198b0b6b80e48e74f519bfdaf0a8
- 开源模型：https://huggingface.co/collections/hbx/justrl
- 评测脚本：https://github.com/thunlp/JustRL

  

背景：RL 训练小模型的 "技术军备竞赛"

  

2025 年初，DeepSeek-R1 开源后，如何用 RL 训练 1.5B 级别的推理模型成为了热门研究方向。短短几个月内，这个领域经历了快速的技术演进：早期的工作尝试超参数调优和长度控制；随后出现了多阶段渐进训练，每个阶段调整数据难度和 RL 超参数；也有方法引入了课程学习，用部分解作为提示精心设计难度梯度；最激进的做法直接将 rollout 数量提升到 512 次，用算力进行暴力探索。

  

![Image](https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9MJaGgnUWevWy2EePDqGWmo1Iu98ytqSsp3e5PXDfJcrvEua42mTzX8cpb59H7vHJ92Sn7icQEITA/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1#imgIndex=3)

近期工作用到的技术对比

  

这些方法都取得了不错的效果，性能在不断刷新 。 动态采样、KL 重置、自适应惩罚、长度控制…… 各种稳定技术和优化 trick 被逐一引入。每个新工作都在前人基础上增加新的模块和机制，整个训练 pipeline 变得越来越复杂。

  

然而，这种复杂度的增长也带来了困惑： 这些技术真的都是必要的吗？ 当不同工作组合使用不同的技术子集时，我们很难分辨哪些是解决根本问题的，哪些只是在修补其他设计选择带来的副作用。更重要的是，如果 baseline 本身就不稳定，那么为了稳定它而加入的技术，可能只是在治标而非治本。

  

清华团队带着一个朴素的想法开始了这项工作： "如果我们用最基础的配方，但训练得足够充分，能到什么水平？"

  

于是就有了 JustRL —— 名字的意思是 "就这样"。

  

方法：极简到极致的训练配方

  

JustRL 的设计哲学是 "减到不能再减"。研究者刻意避免了近期工作中常见的复杂技术，只保留了最基础的组件。

  

训练配方简单到令人意外 ： 算法使用标准的 GRPO，没有任何魔改；训练只有一个阶段，从头到尾连续进行；超参数完全固定，不做任何动态调整；数据来自常规的数学问题集，不进行离线难度筛选、不做数据增强、不使用 dynamic sampling。

  

更关键的是，同一套超参数在两个完全不同的起点上都有效 。 第一个实验使用 DeepSeek-R1-Distill-Qwen-1.5B 作为基座，这是一个相对较弱的起点（AIME 2024 准确率 29%）；第二个实验使用 OpenMath-Nemotron-1.5B，这已经是一个相当强的基座（AIME 2024 准确率 61%）。 研究者没有针对不同模型调整任何参数，在 9 个数学推理基准（AIME 2024/2025、AMC 2023、MATH-500、Minerva Math、OlympiadBench、HMMT/CMIMC/BRUMO 2025）上的全面评测显示，JustRL 达到了 1.5B 模型的最高水平 。

  

![Image](https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9MJaGgnUWevWy2EePDqGWmeePNjGwKqSXWYb8G9HicqQN1l68Inwtx7bEIRMqnPK9R6W4ec46Ge4w/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1#imgIndex=4)

  

一个关键问题：会不会是用了更多算力？正好 相反，我们 用了更少。

  

从弱基座起步的 JustRL-DeepSeek-1.5B，最终在 9 项基准上平均达到 54.87%，超越了采用 9 阶段训练的 ProRL-V2（53.08%）。更值得注意的是计算效率：JustRL 使用的总 token 预算约为 1.4E+11， 仅为 ProRL-V2 的一半，为 BroRL 的五分之一 。在算力 - 性能的权衡上，JustRL 达到了一个新的平衡点。

  

从强基座起步的 JustRL-Nemotron-1.5B 表现更加出色，平均准确率达到 64.32%，略微超过使用课程学习的 QuestA（63.81%）。关键的差异在于，QuestA 需要完整的推理轨迹来构建 hint，还要分阶段调整提示难度；而 JustRL 只需要标准的问题与标答，不需要额外的数据工程 ，总 token 预算也相对较小。

  

整个训练在 32 张 A800-80GB GPU 上进行，每个模型训练约 15 天。相比一些需要多阶段训练、频繁调参的方法，JustRL 的工程复杂度和计算开销都显著更低。这些结果的意义不仅在于数字本身，更在于它们揭示的一个可能性：很多时候， 我 们可能低估了简单方法在充分 scale 下的潜力 。

  

意外发现：4000 步训练，异常平稳

  

也许比最终性能更令人惊讶的是训练过程本身。研究者详细记录了 JustRL-DeepSeek-1.5B 整个 4000 步 RL 过程中的关键动态指标：策略熵、平均奖励、响应长度。

  

策略熵始终在 1.2-1.4 范围内健康震荡，没有出现向上漂移（探索崩塌）或向下崩溃（过早收敛）；平均奖励从 - 0.6 单调上升到 +0.4，虽然有噪声但趋势清晰，没有长时间的 plateau 或突然的下跌；响应长度从初始的 8000 tokens 自然压缩到 4000-5000 tokens，并稳定在这个范围， 这一切都是在没有使用 overlong penalty 的情况下发生 的 ， 仅仅设置了最大 16k 的上下文长度 。

  

![Image](https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9MJaGgnUWevWy2EePDqGWmftNI4rgI5UMdF0cW1mDwpBJCbjoVzU521s3SGAKibw5GXDR67Awk2cg/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1#imgIndex=5)

JustRL-DeepSeek-1.5B 的训练 dynamic

  

这与很多现有工作报告 的 训练 困难形成 鲜明对比。

  

- ProRL："我们观察到熵崩溃和训练不稳定性…"
- BroRL："训练到瓶颈只能加 rollout 加大探索…"
- QuestA："需要课程学习避免熵崩塌（简单题）或者减缓学习效率（难任务）…"

  

而在 JustRL 的训练中，这些问题都没有出现。这给了我们一个有趣的观察： 也许在某些配置下，当 baseline 足够简单、训练规模足够充分时，一些在复杂系统中出现的稳定性问题可能就不容易发生。

  

一个有趣的插曲：加 "优化" 反而更差

  

训练过程中，团队尝试了两个 "按常理应该有帮助" 的修改。这两个实验的结果颇具启发性。

  

第一个实验是加入显式的长度惩罚 。 动机很直接：不少工作证明长度惩罚有效，那么添加一个惩罚项应该能让模型输出更简洁，提高训练效率。结果却令人意外：性能从 55% 下降到 50%。深入分析发现，显式惩罚导致了熵崩塌，熵值从 1.2-1.4 降到 0.4-0.6 ，探索空间被过早压缩。模型还没来得及充分探索有效的解题策略，就被迫收敛到更短的响应上。

  

第二个实验是换用更宽松的验证器 。 逻辑同样合理：减少假阴性（正确答案被误判为错误）应该能提供更清晰的学习信号。但性能继续下滑到 45%。可能的原因包括：更宽松的验证器虽然减少了误判，但也降低了学习信号的细粒度 ——"几乎正确" 和 "完全正确" 不再有明显区分；另一种可能是，严格的格式要求实际上在迫使模型发展更鲁棒的内部推理，而宽松的验证器消除了这种压力。

  

![Image](https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9MJaGgnUWevWy2EePDqGWm7VELDpyOHZIBicwiaJg1iajug4MZ9eqAasz4MJfD4S0IU9Y2ccRuNFAtw/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1#imgIndex=6)

两组 ablation 效果

  

这说明什么？ 一方面，ablation 在接近 2k steps 的尺度上 才开始分道扬镳 ， 意味着现有的 RL tricks ablation 可能在小规模上（几十 / 几百步）得到的结论不一定适合于大规模 scaling， 要验证 tricks 的作用可能长期才能看出区别 ；另一方面，不是说这些技术本身不好（它们在其他工作中确实有效），而是：

  

- 技术的价值 高度依 赖于 baseline 的特性
- 在一个稳定的 baseline 上，某些 "优化" 可能适得其反
- 不是所有看起来合理的东西都该加

  

这个工作想说什么？

  

不是要证 明 "简单 永远最好"

  

- 不是说： "复杂方法都没用"
- 而是说： "我们可能低估了简单方法在充分 scale 下的潜力"
- 不是说： "大家都做错了"
- 而是说： "建立清晰的简单 baseline，能更准确地评估复杂技术的价值"
- 不是说： "永远别用复杂技术"
- 而是说： "先验证简单方法的极限在哪，再决定是否需要复杂度"

  

写在最后：关于 "够用" 的哲学

  

> "Perfection is achieved, not when there is nothing more to add, but when there is nothing left to take away.“
> 
> — Antoine de Saint-Exupéry, Airman's Odyssey

JustRL 不是要证明 "简单就是答案"。它想提醒的是： 在不断追求技术创新的同时，别忘了回头看看 —— 最朴素的方法，在足够的努力下，能做到什么程度。

  

也许在 RL 训练小模型这个领域，我们一直在做加法：加阶段、加调度、加采样策略、加稳定技巧。也许现在是时候试试奥卡姆剃刀的做法：减到不能再减，看看还剩什么。JustRL 的发现是：剩下的，可能已经够用了。

  

如果你正在做 RL，不妨试试：先把简单配方训练充分，看看它能带你走多远。

  

也许你会发现：够用了。

  

也许你会发现：还不够，但现在你知道差在哪了。

  

无论哪种，都是有价值的收获。 "如无必要，勿增实体"。

  

![图片](https://mp.weixin.qq.com/www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg%20stroke='none'%20stroke-width='1'%20fill='none'%20fill-rule='evenodd'%20fill-opacity='0'%3E%3Cg%20transform='translate(-249.000000,%20-126.000000)'%20fill='%23FFFFFF'%3E%3Crect%20x='249'%20y='126'%20width='1'%20height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

  

© THE END

转载请联系本公众号获得授权

投稿或寻求报道：liyazhou@jiqizhixin.com

  

继续滑动看下一个

机器之心

向上滑动看下一个